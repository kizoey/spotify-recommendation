{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Spotify_추천시스템 취합.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8lCpKIFaDtL8"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqq47K44DEEo"
      },
      "source": [
        "## Setup ## \n",
        "import tensorflow\n",
        "import IPython\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import sklearn as skl\n",
        "import tensorflow as tf\n",
        "import os,sys\n",
        "import joblib\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idB-VTVJDEEr"
      },
      "source": [
        "# 분석에 필요한 함수 \n",
        "\n",
        "def today():\n",
        "  from datetime import datetime\n",
        "  year = str(datetime.today().year)\n",
        "\n",
        "  if datetime.today().month<10:\n",
        "    month = str(0)+ str(datetime.today().month)\n",
        "  else : month = str(datetime.today().month)\n",
        "\n",
        "  if datetime.today().day<10:\n",
        "    day = str(0)+ str(datetime.today().day)\n",
        "  else : day = str(datetime.today().day)\n",
        "\n",
        "  if (datetime.today().hour + 9)%24 < 10:\n",
        "    hour = str(0)+ str((datetime.today().hour+9)%24)\n",
        "  else : hour = str((datetime.today().hour+9)%24)\n",
        "\n",
        "  if datetime.today().minute<10:\n",
        "    min = str(0)+ str(datetime.today().minute)\n",
        "  else : min = str(datetime.today().minute)\n",
        "\n",
        "\n",
        "  return year+month+day+'_'+hour+'시'+min+'분'\n",
        "def get_clf_eval(y_test, y_pred):\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    F1 = f1_score(y_test, y_pred)\n",
        "    AUC = roc_auc_score(y_test, y_pred)\n",
        "    print('오차행렬:\\n', confusion)\n",
        "    print('\\n정확도: {:.4f}'.format(accuracy))\n",
        "    print('정밀도: {:.4f}'.format(precision))\n",
        "    print('재현율: {:.4f}'.format(recall))\n",
        "    print('F1: {:.4f}'.format(F1))\n",
        "    print('AUC: {:.4f}'.format(AUC))\n",
        "\n",
        "def Directory(keyword):\n",
        "  while True:\n",
        "    try:\n",
        "      base = '/gdrive/MyDrive/Colab Notebooks/Spotify Recommendation/Recommendation Models/'\n",
        "      directory=base + keyword+'/'\n",
        "      if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "            os.chdir(directory)\n",
        "            return directory\n",
        "            break\n",
        "      else:\n",
        "        os.chdir(directory)\n",
        "        return directory\n",
        "        break\n",
        "    except OSError:\n",
        "      print ('Error: Creating directory. ' +  directory)\n",
        "      continue\n",
        "  return directory\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_top_n(predictions, n=10):\n",
        "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
        "\n",
        "    Args:\n",
        "        predictions(list of Prediction objects): The list of predictions, as\n",
        "            returned by the test method of an algorithm.\n",
        "        n(int): The number of recommendation to output for each user. Default\n",
        "            is 10.\n",
        "\n",
        "    Returns:\n",
        "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
        "        [(raw item id, rating estimation), ...] of size n.\n",
        "    \"\"\"\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_n[uid].append((iid, est))\n",
        "\n",
        "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = user_ratings[:n]\n",
        "\n",
        "    return top_n\n",
        "\n",
        "def Recommend(uid,top_n):\n",
        "  print(f'<{uid} 님을 위한 추천곡>\\n')\n",
        "  for (iid,est) in top_n[uid]:\n",
        "    print(f'{iid} => {round(round(est,4)*100,2)} %의 확률로 좋아하실 거에요')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n5FpWIGDEEt",
        "outputId": "98289ea9-dc5b-4d8d-80bd-15bc791d2d8a"
      },
      "source": [
        "#Surpirse 설치\n",
        "!pip install surprise\n",
        "import surprise\n",
        "from surprise import BaselineOnly \n",
        "from surprise import KNNWithMeans, KNNBaseline, KNNBasic, KNNWithZScore\n",
        "from surprise import SVD,SVDpp\n",
        "from surprise import SlopeOne, CoClustering\n",
        "from surprise import NMF\n",
        "from surprise import Dataset\n",
        "from surprise import accuracy\n",
        "from surprise import Reader\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import NormalPredictor\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.model_selection import KFold\n",
        "from surprise.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: surprise in c:\\users\\82102\\anaconda3\\lib\\site-packages (0.1)\n",
            "Requirement already satisfied: scikit-surprise in c:\\users\\82102\\anaconda3\\lib\\site-packages (from surprise) (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from scikit-surprise->surprise) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from scikit-surprise->surprise) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from scikit-surprise->surprise) (1.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\82102\\anaconda3\\lib\\site-packages (from scikit-surprise->surprise) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fsyqljAEDEEu",
        "outputId": "58487751-e8cd-4e04-d934-d6537d066b2e"
      },
      "source": [
        "# 데이터 불러오기 및 편집\n",
        "\n",
        "data_dict = pd.read_excel(r\"C:\\Users\\82102\\Desktop\\KU-BIG\\Spotify\\Spotify_Data.xlsx\",sheet_name=None,header=0,index_col=0)\n",
        "ratings = data_dict['Ratings']\n",
        "tracks = data_dict['Track_Features']\n",
        "display(ratings,tracks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>도윤</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_Herng</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤__Melanie</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_JInju</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_Maya</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_MI</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_Geraldine</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_Tiffany</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>형록1</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>형록2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>헝록3</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>형록4</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>창현_승련</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>창현</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>창현_세영</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>다연1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>다연2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>다연3</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>나윤1</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>나윤2</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>나윤3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>나윤4</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>효진1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>효진2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>효진3</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>근호</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>주영</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>윤지현</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>금지헌</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29 rows × 162 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              1    2    3    4    5    6    7    8    9    10   ...  153  154  \\\n",
              "도윤              1    0    1    1    0    1    1    0    1    0  ...    1    1   \n",
              "도윤_Herng        1    0    1    0    0    0    0   -1    0    1  ...    1    0   \n",
              "도윤__Melanie    -1   -1   -1   -1   -1   -1   -1    1    1    1  ...    1    1   \n",
              "도윤_JInju        1   -1    1   -1    1   -1    1    1   -1    1  ...    1    0   \n",
              "도윤_Maya         1    1    0    1    1    0    1    1    1    1  ...    1    1   \n",
              "도윤_MI           1    0    0    0    0    0    0    0    0    0  ...    1    1   \n",
              "도윤_Geraldine    1    1    0    0    1    0    0    0    0    0  ...    1    0   \n",
              "도윤_Tiffany      0    0    0    0    0    0    0    1    0    1  ...    1    1   \n",
              "형록1            -1   -1   -1   -1   -1   -1    0    1    1    1  ...   -1   -1   \n",
              "형록2             0    0    1    1    0    1    0   -1   -1    1  ...   -1    1   \n",
              "헝록3            -1   -1   -1   -1   -1   -1   -1   -1    0    1  ...    1   -1   \n",
              "형록4             1   -1   -1   -1   -1    1    1    1    1    1  ...    1    1   \n",
              "창현_승련           0    0    0    0    0    0    0    0    0    1  ...    1   -1   \n",
              "창현              0    0    0    0    0    0    0    0    0    1  ...    1   -1   \n",
              "창현_세영           0    0    0    0    0    0    0    1    0    1  ...    1    1   \n",
              "다연1             0    0    0    0    0    0    0    1    0    0  ...   -1    1   \n",
              "다연2             0    0    0    1    0    1   -1    1    0    1  ...    0   -1   \n",
              "다연3             1   -1    0    0    0   -1    1    1    1    1  ...    0    1   \n",
              "나윤1            -1   -1    1   -1    1   -1    1    1   -1   -1  ...    1   -1   \n",
              "나윤2            -1   -1    1    1    1    1    1    1   -1    1  ...    0    1   \n",
              "나윤3             1    0    1    0    1   -1    0    1    1   -1  ...    0    0   \n",
              "나윤4            -1   -1   -1   -1    1   -1    0    1    0    1  ...    1    1   \n",
              "효진1             1    0    0   -1    1    0    1    1    0    1  ...    1    0   \n",
              "효진2             1    0    0   -1    1    0    1    1    0   -1  ...    0    0   \n",
              "효진3             1   -1    0   -1    1    0    0    1    0    1  ...    0    1   \n",
              "근호             -1   -1   -1   -1   -1   -1    1    1   -1    1  ...   -1   -1   \n",
              "주영              0    1    1    0    0    0    1   -1    0    1  ...    1    1   \n",
              "윤지현             0    0    0    0    0    0    0    1    0    0  ...   -1    0   \n",
              "금지헌             1    1    1    0    0    0    1    0    0   -1  ...   -1    0   \n",
              "\n",
              "              155  156  157  158  159  160  161  162  \n",
              "도윤              1    1    0    0    0    1    1    1  \n",
              "도윤_Herng        0    0    1    0    1    1    1    1  \n",
              "도윤__Melanie     1   -1    1   -1   -1   -1   -1   -1  \n",
              "도윤_JInju       -1    0    1   -1    1   -1   -1    1  \n",
              "도윤_Maya         1    1    1   -1    0    0   -1    0  \n",
              "도윤_MI           1    1    1   -1    1    1    1    1  \n",
              "도윤_Geraldine    0    1    1    0    1    1    0    1  \n",
              "도윤_Tiffany      1    1    1    0    1    1    1    1  \n",
              "형록1             1   -1   -1   -1    1    1    1    1  \n",
              "형록2             1    0    1    0    1    1    1    1  \n",
              "헝록3             1   -1   -1    1    1    1    1    1  \n",
              "형록4             1    1    1    1    1    1    1    1  \n",
              "창현_승련           0   -1    0    0   -1    1    1    1  \n",
              "창현              0   -1    0    0   -1    1    1    1  \n",
              "창현_세영           1    1    1    1    1    1    1    1  \n",
              "다연1             1    1    1    0    1    1    1    1  \n",
              "다연2            -1    0    0    0    1    1    1    1  \n",
              "다연3            -1    1    0    0    1    1   -1    1  \n",
              "나윤1             1    0   -1   -1    1   -1    1    1  \n",
              "나윤2            -1   -1   -1   -1    1   -1    1    1  \n",
              "나윤3             1   -1   -1   -1    1   -1    0    1  \n",
              "나윤4             1    1   -1   -1    1    1    1    1  \n",
              "효진1             1   -1   -1    0    1    1    1    1  \n",
              "효진2            -1   -1   -1    1   -1    1    1    1  \n",
              "효진3             1    1    1    0    1   -1    0    1  \n",
              "근호             -1   -1   -1   -1   -1   -1   -1    1  \n",
              "주영              1    1    1    1    1    1    1    1  \n",
              "윤지현             0    0    0    0    0    0    0    1  \n",
              "금지헌             0    0    0    0    1    1   -1    1  \n",
              "\n",
              "[29 rows x 162 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acousticness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>energy</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>valence</th>\n",
              "      <th>song_title</th>\n",
              "      <th>artist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.01980</td>\n",
              "      <td>0.571</td>\n",
              "      <td>199640</td>\n",
              "      <td>0.870</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0564</td>\n",
              "      <td>-3.877</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0641</td>\n",
              "      <td>172.716</td>\n",
              "      <td>4</td>\n",
              "      <td>0.815</td>\n",
              "      <td>Bye Bye Bye</td>\n",
              "      <td>NSYNC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.30000</td>\n",
              "      <td>0.550</td>\n",
              "      <td>266293</td>\n",
              "      <td>0.563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1220</td>\n",
              "      <td>-7.019</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0313</td>\n",
              "      <td>166.004</td>\n",
              "      <td>4</td>\n",
              "      <td>0.462</td>\n",
              "      <td>This I Promise You - Radio Edit</td>\n",
              "      <td>NSYNC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04790</td>\n",
              "      <td>0.647</td>\n",
              "      <td>191280</td>\n",
              "      <td>0.870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>-4.702</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0798</td>\n",
              "      <td>165.103</td>\n",
              "      <td>4</td>\n",
              "      <td>0.906</td>\n",
              "      <td>It's Gonna Be Me</td>\n",
              "      <td>NSYNC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.44000</td>\n",
              "      <td>0.468</td>\n",
              "      <td>243493</td>\n",
              "      <td>0.535</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>-8.264</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0436</td>\n",
              "      <td>168.001</td>\n",
              "      <td>4</td>\n",
              "      <td>0.298</td>\n",
              "      <td>God Must Have Spent a Little More Time on You ...</td>\n",
              "      <td>NSYNC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.07430</td>\n",
              "      <td>0.754</td>\n",
              "      <td>199787</td>\n",
              "      <td>0.946</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>8</td>\n",
              "      <td>0.4010</td>\n",
              "      <td>-2.149</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0403</td>\n",
              "      <td>112.042</td>\n",
              "      <td>4</td>\n",
              "      <td>0.865</td>\n",
              "      <td>I Want You Back - Radio Edit</td>\n",
              "      <td>NSYNC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>0.26700</td>\n",
              "      <td>0.563</td>\n",
              "      <td>315787</td>\n",
              "      <td>0.847</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>-6.190</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>82.757</td>\n",
              "      <td>4</td>\n",
              "      <td>0.810</td>\n",
              "      <td>Can I Touch You...There?</td>\n",
              "      <td>Michael Bolton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>0.00574</td>\n",
              "      <td>0.682</td>\n",
              "      <td>192000</td>\n",
              "      <td>0.812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0553</td>\n",
              "      <td>-2.691</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1180</td>\n",
              "      <td>104.891</td>\n",
              "      <td>4</td>\n",
              "      <td>0.347</td>\n",
              "      <td>Shape of You (Major Lazer Remix) [feat. Nyla &amp;...</td>\n",
              "      <td>Ed Sheeran</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>0.02020</td>\n",
              "      <td>0.333</td>\n",
              "      <td>174240</td>\n",
              "      <td>0.913</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1460</td>\n",
              "      <td>-3.722</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0808</td>\n",
              "      <td>99.889</td>\n",
              "      <td>4</td>\n",
              "      <td>0.129</td>\n",
              "      <td>Paris - Pegboard Nerds Remix</td>\n",
              "      <td>The Chainsmokers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0.00087</td>\n",
              "      <td>0.465</td>\n",
              "      <td>265600</td>\n",
              "      <td>0.953</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>8</td>\n",
              "      <td>0.2260</td>\n",
              "      <td>-4.684</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2950</td>\n",
              "      <td>160.064</td>\n",
              "      <td>4</td>\n",
              "      <td>0.554</td>\n",
              "      <td>Don't Let Me Down - Zomboy Remix</td>\n",
              "      <td>The Chainsmokers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.00262</td>\n",
              "      <td>0.484</td>\n",
              "      <td>224320</td>\n",
              "      <td>0.609</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11</td>\n",
              "      <td>0.3390</td>\n",
              "      <td>-5.652</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0303</td>\n",
              "      <td>131.049</td>\n",
              "      <td>4</td>\n",
              "      <td>0.266</td>\n",
              "      <td>Something Just Like This - ARMNHMR Remix</td>\n",
              "      <td>The Chainsmokers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>162 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     acousticness  danceability  duration_ms  energy  instrumentalness  key  \\\n",
              "1         0.01980         0.571       199640   0.870          0.000007    8   \n",
              "2         0.30000         0.550       266293   0.563          0.000000    2   \n",
              "3         0.04790         0.647       191280   0.870          0.000000    0   \n",
              "4         0.44000         0.468       243493   0.535          0.000000   10   \n",
              "5         0.07430         0.754       199787   0.946          0.000391    8   \n",
              "..            ...           ...          ...     ...               ...  ...   \n",
              "158       0.26700         0.563       315787   0.847          0.000000    0   \n",
              "159       0.00574         0.682       192000   0.812          0.000000    1   \n",
              "160       0.02020         0.333       174240   0.913          0.000000    2   \n",
              "161       0.00087         0.465       265600   0.953          0.014600    8   \n",
              "162       0.00262         0.484       224320   0.609          0.000000   11   \n",
              "\n",
              "     liveness  loudness  mode  speechiness    tempo  time_signature  valence  \\\n",
              "1      0.0564    -3.877     0       0.0641  172.716               4    0.815   \n",
              "2      0.1220    -7.019     1       0.0313  166.004               4    0.462   \n",
              "3      0.0485    -4.702     0       0.0798  165.103               4    0.906   \n",
              "4      0.1010    -8.264     1       0.0436  168.001               4    0.298   \n",
              "5      0.4010    -2.149     0       0.0403  112.042               4    0.865   \n",
              "..        ...       ...   ...          ...      ...             ...      ...   \n",
              "158    0.2430    -6.190     1       0.3250   82.757               4    0.810   \n",
              "159    0.0553    -2.691     0       0.1180  104.891               4    0.347   \n",
              "160    0.1460    -3.722     1       0.0808   99.889               4    0.129   \n",
              "161    0.2260    -4.684     0       0.2950  160.064               4    0.554   \n",
              "162    0.3390    -5.652     0       0.0303  131.049               4    0.266   \n",
              "\n",
              "                                            song_title            artist  \n",
              "1                                          Bye Bye Bye             NSYNC  \n",
              "2                      This I Promise You - Radio Edit             NSYNC  \n",
              "3                                     It's Gonna Be Me             NSYNC  \n",
              "4    God Must Have Spent a Little More Time on You ...             NSYNC  \n",
              "5                         I Want You Back - Radio Edit             NSYNC  \n",
              "..                                                 ...               ...  \n",
              "158                           Can I Touch You...There?    Michael Bolton  \n",
              "159  Shape of You (Major Lazer Remix) [feat. Nyla &...        Ed Sheeran  \n",
              "160                       Paris - Pegboard Nerds Remix  The Chainsmokers  \n",
              "161                   Don't Let Me Down - Zomboy Remix  The Chainsmokers  \n",
              "162           Something Just Like This - ARMNHMR Remix  The Chainsmokers  \n",
              "\n",
              "[162 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "npFVnBYCDEEv",
        "outputId": "bb775e76-6b93-4f67-e068-0a51dde3c872"
      },
      "source": [
        "#노래제목과 가수이름 합치기\n",
        "\n",
        "temp = [title+'__'+artist for title,artist in zip(tracks.song_title,tracks.artist)]\n",
        "track_ft = tracks.drop(['song_title','artist'],axis=1)\n",
        "track_ft['track_name']=temp\n",
        "display(track_ft)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acousticness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>energy</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>valence</th>\n",
              "      <th>track_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.01980</td>\n",
              "      <td>0.571</td>\n",
              "      <td>199640</td>\n",
              "      <td>0.870</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0564</td>\n",
              "      <td>-3.877</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0641</td>\n",
              "      <td>172.716</td>\n",
              "      <td>4</td>\n",
              "      <td>0.815</td>\n",
              "      <td>Bye Bye Bye__NSYNC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.30000</td>\n",
              "      <td>0.550</td>\n",
              "      <td>266293</td>\n",
              "      <td>0.563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1220</td>\n",
              "      <td>-7.019</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0313</td>\n",
              "      <td>166.004</td>\n",
              "      <td>4</td>\n",
              "      <td>0.462</td>\n",
              "      <td>This I Promise You - Radio Edit__NSYNC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04790</td>\n",
              "      <td>0.647</td>\n",
              "      <td>191280</td>\n",
              "      <td>0.870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>-4.702</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0798</td>\n",
              "      <td>165.103</td>\n",
              "      <td>4</td>\n",
              "      <td>0.906</td>\n",
              "      <td>It's Gonna Be Me__NSYNC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.44000</td>\n",
              "      <td>0.468</td>\n",
              "      <td>243493</td>\n",
              "      <td>0.535</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>-8.264</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0436</td>\n",
              "      <td>168.001</td>\n",
              "      <td>4</td>\n",
              "      <td>0.298</td>\n",
              "      <td>God Must Have Spent a Little More Time on You ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.07430</td>\n",
              "      <td>0.754</td>\n",
              "      <td>199787</td>\n",
              "      <td>0.946</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>8</td>\n",
              "      <td>0.4010</td>\n",
              "      <td>-2.149</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0403</td>\n",
              "      <td>112.042</td>\n",
              "      <td>4</td>\n",
              "      <td>0.865</td>\n",
              "      <td>I Want You Back - Radio Edit__NSYNC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>0.26700</td>\n",
              "      <td>0.563</td>\n",
              "      <td>315787</td>\n",
              "      <td>0.847</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>-6.190</td>\n",
              "      <td>1</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>82.757</td>\n",
              "      <td>4</td>\n",
              "      <td>0.810</td>\n",
              "      <td>Can I Touch You...There?__Michael Bolton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>0.00574</td>\n",
              "      <td>0.682</td>\n",
              "      <td>192000</td>\n",
              "      <td>0.812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0553</td>\n",
              "      <td>-2.691</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1180</td>\n",
              "      <td>104.891</td>\n",
              "      <td>4</td>\n",
              "      <td>0.347</td>\n",
              "      <td>Shape of You (Major Lazer Remix) [feat. Nyla &amp;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>0.02020</td>\n",
              "      <td>0.333</td>\n",
              "      <td>174240</td>\n",
              "      <td>0.913</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1460</td>\n",
              "      <td>-3.722</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0808</td>\n",
              "      <td>99.889</td>\n",
              "      <td>4</td>\n",
              "      <td>0.129</td>\n",
              "      <td>Paris - Pegboard Nerds Remix__The Chainsmokers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>0.00087</td>\n",
              "      <td>0.465</td>\n",
              "      <td>265600</td>\n",
              "      <td>0.953</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>8</td>\n",
              "      <td>0.2260</td>\n",
              "      <td>-4.684</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2950</td>\n",
              "      <td>160.064</td>\n",
              "      <td>4</td>\n",
              "      <td>0.554</td>\n",
              "      <td>Don't Let Me Down - Zomboy Remix__The Chainsmo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.00262</td>\n",
              "      <td>0.484</td>\n",
              "      <td>224320</td>\n",
              "      <td>0.609</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11</td>\n",
              "      <td>0.3390</td>\n",
              "      <td>-5.652</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0303</td>\n",
              "      <td>131.049</td>\n",
              "      <td>4</td>\n",
              "      <td>0.266</td>\n",
              "      <td>Something Just Like This - ARMNHMR Remix__The ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>162 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     acousticness  danceability  duration_ms  energy  instrumentalness  key  \\\n",
              "1         0.01980         0.571       199640   0.870          0.000007    8   \n",
              "2         0.30000         0.550       266293   0.563          0.000000    2   \n",
              "3         0.04790         0.647       191280   0.870          0.000000    0   \n",
              "4         0.44000         0.468       243493   0.535          0.000000   10   \n",
              "5         0.07430         0.754       199787   0.946          0.000391    8   \n",
              "..            ...           ...          ...     ...               ...  ...   \n",
              "158       0.26700         0.563       315787   0.847          0.000000    0   \n",
              "159       0.00574         0.682       192000   0.812          0.000000    1   \n",
              "160       0.02020         0.333       174240   0.913          0.000000    2   \n",
              "161       0.00087         0.465       265600   0.953          0.014600    8   \n",
              "162       0.00262         0.484       224320   0.609          0.000000   11   \n",
              "\n",
              "     liveness  loudness  mode  speechiness    tempo  time_signature  valence  \\\n",
              "1      0.0564    -3.877     0       0.0641  172.716               4    0.815   \n",
              "2      0.1220    -7.019     1       0.0313  166.004               4    0.462   \n",
              "3      0.0485    -4.702     0       0.0798  165.103               4    0.906   \n",
              "4      0.1010    -8.264     1       0.0436  168.001               4    0.298   \n",
              "5      0.4010    -2.149     0       0.0403  112.042               4    0.865   \n",
              "..        ...       ...   ...          ...      ...             ...      ...   \n",
              "158    0.2430    -6.190     1       0.3250   82.757               4    0.810   \n",
              "159    0.0553    -2.691     0       0.1180  104.891               4    0.347   \n",
              "160    0.1460    -3.722     1       0.0808   99.889               4    0.129   \n",
              "161    0.2260    -4.684     0       0.2950  160.064               4    0.554   \n",
              "162    0.3390    -5.652     0       0.0303  131.049               4    0.266   \n",
              "\n",
              "                                            track_name  \n",
              "1                                   Bye Bye Bye__NSYNC  \n",
              "2               This I Promise You - Radio Edit__NSYNC  \n",
              "3                              It's Gonna Be Me__NSYNC  \n",
              "4    God Must Have Spent a Little More Time on You ...  \n",
              "5                  I Want You Back - Radio Edit__NSYNC  \n",
              "..                                                 ...  \n",
              "158           Can I Touch You...There?__Michael Bolton  \n",
              "159  Shape of You (Major Lazer Remix) [feat. Nyla &...  \n",
              "160     Paris - Pegboard Nerds Remix__The Chainsmokers  \n",
              "161  Don't Let Me Down - Zomboy Remix__The Chainsmo...  \n",
              "162  Something Just Like This - ARMNHMR Remix__The ...  \n",
              "\n",
              "[162 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NYYWlAg0DEEv",
        "outputId": "f115989e-a6c4-40f4-f2b1-ee97cc5baa43"
      },
      "source": [
        "ratings = ratings.set_axis(track_ft.track_name,axis='columns')\n",
        "ratings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>track_name</th>\n",
              "      <th>Bye Bye Bye__NSYNC</th>\n",
              "      <th>This I Promise You - Radio Edit__NSYNC</th>\n",
              "      <th>It's Gonna Be Me__NSYNC</th>\n",
              "      <th>God Must Have Spent a Little More Time on You - Remix__NSYNC</th>\n",
              "      <th>I Want You Back - Radio Edit__NSYNC</th>\n",
              "      <th>Pop - Radio Version__NSYNC</th>\n",
              "      <th>Tearin' up My Heart - Radio Edit__NSYNC</th>\n",
              "      <th>Ciao Adios__Anne-Marie</th>\n",
              "      <th>Gary's Theme - Remastered__Bill Evans</th>\n",
              "      <th>Levels - Radio Edit__Avicii</th>\n",
              "      <th>...</th>\n",
              "      <th>SexyBack__Justin Timberlake</th>\n",
              "      <th>Fantasy__Mariah Carey</th>\n",
              "      <th>Remember the Time__Michael Jackson</th>\n",
              "      <th>Against All Odds (Take a Look at Me Now)__Mariah Carey</th>\n",
              "      <th>Earth Song - Remastered Version__Michael Jackson</th>\n",
              "      <th>Can I Touch You...There?__Michael Bolton</th>\n",
              "      <th>Shape of You (Major Lazer Remix) [feat. Nyla &amp; Kranium]__Ed Sheeran</th>\n",
              "      <th>Paris - Pegboard Nerds Remix__The Chainsmokers</th>\n",
              "      <th>Don't Let Me Down - Zomboy Remix__The Chainsmokers</th>\n",
              "      <th>Something Just Like This - ARMNHMR Remix__The Chainsmokers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>도윤</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_Herng</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤__Melanie</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_JInju</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_Maya</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_MI</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_Geraldine</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>도윤_Tiffany</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>형록1</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>형록2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>헝록3</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>형록4</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>창현_승련</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>창현</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>창현_세영</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>다연1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>다연2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>다연3</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>나윤1</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>나윤2</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>나윤3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>나윤4</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>효진1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>효진2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>효진3</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>근호</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>주영</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>윤지현</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>금지헌</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29 rows × 162 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "track_name    Bye Bye Bye__NSYNC  This I Promise You - Radio Edit__NSYNC  \\\n",
              "도윤                             1                                       0   \n",
              "도윤_Herng                       1                                       0   \n",
              "도윤__Melanie                   -1                                      -1   \n",
              "도윤_JInju                       1                                      -1   \n",
              "도윤_Maya                        1                                       1   \n",
              "도윤_MI                          1                                       0   \n",
              "도윤_Geraldine                   1                                       1   \n",
              "도윤_Tiffany                     0                                       0   \n",
              "형록1                           -1                                      -1   \n",
              "형록2                            0                                       0   \n",
              "헝록3                           -1                                      -1   \n",
              "형록4                            1                                      -1   \n",
              "창현_승련                          0                                       0   \n",
              "창현                             0                                       0   \n",
              "창현_세영                          0                                       0   \n",
              "다연1                            0                                       0   \n",
              "다연2                            0                                       0   \n",
              "다연3                            1                                      -1   \n",
              "나윤1                           -1                                      -1   \n",
              "나윤2                           -1                                      -1   \n",
              "나윤3                            1                                       0   \n",
              "나윤4                           -1                                      -1   \n",
              "효진1                            1                                       0   \n",
              "효진2                            1                                       0   \n",
              "효진3                            1                                      -1   \n",
              "근호                            -1                                      -1   \n",
              "주영                             0                                       1   \n",
              "윤지현                            0                                       0   \n",
              "금지헌                            1                                       1   \n",
              "\n",
              "track_name    It's Gonna Be Me__NSYNC  \\\n",
              "도윤                                  1   \n",
              "도윤_Herng                            1   \n",
              "도윤__Melanie                        -1   \n",
              "도윤_JInju                            1   \n",
              "도윤_Maya                             0   \n",
              "도윤_MI                               0   \n",
              "도윤_Geraldine                        0   \n",
              "도윤_Tiffany                          0   \n",
              "형록1                                -1   \n",
              "형록2                                 1   \n",
              "헝록3                                -1   \n",
              "형록4                                -1   \n",
              "창현_승련                               0   \n",
              "창현                                  0   \n",
              "창현_세영                               0   \n",
              "다연1                                 0   \n",
              "다연2                                 0   \n",
              "다연3                                 0   \n",
              "나윤1                                 1   \n",
              "나윤2                                 1   \n",
              "나윤3                                 1   \n",
              "나윤4                                -1   \n",
              "효진1                                 0   \n",
              "효진2                                 0   \n",
              "효진3                                 0   \n",
              "근호                                 -1   \n",
              "주영                                  1   \n",
              "윤지현                                 0   \n",
              "금지헌                                 1   \n",
              "\n",
              "track_name    God Must Have Spent a Little More Time on You - Remix__NSYNC  \\\n",
              "도윤                                                            1              \n",
              "도윤_Herng                                                      0              \n",
              "도윤__Melanie                                                  -1              \n",
              "도윤_JInju                                                     -1              \n",
              "도윤_Maya                                                       1              \n",
              "도윤_MI                                                         0              \n",
              "도윤_Geraldine                                                  0              \n",
              "도윤_Tiffany                                                    0              \n",
              "형록1                                                          -1              \n",
              "형록2                                                           1              \n",
              "헝록3                                                          -1              \n",
              "형록4                                                          -1              \n",
              "창현_승련                                                         0              \n",
              "창현                                                            0              \n",
              "창현_세영                                                         0              \n",
              "다연1                                                           0              \n",
              "다연2                                                           1              \n",
              "다연3                                                           0              \n",
              "나윤1                                                          -1              \n",
              "나윤2                                                           1              \n",
              "나윤3                                                           0              \n",
              "나윤4                                                          -1              \n",
              "효진1                                                          -1              \n",
              "효진2                                                          -1              \n",
              "효진3                                                          -1              \n",
              "근호                                                           -1              \n",
              "주영                                                            0              \n",
              "윤지현                                                           0              \n",
              "금지헌                                                           0              \n",
              "\n",
              "track_name    I Want You Back - Radio Edit__NSYNC  Pop - Radio Version__NSYNC  \\\n",
              "도윤                                              0                           1   \n",
              "도윤_Herng                                        0                           0   \n",
              "도윤__Melanie                                    -1                          -1   \n",
              "도윤_JInju                                        1                          -1   \n",
              "도윤_Maya                                         1                           0   \n",
              "도윤_MI                                           0                           0   \n",
              "도윤_Geraldine                                    1                           0   \n",
              "도윤_Tiffany                                      0                           0   \n",
              "형록1                                            -1                          -1   \n",
              "형록2                                             0                           1   \n",
              "헝록3                                            -1                          -1   \n",
              "형록4                                            -1                           1   \n",
              "창현_승련                                           0                           0   \n",
              "창현                                              0                           0   \n",
              "창현_세영                                           0                           0   \n",
              "다연1                                             0                           0   \n",
              "다연2                                             0                           1   \n",
              "다연3                                             0                          -1   \n",
              "나윤1                                             1                          -1   \n",
              "나윤2                                             1                           1   \n",
              "나윤3                                             1                          -1   \n",
              "나윤4                                             1                          -1   \n",
              "효진1                                             1                           0   \n",
              "효진2                                             1                           0   \n",
              "효진3                                             1                           0   \n",
              "근호                                             -1                          -1   \n",
              "주영                                              0                           0   \n",
              "윤지현                                             0                           0   \n",
              "금지헌                                             0                           0   \n",
              "\n",
              "track_name    Tearin' up My Heart - Radio Edit__NSYNC  Ciao Adios__Anne-Marie  \\\n",
              "도윤                                                  1                       0   \n",
              "도윤_Herng                                            0                      -1   \n",
              "도윤__Melanie                                        -1                       1   \n",
              "도윤_JInju                                            1                       1   \n",
              "도윤_Maya                                             1                       1   \n",
              "도윤_MI                                               0                       0   \n",
              "도윤_Geraldine                                        0                       0   \n",
              "도윤_Tiffany                                          0                       1   \n",
              "형록1                                                 0                       1   \n",
              "형록2                                                 0                      -1   \n",
              "헝록3                                                -1                      -1   \n",
              "형록4                                                 1                       1   \n",
              "창현_승련                                               0                       0   \n",
              "창현                                                  0                       0   \n",
              "창현_세영                                               0                       1   \n",
              "다연1                                                 0                       1   \n",
              "다연2                                                -1                       1   \n",
              "다연3                                                 1                       1   \n",
              "나윤1                                                 1                       1   \n",
              "나윤2                                                 1                       1   \n",
              "나윤3                                                 0                       1   \n",
              "나윤4                                                 0                       1   \n",
              "효진1                                                 1                       1   \n",
              "효진2                                                 1                       1   \n",
              "효진3                                                 0                       1   \n",
              "근호                                                  1                       1   \n",
              "주영                                                  1                      -1   \n",
              "윤지현                                                 0                       1   \n",
              "금지헌                                                 1                       0   \n",
              "\n",
              "track_name    Gary's Theme - Remastered__Bill Evans  \\\n",
              "도윤                                                1   \n",
              "도윤_Herng                                          0   \n",
              "도윤__Melanie                                       1   \n",
              "도윤_JInju                                         -1   \n",
              "도윤_Maya                                           1   \n",
              "도윤_MI                                             0   \n",
              "도윤_Geraldine                                      0   \n",
              "도윤_Tiffany                                        0   \n",
              "형록1                                               1   \n",
              "형록2                                              -1   \n",
              "헝록3                                               0   \n",
              "형록4                                               1   \n",
              "창현_승련                                             0   \n",
              "창현                                                0   \n",
              "창현_세영                                             0   \n",
              "다연1                                               0   \n",
              "다연2                                               0   \n",
              "다연3                                               1   \n",
              "나윤1                                              -1   \n",
              "나윤2                                              -1   \n",
              "나윤3                                               1   \n",
              "나윤4                                               0   \n",
              "효진1                                               0   \n",
              "효진2                                               0   \n",
              "효진3                                               0   \n",
              "근호                                               -1   \n",
              "주영                                                0   \n",
              "윤지현                                               0   \n",
              "금지헌                                               0   \n",
              "\n",
              "track_name    Levels - Radio Edit__Avicii  ...  SexyBack__Justin Timberlake  \\\n",
              "도윤                                      0  ...                            1   \n",
              "도윤_Herng                                1  ...                            1   \n",
              "도윤__Melanie                             1  ...                            1   \n",
              "도윤_JInju                                1  ...                            1   \n",
              "도윤_Maya                                 1  ...                            1   \n",
              "도윤_MI                                   0  ...                            1   \n",
              "도윤_Geraldine                            0  ...                            1   \n",
              "도윤_Tiffany                              1  ...                            1   \n",
              "형록1                                     1  ...                           -1   \n",
              "형록2                                     1  ...                           -1   \n",
              "헝록3                                     1  ...                            1   \n",
              "형록4                                     1  ...                            1   \n",
              "창현_승련                                   1  ...                            1   \n",
              "창현                                      1  ...                            1   \n",
              "창현_세영                                   1  ...                            1   \n",
              "다연1                                     0  ...                           -1   \n",
              "다연2                                     1  ...                            0   \n",
              "다연3                                     1  ...                            0   \n",
              "나윤1                                    -1  ...                            1   \n",
              "나윤2                                     1  ...                            0   \n",
              "나윤3                                    -1  ...                            0   \n",
              "나윤4                                     1  ...                            1   \n",
              "효진1                                     1  ...                            1   \n",
              "효진2                                    -1  ...                            0   \n",
              "효진3                                     1  ...                            0   \n",
              "근호                                      1  ...                           -1   \n",
              "주영                                      1  ...                            1   \n",
              "윤지현                                     0  ...                           -1   \n",
              "금지헌                                    -1  ...                           -1   \n",
              "\n",
              "track_name    Fantasy__Mariah Carey  Remember the Time__Michael Jackson  \\\n",
              "도윤                                1                                   1   \n",
              "도윤_Herng                          0                                   0   \n",
              "도윤__Melanie                       1                                   1   \n",
              "도윤_JInju                          0                                  -1   \n",
              "도윤_Maya                           1                                   1   \n",
              "도윤_MI                             1                                   1   \n",
              "도윤_Geraldine                      0                                   0   \n",
              "도윤_Tiffany                        1                                   1   \n",
              "형록1                              -1                                   1   \n",
              "형록2                               1                                   1   \n",
              "헝록3                              -1                                   1   \n",
              "형록4                               1                                   1   \n",
              "창현_승련                            -1                                   0   \n",
              "창현                               -1                                   0   \n",
              "창현_세영                             1                                   1   \n",
              "다연1                               1                                   1   \n",
              "다연2                              -1                                  -1   \n",
              "다연3                               1                                  -1   \n",
              "나윤1                              -1                                   1   \n",
              "나윤2                               1                                  -1   \n",
              "나윤3                               0                                   1   \n",
              "나윤4                               1                                   1   \n",
              "효진1                               0                                   1   \n",
              "효진2                               0                                  -1   \n",
              "효진3                               1                                   1   \n",
              "근호                               -1                                  -1   \n",
              "주영                                1                                   1   \n",
              "윤지현                               0                                   0   \n",
              "금지헌                               0                                   0   \n",
              "\n",
              "track_name    Against All Odds (Take a Look at Me Now)__Mariah Carey  \\\n",
              "도윤                                                            1        \n",
              "도윤_Herng                                                      0        \n",
              "도윤__Melanie                                                  -1        \n",
              "도윤_JInju                                                      0        \n",
              "도윤_Maya                                                       1        \n",
              "도윤_MI                                                         1        \n",
              "도윤_Geraldine                                                  1        \n",
              "도윤_Tiffany                                                    1        \n",
              "형록1                                                          -1        \n",
              "형록2                                                           0        \n",
              "헝록3                                                          -1        \n",
              "형록4                                                           1        \n",
              "창현_승련                                                        -1        \n",
              "창현                                                           -1        \n",
              "창현_세영                                                         1        \n",
              "다연1                                                           1        \n",
              "다연2                                                           0        \n",
              "다연3                                                           1        \n",
              "나윤1                                                           0        \n",
              "나윤2                                                          -1        \n",
              "나윤3                                                          -1        \n",
              "나윤4                                                           1        \n",
              "효진1                                                          -1        \n",
              "효진2                                                          -1        \n",
              "효진3                                                           1        \n",
              "근호                                                           -1        \n",
              "주영                                                            1        \n",
              "윤지현                                                           0        \n",
              "금지헌                                                           0        \n",
              "\n",
              "track_name    Earth Song - Remastered Version__Michael Jackson  \\\n",
              "도윤                                                           0   \n",
              "도윤_Herng                                                     1   \n",
              "도윤__Melanie                                                  1   \n",
              "도윤_JInju                                                     1   \n",
              "도윤_Maya                                                      1   \n",
              "도윤_MI                                                        1   \n",
              "도윤_Geraldine                                                 1   \n",
              "도윤_Tiffany                                                   1   \n",
              "형록1                                                         -1   \n",
              "형록2                                                          1   \n",
              "헝록3                                                         -1   \n",
              "형록4                                                          1   \n",
              "창현_승련                                                        0   \n",
              "창현                                                           0   \n",
              "창현_세영                                                        1   \n",
              "다연1                                                          1   \n",
              "다연2                                                          0   \n",
              "다연3                                                          0   \n",
              "나윤1                                                         -1   \n",
              "나윤2                                                         -1   \n",
              "나윤3                                                         -1   \n",
              "나윤4                                                         -1   \n",
              "효진1                                                         -1   \n",
              "효진2                                                         -1   \n",
              "효진3                                                          1   \n",
              "근호                                                          -1   \n",
              "주영                                                           1   \n",
              "윤지현                                                          0   \n",
              "금지헌                                                          0   \n",
              "\n",
              "track_name    Can I Touch You...There?__Michael Bolton  \\\n",
              "도윤                                                   0   \n",
              "도윤_Herng                                             0   \n",
              "도윤__Melanie                                         -1   \n",
              "도윤_JInju                                            -1   \n",
              "도윤_Maya                                             -1   \n",
              "도윤_MI                                               -1   \n",
              "도윤_Geraldine                                         0   \n",
              "도윤_Tiffany                                           0   \n",
              "형록1                                                 -1   \n",
              "형록2                                                  0   \n",
              "헝록3                                                  1   \n",
              "형록4                                                  1   \n",
              "창현_승련                                                0   \n",
              "창현                                                   0   \n",
              "창현_세영                                                1   \n",
              "다연1                                                  0   \n",
              "다연2                                                  0   \n",
              "다연3                                                  0   \n",
              "나윤1                                                 -1   \n",
              "나윤2                                                 -1   \n",
              "나윤3                                                 -1   \n",
              "나윤4                                                 -1   \n",
              "효진1                                                  0   \n",
              "효진2                                                  1   \n",
              "효진3                                                  0   \n",
              "근호                                                  -1   \n",
              "주영                                                   1   \n",
              "윤지현                                                  0   \n",
              "금지헌                                                  0   \n",
              "\n",
              "track_name    Shape of You (Major Lazer Remix) [feat. Nyla & Kranium]__Ed Sheeran  \\\n",
              "도윤                                                            0                     \n",
              "도윤_Herng                                                      1                     \n",
              "도윤__Melanie                                                  -1                     \n",
              "도윤_JInju                                                      1                     \n",
              "도윤_Maya                                                       0                     \n",
              "도윤_MI                                                         1                     \n",
              "도윤_Geraldine                                                  1                     \n",
              "도윤_Tiffany                                                    1                     \n",
              "형록1                                                           1                     \n",
              "형록2                                                           1                     \n",
              "헝록3                                                           1                     \n",
              "형록4                                                           1                     \n",
              "창현_승련                                                        -1                     \n",
              "창현                                                           -1                     \n",
              "창현_세영                                                         1                     \n",
              "다연1                                                           1                     \n",
              "다연2                                                           1                     \n",
              "다연3                                                           1                     \n",
              "나윤1                                                           1                     \n",
              "나윤2                                                           1                     \n",
              "나윤3                                                           1                     \n",
              "나윤4                                                           1                     \n",
              "효진1                                                           1                     \n",
              "효진2                                                          -1                     \n",
              "효진3                                                           1                     \n",
              "근호                                                           -1                     \n",
              "주영                                                            1                     \n",
              "윤지현                                                           0                     \n",
              "금지헌                                                           1                     \n",
              "\n",
              "track_name    Paris - Pegboard Nerds Remix__The Chainsmokers  \\\n",
              "도윤                                                         1   \n",
              "도윤_Herng                                                   1   \n",
              "도윤__Melanie                                               -1   \n",
              "도윤_JInju                                                  -1   \n",
              "도윤_Maya                                                    0   \n",
              "도윤_MI                                                      1   \n",
              "도윤_Geraldine                                               1   \n",
              "도윤_Tiffany                                                 1   \n",
              "형록1                                                        1   \n",
              "형록2                                                        1   \n",
              "헝록3                                                        1   \n",
              "형록4                                                        1   \n",
              "창현_승련                                                      1   \n",
              "창현                                                         1   \n",
              "창현_세영                                                      1   \n",
              "다연1                                                        1   \n",
              "다연2                                                        1   \n",
              "다연3                                                        1   \n",
              "나윤1                                                       -1   \n",
              "나윤2                                                       -1   \n",
              "나윤3                                                       -1   \n",
              "나윤4                                                        1   \n",
              "효진1                                                        1   \n",
              "효진2                                                        1   \n",
              "효진3                                                       -1   \n",
              "근호                                                        -1   \n",
              "주영                                                         1   \n",
              "윤지현                                                        0   \n",
              "금지헌                                                        1   \n",
              "\n",
              "track_name    Don't Let Me Down - Zomboy Remix__The Chainsmokers  \\\n",
              "도윤                                                            1    \n",
              "도윤_Herng                                                      1    \n",
              "도윤__Melanie                                                  -1    \n",
              "도윤_JInju                                                     -1    \n",
              "도윤_Maya                                                      -1    \n",
              "도윤_MI                                                         1    \n",
              "도윤_Geraldine                                                  0    \n",
              "도윤_Tiffany                                                    1    \n",
              "형록1                                                           1    \n",
              "형록2                                                           1    \n",
              "헝록3                                                           1    \n",
              "형록4                                                           1    \n",
              "창현_승련                                                         1    \n",
              "창현                                                            1    \n",
              "창현_세영                                                         1    \n",
              "다연1                                                           1    \n",
              "다연2                                                           1    \n",
              "다연3                                                          -1    \n",
              "나윤1                                                           1    \n",
              "나윤2                                                           1    \n",
              "나윤3                                                           0    \n",
              "나윤4                                                           1    \n",
              "효진1                                                           1    \n",
              "효진2                                                           1    \n",
              "효진3                                                           0    \n",
              "근호                                                           -1    \n",
              "주영                                                            1    \n",
              "윤지현                                                           0    \n",
              "금지헌                                                          -1    \n",
              "\n",
              "track_name    Something Just Like This - ARMNHMR Remix__The Chainsmokers  \n",
              "도윤                                                            1           \n",
              "도윤_Herng                                                      1           \n",
              "도윤__Melanie                                                  -1           \n",
              "도윤_JInju                                                      1           \n",
              "도윤_Maya                                                       0           \n",
              "도윤_MI                                                         1           \n",
              "도윤_Geraldine                                                  1           \n",
              "도윤_Tiffany                                                    1           \n",
              "형록1                                                           1           \n",
              "형록2                                                           1           \n",
              "헝록3                                                           1           \n",
              "형록4                                                           1           \n",
              "창현_승련                                                         1           \n",
              "창현                                                            1           \n",
              "창현_세영                                                         1           \n",
              "다연1                                                           1           \n",
              "다연2                                                           1           \n",
              "다연3                                                           1           \n",
              "나윤1                                                           1           \n",
              "나윤2                                                           1           \n",
              "나윤3                                                           1           \n",
              "나윤4                                                           1           \n",
              "효진1                                                           1           \n",
              "효진2                                                           1           \n",
              "효진3                                                           1           \n",
              "근호                                                            1           \n",
              "주영                                                            1           \n",
              "윤지현                                                           1           \n",
              "금지헌                                                           1           \n",
              "\n",
              "[29 rows x 162 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iuf-0B-wDEEw",
        "outputId": "2e28d0f7-7cf6-49c4-8662-899522d0f7e0"
      },
      "source": [
        "# track,index\n",
        "\n",
        "track2idx={name:i+1 for i, name in enumerate(track_ft.track_name)}\n",
        "idx2track={i+1:name for i, name in enumerate(track_ft.track_name)}\n",
        "\n",
        "user2idx={name:i+1 for i, name in enumerate(ratings.index)}\n",
        "idx2user={i+1:name for i, name in enumerate(ratings.index)}\n",
        "\n",
        "\n",
        "display(track2idx,user2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'Bye Bye Bye__NSYNC': 1,\n",
              " 'This I Promise You - Radio Edit__NSYNC': 2,\n",
              " \"It's Gonna Be Me__NSYNC\": 3,\n",
              " 'God Must Have Spent a Little More Time on You - Remix__NSYNC': 4,\n",
              " 'I Want You Back - Radio Edit__NSYNC': 5,\n",
              " 'Pop - Radio Version__NSYNC': 6,\n",
              " \"Tearin' up My Heart - Radio Edit__NSYNC\": 7,\n",
              " 'Ciao Adios__Anne-Marie': 8,\n",
              " \"Gary's Theme - Remastered__Bill Evans\": 9,\n",
              " 'Levels - Radio Edit__Avicii': 10,\n",
              " 'On Bended Knee__Boyz II Men': 11,\n",
              " 'End Of The Road__Boyz II Men': 12,\n",
              " \"I'll Make Love To You__Boyz II Men\": 13,\n",
              " 'Water Runs Dry__Boyz II Men': 14,\n",
              " 'A Song For Mama__Boyz II Men': 15,\n",
              " 'As Long as You Love Me__Backstreet Boys': 16,\n",
              " 'Quit Playing Games (With My Heart)__Backstreet Boys': 17,\n",
              " \"I'll Never Break Your Heart__Backstreet Boys\": 18,\n",
              " 'All I Have to Give__Backstreet Boys': 19,\n",
              " 'The Call__Backstreet Boys': 20,\n",
              " 'I Want It That Way__Backstreet Boys': 27,\n",
              " 'Show Me the Meaning of Being Lonely__Backstreet Boys': 22,\n",
              " 'Shape of My Heart__Backstreet Boys': 23,\n",
              " 'Take Me There - Remix__Blackstreet': 24,\n",
              " 'If I Were a Boy__Beyoncé': 25,\n",
              " '...Baby One More Time__Britney Spears': 26,\n",
              " '(You Drive Me) Crazy__Britney Spears': 28,\n",
              " \"Everybody (Backstreet's Back) - Radio Edit__Backstreet Boys\": 29,\n",
              " 'Complicated__Avril Lavigne': 30,\n",
              " 'Lucky__Britney Spears': 31,\n",
              " 'Childs Play__Drake': 32,\n",
              " 'Jumpman__Drake': 33,\n",
              " 'Monster__Kanye West': 34,\n",
              " 'Return Of The Mack - C & J Street Mix__Mark Morrison': 35,\n",
              " \"I Know There's Gonna Be (Good Times)__Jamie xx\": 39,\n",
              " 'Loud Places__Jamie xx': 37,\n",
              " \"Sugah Daddy__D'Angelo\": 38,\n",
              " 'Hotline Bling__Drake': 40,\n",
              " 'Kush__Lil Wayne': 41,\n",
              " 'All Day__Kanye West': 42,\n",
              " 'Big Rings__Drake': 43,\n",
              " 'Know Yourself__Drake': 44,\n",
              " 'Wu-Tang Forever__Drake': 45,\n",
              " 'Fame (2007 Remastered)__David Bowie': 46,\n",
              " 'New Slaves__Kanye West': 47,\n",
              " 'Mercy__Kanye West': 48,\n",
              " 'Double Bubble Trouble__M.I.A.': 49,\n",
              " 'Blood On The Leaves__Kanye West': 50,\n",
              " 'Upper Echelon__Travis Scott': 51,\n",
              " 'Got To Give It Up (Part 1)__Marvin Gaye': 52,\n",
              " 'I Want You - Single Version__Marvin Gaye': 53,\n",
              " 'Swords__M.I.A.': 54,\n",
              " 'Antidote__Travis Scott': 55,\n",
              " 'Unbreakable__Janet Jackson': 56,\n",
              " 'Mamacita__Travis Scott': 57,\n",
              " 'I Wish__Stevie Wonder': 58,\n",
              " 'Headlines__Drake': 59,\n",
              " 'Started From the Bottom__Drake': 60,\n",
              " 'Gone, Gone__Juicy J': 61,\n",
              " 'Get Lucky - Radio Edit__Daft Punk': 62,\n",
              " 'One of Those Nights__Juicy J': 63,\n",
              " 'Get Lucky - Daft Punk Remix__Daft Punk': 64,\n",
              " 'Bounce It__Juicy J': 65,\n",
              " 'I Wanna Be With You__DJ Khaled': 66,\n",
              " \"Hold On, We're Going Home__Drake\": 69,\n",
              " 'No New Friends - SFTB Remix__DJ Khaled': 68,\n",
              " 'Bound 2__Kanye West': 70,\n",
              " '0 To 100 / The Catch Up__Drake': 71,\n",
              " 'Mary Jane (Jamie Xx - Girl Remix)__Jamie xx': 72,\n",
              " 'No Woman__Whitney': 73,\n",
              " 'For Free__DJ Khaled': 74,\n",
              " 'Get Lucky__Daft Punk': 99,\n",
              " 'Black Skinhead__Kanye West': 76,\n",
              " \"Let's Dance (Single Version) [2002 Remastered Version]__David Bowie\": 77,\n",
              " 'Father Stretch My Hands Pt. 1__Kanye West': 78,\n",
              " 'Skepta Interlude__Drake': 79,\n",
              " 'No Vacancy__OneRepublic': 80,\n",
              " 'Malibu__Miley Cyrus': 81,\n",
              " 'Stay (with Alessia Cara)__Zedd': 82,\n",
              " 'Rollin__Calvin Harris': 83,\n",
              " 'Whippin (feat. Felix Snow)__Kiiara': 84,\n",
              " 'Break Up Every Night__The Chainsmokers': 85,\n",
              " 'The One__The Chainsmokers': 86,\n",
              " 'Passionfruit__Drake': 87,\n",
              " 'Shape of You__Ed Sheeran': 88,\n",
              " 'Wild Ones (feat. Sia)__Flo Rida': 89,\n",
              " 'Lollipop__Lil Wayne': 90,\n",
              " 'Clique__JAY Z': 91,\n",
              " 'Live Your Life - feat. Rihanna__T.I.': 92,\n",
              " 'Titanium (feat. Sia)__David Guetta': 93,\n",
              " 'GDFR (feat. Sage The Gemini & Lookas)__Flo Rida': 94,\n",
              " 'The Monster__Eminem': 95,\n",
              " 'Sexy Bitch (feat. Akon) - Featuring Akon;explicit__David Guetta': 96,\n",
              " 'Maps__Maroon 5': 97,\n",
              " 'Counting Stars__OneRepublic': 98,\n",
              " 'My House__Flo Rida': 100,\n",
              " 'Best I Ever Had__Drake': 101,\n",
              " 'Take Care__Drake': 102,\n",
              " 'Right In__Skrillex': 103,\n",
              " 'Bangarang (feat. Sirah)__Skrillex': 104,\n",
              " \"Breakn' A Sweat__Skrillex\": 105,\n",
              " \"The Devil's Den__Skrillex\": 106,\n",
              " 'Right On Time__Skrillex': 107,\n",
              " 'Kyoto (feat. Sirah)__Skrillex': 108,\n",
              " 'Skrillex Orchestral Suite By Varien - Bonus Track__Skrillex': 109,\n",
              " 'Nosedive__Dynamic Duo': 110,\n",
              " 'Loop__NELL': 111,\n",
              " 'What 2 Do__DEAN': 112,\n",
              " 'Chicken Soup__Skrillex': 113,\n",
              " 'Something Just Like This - Don Diablo Remix__The Chainsmokers': 114,\n",
              " \"I'm the One__DJ Khaled\": 115,\n",
              " 'Young__The Chainsmokers': 116,\n",
              " 'XO TOUR Llif3__Lil Uzi Vert': 117,\n",
              " 'Confident__Demi Lovato': 118,\n",
              " 'Cool for the Summer__Demi Lovato': 119,\n",
              " 'Old Ways__Demi Lovato': 120,\n",
              " 'For You__Demi Lovato': 121,\n",
              " 'Stone Cold__Demi Lovato': 122,\n",
              " 'Kingdom Come__Demi Lovato': 123,\n",
              " 'Waitin for You__Demi Lovato': 124,\n",
              " 'Wildfire__Demi Lovato': 125,\n",
              " 'String Quintet in C Major, Op. 29: II. Adagio molto espressivo__Ludwig van Beethoven': 126,\n",
              " 'You Make Me Wanna...__Usher': 127,\n",
              " 'Till I Collapse__Eminem': 128,\n",
              " '$ave Dat Money (feat. Fetty Wap & Rich Homie Quan)__Lil Dicky': 129,\n",
              " 'Cold__Maroon 5': 130,\n",
              " 'Popular Song__MIKA': 131,\n",
              " 'Together Again__Janet Jackson': 132,\n",
              " 'Someone To Call My Lover__Janet Jackson': 133,\n",
              " 'Escapade__Janet Jackson': 134,\n",
              " 'Stay__Ne-Yo': 135,\n",
              " 'So Sick__Ne-Yo': 136,\n",
              " \"When You're Mad__Ne-Yo\": 137,\n",
              " 'Sexy Love__Ne-Yo': 138,\n",
              " 'Slow Jam__Usher': 139,\n",
              " 'U Got It Bad__Usher': 140,\n",
              " 'My Boo__Usher': 141,\n",
              " \"Don't Love You No More (I'm Sorry)__Craig David\": 142,\n",
              " 'Billie Jean__Michael Jackson': 143,\n",
              " 'Beat It - Single Version__Michael Jackson': 144,\n",
              " 'Black or White - Single Version__Michael Jackson': 145,\n",
              " 'The Way You Make Me Feel__Michael Jackson': 146,\n",
              " 'Man In The Mirror__Michael Jackson': 147,\n",
              " 'P.Y.T. (Pretty Young Thing)__Michael Jackson': 148,\n",
              " 'With You__Chris Brown': 149,\n",
              " 'Forever__Chris Brown': 150,\n",
              " 'When Will My Life Begin - From \"Tangled\"/Soundtrack Version__Mandy Moore': 151,\n",
              " 'I See the Light - From \"Tangled\"/Soundtrack Version__Mandy Moore': 152,\n",
              " 'SexyBack__Justin Timberlake': 153,\n",
              " 'Fantasy__Mariah Carey': 154,\n",
              " 'Remember the Time__Michael Jackson': 155,\n",
              " 'Against All Odds (Take a Look at Me Now)__Mariah Carey': 156,\n",
              " 'Earth Song - Remastered Version__Michael Jackson': 157,\n",
              " 'Can I Touch You...There?__Michael Bolton': 158,\n",
              " 'Shape of You (Major Lazer Remix) [feat. Nyla & Kranium]__Ed Sheeran': 159,\n",
              " 'Paris - Pegboard Nerds Remix__The Chainsmokers': 160,\n",
              " \"Don't Let Me Down - Zomboy Remix__The Chainsmokers\": 161,\n",
              " 'Something Just Like This - ARMNHMR Remix__The Chainsmokers': 162}"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'도윤': 1,\n",
              " '도윤_Herng': 2,\n",
              " '도윤__Melanie': 3,\n",
              " '도윤_JInju': 4,\n",
              " '도윤_Maya': 5,\n",
              " '도윤_MI': 6,\n",
              " '도윤_Geraldine': 7,\n",
              " '도윤_Tiffany': 8,\n",
              " '형록1': 9,\n",
              " '형록2': 10,\n",
              " '헝록3': 11,\n",
              " '형록4': 12,\n",
              " '창현_승련': 13,\n",
              " '창현': 14,\n",
              " '창현_세영': 15,\n",
              " '다연1': 16,\n",
              " '다연2': 17,\n",
              " '다연3': 18,\n",
              " '나윤1': 19,\n",
              " '나윤2': 20,\n",
              " '나윤3': 21,\n",
              " '나윤4': 22,\n",
              " '효진1': 23,\n",
              " '효진2': 24,\n",
              " '효진3': 25,\n",
              " '근호': 26,\n",
              " '주영': 27,\n",
              " '윤지현': 28,\n",
              " '금지헌': 29}"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "D4cNjjdQDEEw",
        "outputId": "974d35bb-3ba0-492f-f810-df89da779574"
      },
      "source": [
        "users=[]\n",
        "for user in ratings.index:\n",
        "  users.extend([user]*len(ratings.columns))\n",
        " \n",
        "tracks = list(ratings.columns)*len(ratings.index)\n",
        "score=[]\n",
        "for i in range(len(ratings.index)):\n",
        "  for j in range(len(ratings.columns)):\n",
        "    score.append(float(ratings.iloc[i,j]))\n",
        "temp={'users':users,'tracks':tracks,'score':score}\n",
        "data = pd.DataFrame(temp)\n",
        "display(data.head(50))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>users</th>\n",
              "      <th>tracks</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Bye Bye Bye__NSYNC</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>도윤</td>\n",
              "      <td>This I Promise You - Radio Edit__NSYNC</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>도윤</td>\n",
              "      <td>It's Gonna Be Me__NSYNC</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>도윤</td>\n",
              "      <td>God Must Have Spent a Little More Time on You ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>도윤</td>\n",
              "      <td>I Want You Back - Radio Edit__NSYNC</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Pop - Radio Version__NSYNC</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Tearin' up My Heart - Radio Edit__NSYNC</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Ciao Adios__Anne-Marie</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Gary's Theme - Remastered__Bill Evans</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Levels - Radio Edit__Avicii</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>도윤</td>\n",
              "      <td>On Bended Knee__Boyz II Men</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>도윤</td>\n",
              "      <td>End Of The Road__Boyz II Men</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>도윤</td>\n",
              "      <td>I'll Make Love To You__Boyz II Men</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Water Runs Dry__Boyz II Men</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>도윤</td>\n",
              "      <td>A Song For Mama__Boyz II Men</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>도윤</td>\n",
              "      <td>As Long as You Love Me__Backstreet Boys</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Quit Playing Games (With My Heart)__Backstreet...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>도윤</td>\n",
              "      <td>I'll Never Break Your Heart__Backstreet Boys</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>도윤</td>\n",
              "      <td>All I Have to Give__Backstreet Boys</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>도윤</td>\n",
              "      <td>The Call__Backstreet Boys</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>도윤</td>\n",
              "      <td>I Want It That Way__Backstreet Boys</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Show Me the Meaning of Being Lonely__Backstree...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Shape of My Heart__Backstreet Boys</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Take Me There - Remix__Blackstreet</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>도윤</td>\n",
              "      <td>If I Were a Boy__Beyoncé</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>도윤</td>\n",
              "      <td>...Baby One More Time__Britney Spears</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>도윤</td>\n",
              "      <td>I Want It That Way__Backstreet Boys</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>도윤</td>\n",
              "      <td>(You Drive Me) Crazy__Britney Spears</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Everybody (Backstreet's Back) - Radio Edit__Ba...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Complicated__Avril Lavigne</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Lucky__Britney Spears</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Childs Play__Drake</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Jumpman__Drake</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Monster__Kanye West</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Return Of The Mack - C &amp; J Street Mix__Mark Mo...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>도윤</td>\n",
              "      <td>I Know There's Gonna Be (Good Times)__Jamie xx</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Loud Places__Jamie xx</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Sugah Daddy__D'Angelo</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>도윤</td>\n",
              "      <td>I Know There's Gonna Be (Good Times)__Jamie xx</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Hotline Bling__Drake</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Kush__Lil Wayne</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>도윤</td>\n",
              "      <td>All Day__Kanye West</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Big Rings__Drake</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Know Yourself__Drake</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Wu-Tang Forever__Drake</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Fame (2007 Remastered)__David Bowie</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>도윤</td>\n",
              "      <td>New Slaves__Kanye West</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Mercy__Kanye West</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Double Bubble Trouble__M.I.A.</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Blood On The Leaves__Kanye West</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   users                                             tracks  score\n",
              "0     도윤                                 Bye Bye Bye__NSYNC    1.0\n",
              "1     도윤             This I Promise You - Radio Edit__NSYNC    0.0\n",
              "2     도윤                            It's Gonna Be Me__NSYNC    1.0\n",
              "3     도윤  God Must Have Spent a Little More Time on You ...    1.0\n",
              "4     도윤                I Want You Back - Radio Edit__NSYNC    0.0\n",
              "5     도윤                         Pop - Radio Version__NSYNC    1.0\n",
              "6     도윤            Tearin' up My Heart - Radio Edit__NSYNC    1.0\n",
              "7     도윤                             Ciao Adios__Anne-Marie    0.0\n",
              "8     도윤              Gary's Theme - Remastered__Bill Evans    1.0\n",
              "9     도윤                        Levels - Radio Edit__Avicii    0.0\n",
              "10    도윤                        On Bended Knee__Boyz II Men    1.0\n",
              "11    도윤                       End Of The Road__Boyz II Men    1.0\n",
              "12    도윤                 I'll Make Love To You__Boyz II Men    1.0\n",
              "13    도윤                        Water Runs Dry__Boyz II Men    0.0\n",
              "14    도윤                       A Song For Mama__Boyz II Men    1.0\n",
              "15    도윤            As Long as You Love Me__Backstreet Boys    1.0\n",
              "16    도윤  Quit Playing Games (With My Heart)__Backstreet...    0.0\n",
              "17    도윤       I'll Never Break Your Heart__Backstreet Boys    0.0\n",
              "18    도윤                All I Have to Give__Backstreet Boys    1.0\n",
              "19    도윤                          The Call__Backstreet Boys    0.0\n",
              "20    도윤                I Want It That Way__Backstreet Boys    1.0\n",
              "21    도윤  Show Me the Meaning of Being Lonely__Backstree...    0.0\n",
              "22    도윤                 Shape of My Heart__Backstreet Boys    1.0\n",
              "23    도윤                 Take Me There - Remix__Blackstreet    0.0\n",
              "24    도윤                           If I Were a Boy__Beyoncé    0.0\n",
              "25    도윤              ...Baby One More Time__Britney Spears    1.0\n",
              "26    도윤                I Want It That Way__Backstreet Boys    1.0\n",
              "27    도윤               (You Drive Me) Crazy__Britney Spears    0.0\n",
              "28    도윤  Everybody (Backstreet's Back) - Radio Edit__Ba...    0.0\n",
              "29    도윤                         Complicated__Avril Lavigne    1.0\n",
              "30    도윤                              Lucky__Britney Spears    0.0\n",
              "31    도윤                                 Childs Play__Drake    0.0\n",
              "32    도윤                                     Jumpman__Drake    0.0\n",
              "33    도윤                                Monster__Kanye West    0.0\n",
              "34    도윤  Return Of The Mack - C & J Street Mix__Mark Mo...    0.0\n",
              "35    도윤     I Know There's Gonna Be (Good Times)__Jamie xx    0.0\n",
              "36    도윤                              Loud Places__Jamie xx    0.0\n",
              "37    도윤                              Sugah Daddy__D'Angelo    1.0\n",
              "38    도윤     I Know There's Gonna Be (Good Times)__Jamie xx    0.0\n",
              "39    도윤                               Hotline Bling__Drake    1.0\n",
              "40    도윤                                    Kush__Lil Wayne    0.0\n",
              "41    도윤                                All Day__Kanye West    0.0\n",
              "42    도윤                                   Big Rings__Drake    0.0\n",
              "43    도윤                               Know Yourself__Drake    0.0\n",
              "44    도윤                             Wu-Tang Forever__Drake    0.0\n",
              "45    도윤                Fame (2007 Remastered)__David Bowie    0.0\n",
              "46    도윤                             New Slaves__Kanye West    0.0\n",
              "47    도윤                                  Mercy__Kanye West    0.0\n",
              "48    도윤                      Double Bubble Trouble__M.I.A.    0.0\n",
              "49    도윤                    Blood On The Leaves__Kanye West    0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "x0k-QGu1DEEw",
        "outputId": "e4bee2af-8785-4af7-a6d2-dfa8c295b589"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "import csv\n",
        "reader = Reader(rating_scale=(-1, 1))\n",
        "TRAIN_SIZE = 0.8\n",
        "data = shuffle(data,random_state=3)\n",
        "data = shuffle(data,random_state=33)\n",
        "\n",
        "cutoff = int(TRAIN_SIZE * len(data))\n",
        "ratings_train = data.iloc[:cutoff]\n",
        "ratings_test = data.iloc[cutoff:]\n",
        "display(ratings_train,ratings_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>users</th>\n",
              "      <th>tracks</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>727</th>\n",
              "      <td>도윤_Maya</td>\n",
              "      <td>No Vacancy__OneRepublic</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4517</th>\n",
              "      <td>윤지현</td>\n",
              "      <td>Beat It - Single Version__Michael Jackson</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2813</th>\n",
              "      <td>다연3</td>\n",
              "      <td>Started From the Bottom__Drake</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1629</th>\n",
              "      <td>헝록3</td>\n",
              "      <td>Levels - Radio Edit__Avicii</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359</th>\n",
              "      <td>형록1</td>\n",
              "      <td>Get Lucky - Daft Punk Remix__Daft Punk</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1626</th>\n",
              "      <td>헝록3</td>\n",
              "      <td>Tearin' up My Heart - Radio Edit__NSYNC</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4102</th>\n",
              "      <td>근호</td>\n",
              "      <td>I Want You - Single Version__Marvin Gaye</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2189</th>\n",
              "      <td>창현</td>\n",
              "      <td>Whippin (feat. Felix Snow)__Kiiara</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2029</th>\n",
              "      <td>창현_승련</td>\n",
              "      <td>The One__The Chainsmokers</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>776</th>\n",
              "      <td>도윤_Maya</td>\n",
              "      <td>$ave Dat Money (feat. Fetty Wap &amp; Rich Homie Q...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3758 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        users                                             tracks  score\n",
              "727   도윤_Maya                            No Vacancy__OneRepublic   -1.0\n",
              "4517      윤지현          Beat It - Single Version__Michael Jackson    1.0\n",
              "2813      다연3                     Started From the Bottom__Drake   -1.0\n",
              "1629      헝록3                        Levels - Radio Edit__Avicii    1.0\n",
              "1359      형록1             Get Lucky - Daft Punk Remix__Daft Punk    1.0\n",
              "...       ...                                                ...    ...\n",
              "1626      헝록3            Tearin' up My Heart - Radio Edit__NSYNC   -1.0\n",
              "4102       근호           I Want You - Single Version__Marvin Gaye    1.0\n",
              "2189       창현                 Whippin (feat. Felix Snow)__Kiiara    0.0\n",
              "2029    창현_승련                          The One__The Chainsmokers    1.0\n",
              "776   도윤_Maya  $ave Dat Money (feat. Fetty Wap & Rich Homie Q...    0.0\n",
              "\n",
              "[3758 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>users</th>\n",
              "      <th>tracks</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>도윤</td>\n",
              "      <td>Beat It - Single Version__Michael Jackson</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2213</th>\n",
              "      <td>창현</td>\n",
              "      <td>Kyoto (feat. Sirah)__Skrillex</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>962</th>\n",
              "      <td>도윤_MI</td>\n",
              "      <td>SexyBack__Justin Timberlake</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>도윤</td>\n",
              "      <td>My Boo__Usher</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3365</th>\n",
              "      <td>나윤3</td>\n",
              "      <td>String Quintet in C Major, Op. 29: II. Adagio ...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208</th>\n",
              "      <td>도윤_Tiffany</td>\n",
              "      <td>Get Lucky__Daft Punk</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1699</th>\n",
              "      <td>헝록3</td>\n",
              "      <td>No Vacancy__OneRepublic</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2328</th>\n",
              "      <td>창현_세영</td>\n",
              "      <td>Gone, Gone__Juicy J</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1569</th>\n",
              "      <td>형록2</td>\n",
              "      <td>What 2 Do__DEAN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2157</th>\n",
              "      <td>창현</td>\n",
              "      <td>Got To Give It Up (Part 1)__Marvin Gaye</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>940 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           users                                             tracks  score\n",
              "143           도윤          Beat It - Single Version__Michael Jackson    1.0\n",
              "2213          창현                      Kyoto (feat. Sirah)__Skrillex    1.0\n",
              "962        도윤_MI                        SexyBack__Justin Timberlake    1.0\n",
              "140           도윤                                      My Boo__Usher    1.0\n",
              "3365         나윤3  String Quintet in C Major, Op. 29: II. Adagio ...   -1.0\n",
              "...          ...                                                ...    ...\n",
              "1208  도윤_Tiffany                               Get Lucky__Daft Punk    0.0\n",
              "1699         헝록3                            No Vacancy__OneRepublic    1.0\n",
              "2328       창현_세영                                Gone, Gone__Juicy J   -1.0\n",
              "1569         형록2                                    What 2 Do__DEAN    1.0\n",
              "2157          창현            Got To Give It Up (Part 1)__Marvin Gaye    0.0\n",
              "\n",
              "[940 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3efRHgjYDEEx"
      },
      "source": [
        "# The columns must correspond to user id, item id and ratings (in that order).\n",
        "dataset = Dataset.load_from_df(data,reader)\n",
        "Trainset = Dataset.load_from_df(ratings_train,reader)\n",
        "Testset = Dataset.load_from_df(ratings_test,reader)\n",
        "# Train/Test 분리\n",
        "trainset,testset= train_test_split(dataset,train_size=0.8,random_state=3)\n",
        "#len(trainset.all_items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX-6sbF7DEEx"
      },
      "source": [
        "#MF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpuPePy2Doxf"
      },
      "source": [
        "# 1. Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lCpKIFaDtL8"
      },
      "source": [
        "## 1) Slope One"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI3xU7HWDEE-",
        "outputId": "bf8cc376-5cd9-4b80-9309-ab4e4114a7a1"
      },
      "source": [
        "# Select your best algo with grid search.\n",
        "param_grid = {}\n",
        "grid_search = GridSearchCV(SlopeOne, param_grid, measures=['rmse'], cv=5, n_jobs=-1)\n",
        "grid_search.fit(Trainset) # Grid search 할때는 dataset 전체로 우선 해줌\n",
        "\n",
        "algo = grid_search.best_estimator['rmse']\n",
        "algo.fit(trainset)\n",
        "\n",
        "print(grid_search.best_params)\n",
        "results_df = pd.DataFrame.from_dict(grid_search.cv_results)\n",
        "display(results_df) # 행의 개수는 grid search 한 경우의 수\n",
        "\n",
        "predictions = algo.test(testset)\n",
        "print('RMSE(testset)', end=' ')\n",
        "round(accuracy.rmse(predictions),4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rmse': {}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split0_test_rmse</th>\n",
              "      <th>split1_test_rmse</th>\n",
              "      <th>split2_test_rmse</th>\n",
              "      <th>split3_test_rmse</th>\n",
              "      <th>split4_test_rmse</th>\n",
              "      <th>mean_test_rmse</th>\n",
              "      <th>std_test_rmse</th>\n",
              "      <th>rank_test_rmse</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_test_time</th>\n",
              "      <th>std_test_time</th>\n",
              "      <th>params</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.744585</td>\n",
              "      <td>0.757289</td>\n",
              "      <td>0.752391</td>\n",
              "      <td>0.753296</td>\n",
              "      <td>0.757226</td>\n",
              "      <td>0.752957</td>\n",
              "      <td>0.004637</td>\n",
              "      <td>1</td>\n",
              "      <td>0.011967</td>\n",
              "      <td>0.001669</td>\n",
              "      <td>0.068219</td>\n",
              "      <td>0.007969</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   split0_test_rmse  split1_test_rmse  split2_test_rmse  split3_test_rmse  \\\n",
              "0          0.744585          0.757289          0.752391          0.753296   \n",
              "\n",
              "   split4_test_rmse  mean_test_rmse  std_test_rmse  rank_test_rmse  \\\n",
              "0          0.757226        0.752957       0.004637               1   \n",
              "\n",
              "   mean_fit_time  std_fit_time  mean_test_time  std_test_time params  \n",
              "0       0.011967      0.001669        0.068219       0.007969     {}  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "RMSE(testset) RMSE: 0.7591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7591"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U_avfpZDyS2"
      },
      "source": [
        "## 2) Co Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBt6eke6DEE_",
        "outputId": "6d722ebf-da9d-42f6-e7a5-ade8907a8c0e"
      },
      "source": [
        "# parameter 조정 안 하고, default 값으로만 CoClustering\n",
        "names = []\n",
        "results = []\n",
        "\n",
        "algo = CoClustering()\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)\n",
        "accuracy.rmse(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.7684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7684205339558127"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrqEOlqcDEE_"
      },
      "source": [
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'n_cltr_u': [1,2,3,4,5,6,7,8,9,10,11],\n",
        "              'n_cltr_i': [2,3,4,5,6,7,8,9,10],\n",
        "              'n_epochs': [50,60,70,80,90,100]}\n",
        "\n",
        "gs = GridSearchCV(CoClustering, param_grid, measures=['rmse'], cv=5,n_jobs=-1)\n",
        "gs.fit(Trainset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIaJ4DRdDEE_",
        "outputId": "0529f28e-7838-4ff8-8e03-953d0a9dd1a6"
      },
      "source": [
        "print(gs.best_params)\n",
        "results_df = pd.DataFrame.from_dict(gs.cv_results)\n",
        "display(results_df) # 행의 개수는 grid search 한 경우의 수\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rmse': {'n_cltr_u': 8, 'n_cltr_i': 2, 'n_epochs': 100}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split0_test_rmse</th>\n",
              "      <th>split1_test_rmse</th>\n",
              "      <th>split2_test_rmse</th>\n",
              "      <th>split3_test_rmse</th>\n",
              "      <th>split4_test_rmse</th>\n",
              "      <th>mean_test_rmse</th>\n",
              "      <th>std_test_rmse</th>\n",
              "      <th>rank_test_rmse</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_test_time</th>\n",
              "      <th>std_test_time</th>\n",
              "      <th>params</th>\n",
              "      <th>param_n_cltr_u</th>\n",
              "      <th>param_n_cltr_i</th>\n",
              "      <th>param_n_epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.746907</td>\n",
              "      <td>0.756888</td>\n",
              "      <td>0.759123</td>\n",
              "      <td>0.754362</td>\n",
              "      <td>0.757577</td>\n",
              "      <td>0.754971</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>24</td>\n",
              "      <td>0.204252</td>\n",
              "      <td>0.006951</td>\n",
              "      <td>0.007181</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>{'n_cltr_u': 1, 'n_cltr_i': 2, 'n_epochs': 50}</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.746907</td>\n",
              "      <td>0.756888</td>\n",
              "      <td>0.759123</td>\n",
              "      <td>0.754362</td>\n",
              "      <td>0.757577</td>\n",
              "      <td>0.754971</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>49</td>\n",
              "      <td>0.234772</td>\n",
              "      <td>0.008166</td>\n",
              "      <td>0.006382</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>{'n_cltr_u': 1, 'n_cltr_i': 2, 'n_epochs': 60}</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.746907</td>\n",
              "      <td>0.756888</td>\n",
              "      <td>0.759123</td>\n",
              "      <td>0.754362</td>\n",
              "      <td>0.757577</td>\n",
              "      <td>0.754971</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>47</td>\n",
              "      <td>0.271278</td>\n",
              "      <td>0.013608</td>\n",
              "      <td>0.007181</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>{'n_cltr_u': 1, 'n_cltr_i': 2, 'n_epochs': 70}</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.746907</td>\n",
              "      <td>0.756888</td>\n",
              "      <td>0.759123</td>\n",
              "      <td>0.754362</td>\n",
              "      <td>0.757577</td>\n",
              "      <td>0.754971</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>43</td>\n",
              "      <td>0.357351</td>\n",
              "      <td>0.041229</td>\n",
              "      <td>0.006782</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>{'n_cltr_u': 1, 'n_cltr_i': 2, 'n_epochs': 80}</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.746907</td>\n",
              "      <td>0.756888</td>\n",
              "      <td>0.759123</td>\n",
              "      <td>0.754362</td>\n",
              "      <td>0.757577</td>\n",
              "      <td>0.754971</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>52</td>\n",
              "      <td>0.424066</td>\n",
              "      <td>0.036281</td>\n",
              "      <td>0.006582</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>{'n_cltr_u': 1, 'n_cltr_i': 2, 'n_epochs': 90}</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>589</th>\n",
              "      <td>0.799548</td>\n",
              "      <td>0.813225</td>\n",
              "      <td>0.834305</td>\n",
              "      <td>0.803520</td>\n",
              "      <td>0.821087</td>\n",
              "      <td>0.814337</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>592</td>\n",
              "      <td>1.076921</td>\n",
              "      <td>0.058967</td>\n",
              "      <td>0.012567</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>{'n_cltr_u': 11, 'n_cltr_i': 10, 'n_epochs': 60}</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>590</th>\n",
              "      <td>0.797846</td>\n",
              "      <td>0.852040</td>\n",
              "      <td>0.807164</td>\n",
              "      <td>0.802883</td>\n",
              "      <td>0.808604</td>\n",
              "      <td>0.813707</td>\n",
              "      <td>0.019529</td>\n",
              "      <td>590</td>\n",
              "      <td>1.308361</td>\n",
              "      <td>0.049772</td>\n",
              "      <td>0.009376</td>\n",
              "      <td>0.001493</td>\n",
              "      <td>{'n_cltr_u': 11, 'n_cltr_i': 10, 'n_epochs': 70}</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591</th>\n",
              "      <td>0.789338</td>\n",
              "      <td>0.803284</td>\n",
              "      <td>0.832182</td>\n",
              "      <td>0.776655</td>\n",
              "      <td>0.810544</td>\n",
              "      <td>0.802401</td>\n",
              "      <td>0.018908</td>\n",
              "      <td>562</td>\n",
              "      <td>1.525457</td>\n",
              "      <td>0.132095</td>\n",
              "      <td>0.008776</td>\n",
              "      <td>0.000978</td>\n",
              "      <td>{'n_cltr_u': 11, 'n_cltr_i': 10, 'n_epochs': 80}</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>592</th>\n",
              "      <td>0.815295</td>\n",
              "      <td>0.818681</td>\n",
              "      <td>0.788789</td>\n",
              "      <td>0.822306</td>\n",
              "      <td>0.825638</td>\n",
              "      <td>0.814142</td>\n",
              "      <td>0.013142</td>\n",
              "      <td>591</td>\n",
              "      <td>1.669562</td>\n",
              "      <td>0.100266</td>\n",
              "      <td>0.009177</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>{'n_cltr_u': 11, 'n_cltr_i': 10, 'n_epochs': 90}</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>593</th>\n",
              "      <td>0.778251</td>\n",
              "      <td>0.829056</td>\n",
              "      <td>0.834133</td>\n",
              "      <td>0.802694</td>\n",
              "      <td>0.833493</td>\n",
              "      <td>0.815525</td>\n",
              "      <td>0.021937</td>\n",
              "      <td>593</td>\n",
              "      <td>1.651585</td>\n",
              "      <td>0.170737</td>\n",
              "      <td>0.006782</td>\n",
              "      <td>0.002309</td>\n",
              "      <td>{'n_cltr_u': 11, 'n_cltr_i': 10, 'n_epochs': 100}</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>594 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     split0_test_rmse  split1_test_rmse  split2_test_rmse  split3_test_rmse  \\\n",
              "0            0.746907          0.756888          0.759123          0.754362   \n",
              "1            0.746907          0.756888          0.759123          0.754362   \n",
              "2            0.746907          0.756888          0.759123          0.754362   \n",
              "3            0.746907          0.756888          0.759123          0.754362   \n",
              "4            0.746907          0.756888          0.759123          0.754362   \n",
              "..                ...               ...               ...               ...   \n",
              "589          0.799548          0.813225          0.834305          0.803520   \n",
              "590          0.797846          0.852040          0.807164          0.802883   \n",
              "591          0.789338          0.803284          0.832182          0.776655   \n",
              "592          0.815295          0.818681          0.788789          0.822306   \n",
              "593          0.778251          0.829056          0.834133          0.802694   \n",
              "\n",
              "     split4_test_rmse  mean_test_rmse  std_test_rmse  rank_test_rmse  \\\n",
              "0            0.757577        0.754971       0.004315              24   \n",
              "1            0.757577        0.754971       0.004315              49   \n",
              "2            0.757577        0.754971       0.004315              47   \n",
              "3            0.757577        0.754971       0.004315              43   \n",
              "4            0.757577        0.754971       0.004315              52   \n",
              "..                ...             ...            ...             ...   \n",
              "589          0.821087        0.814337       0.012500             592   \n",
              "590          0.808604        0.813707       0.019529             590   \n",
              "591          0.810544        0.802401       0.018908             562   \n",
              "592          0.825638        0.814142       0.013142             591   \n",
              "593          0.833493        0.815525       0.021937             593   \n",
              "\n",
              "     mean_fit_time  std_fit_time  mean_test_time  std_test_time  \\\n",
              "0         0.204252      0.006951        0.007181       0.000977   \n",
              "1         0.234772      0.008166        0.006382       0.000489   \n",
              "2         0.271278      0.013608        0.007181       0.000977   \n",
              "3         0.357351      0.041229        0.006782       0.001163   \n",
              "4         0.424066      0.036281        0.006582       0.000488   \n",
              "..             ...           ...             ...            ...   \n",
              "589       1.076921      0.058967        0.012567       0.007208   \n",
              "590       1.308361      0.049772        0.009376       0.001493   \n",
              "591       1.525457      0.132095        0.008776       0.000978   \n",
              "592       1.669562      0.100266        0.009177       0.001469   \n",
              "593       1.651585      0.170737        0.006782       0.002309   \n",
              "\n",
              "                                                params  param_n_cltr_u  \\\n",
              "0       {'n_cltr_u': 1, 'n_cltr_i': 2, 'n_epochs': 50}               1   \n",
              "1       {'n_cltr_u': 1, 'n_cltr_i': 2, 'n_epochs': 60}               1   \n",
              "2       {'n_cltr_u': 1, 'n_cltr_i': 2, 'n_epochs': 70}               1   \n",
              "3       {'n_cltr_u': 1, 'n_cltr_i': 2, 'n_epochs': 80}               1   \n",
              "4       {'n_cltr_u': 1, 'n_cltr_i': 2, 'n_epochs': 90}               1   \n",
              "..                                                 ...             ...   \n",
              "589   {'n_cltr_u': 11, 'n_cltr_i': 10, 'n_epochs': 60}              11   \n",
              "590   {'n_cltr_u': 11, 'n_cltr_i': 10, 'n_epochs': 70}              11   \n",
              "591   {'n_cltr_u': 11, 'n_cltr_i': 10, 'n_epochs': 80}              11   \n",
              "592   {'n_cltr_u': 11, 'n_cltr_i': 10, 'n_epochs': 90}              11   \n",
              "593  {'n_cltr_u': 11, 'n_cltr_i': 10, 'n_epochs': 100}              11   \n",
              "\n",
              "     param_n_cltr_i  param_n_epochs  \n",
              "0                 2              50  \n",
              "1                 2              60  \n",
              "2                 2              70  \n",
              "3                 2              80  \n",
              "4                 2              90  \n",
              "..              ...             ...  \n",
              "589              10              60  \n",
              "590              10              70  \n",
              "591              10              80  \n",
              "592              10              90  \n",
              "593              10             100  \n",
              "\n",
              "[594 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GDn4jnTFDEFA",
        "outputId": "cf7375c1-42f8-40da-bffd-efffd65bffe4"
      },
      "source": [
        "algo = gs.best_estimator['rmse']\n",
        "algo.fit(trainset)\n",
        "# Compute unbiased accuracy on teset\n",
        "predictions = algo.test(testset)\n",
        "print('Unbiased accuracy on Test,', end=' ')\n",
        "round(accuracy.rmse(predictions),4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unbiased accuracy on Test, RMSE: 0.7547\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7547"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtakksDJDU8Z"
      },
      "source": [
        "## 3) KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Ot25akDZLd"
      },
      "source": [
        "### Basic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UzR7nsYrDEEy",
        "outputId": "c85a778c-c44b-4d15-88a5-289ac964a23c"
      },
      "source": [
        "#'name' ={'cosine','msd','pearson'}\n",
        "from collections import defaultdict\n",
        "sim_option_user_based=[]\n",
        "sim_option_item_based=[]\n",
        "for support in list(np.linspace(10,20,10,dtype=int)):\n",
        "  for name in ['cosine','msd','pearson']:\n",
        "      sim_options = {'name': name,\n",
        "                'user_based': True,# compute  similarities between Users\n",
        "                'min_support':support\n",
        "        }\n",
        "      sim_option_user_based.append(sim_options)\n",
        "# Select your best algo with grid search.\n",
        "item_param_grid = {'k': np.linspace(20,80,7,dtype=int),\n",
        "              'sim_options': {'name': ['pearson','msd', 'cosine'],\n",
        "                              'min_support': [1, 5],\n",
        "                              'user_based': [False],\n",
        "                              'min_support':[2,3,4,5]}\n",
        "              }\n",
        "\n",
        "user_param_grid = {'k': [3,4,5,6],\n",
        "              'sim_options': {'name': ['pearson','msd', 'cosine'],\n",
        "                              'min_support': [1, 5],\n",
        "                              'user_based': [True],\n",
        "                              'min_support':np.linspace(10,20,10,dtype=int)}\n",
        "              }\n",
        "\n",
        "\n",
        "param_grid={'item':item_param_grid,'user':user_param_grid}\n",
        "\n",
        "results={}\n",
        "\n",
        "for key,value in param_grid.items():\n",
        "  print('--------------',key,'start --------------------')\n",
        "  grid_search = GridSearchCV(KNNBasic, value, measures=['rmse'], cv=5)\n",
        "  grid_search.fit(Trainset) # Grid search 할때는 dataset 전체로 우선 해줌\n",
        "\n",
        "  algo = grid_search.best_estimator['rmse']\n",
        "  algo.fit(trainset)\n",
        "  best_params = grid_search.best_params \n",
        "  results_df = pd.DataFrame.from_dict(grid_search.cv_results)\n",
        "  user=[trainset.to_raw_uid(i) for i in range(trainset.n_users)]\n",
        "  item=[trainset.to_raw_iid(i) for i in range(trainset.n_items)]\n",
        "  sim_matrix = algo.compute_similarities()\n",
        "  if key=='user':\n",
        "    sim_matrix = pd.DataFrame(sim_matrix,index=user,columns=user)\n",
        "  elif key=='item':\n",
        "    sim_matrix = pd.DataFrame(sim_matrix,index=item,columns=item)\n",
        "  #display(results_df) # 행의 개수는 grid search 한 경우의 수\n",
        "\n",
        "  # Compute biased accuracy on trainset\n",
        "  predictions = algo.test(trainset.build_testset())\n",
        "  train_loss = round(accuracy.rmse(predictions),4)\n",
        "  print(f'Biased accuracy on Train :{train_loss}')\n",
        "  \n",
        "\n",
        "  # Compute unbiased accuracy on teset\n",
        "  predictions = algo.test(testset)\n",
        "  test_loss = round(accuracy.rmse(predictions),4)\n",
        "  print(f'Unbiased accuracy on Test: {test_loss}')\n",
        "  \n",
        "\n",
        "  results[key]={}\n",
        "  results[key]['model'] = algo\n",
        "  results[key]['best_params']=best_params\n",
        "  results[key]['results_df']=results_df\n",
        "  results[key]['loss']={'train_loss':train_loss,'test_loss':test_loss}\n",
        "  results[key]['sim_matrix']=sim_matrix\n",
        "  print(f'--------{key} Done --------')\n",
        "\n",
        "KNN['basic']=results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------- item start --------------------\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.4645\n",
            "Biased accuracy on Train :0.4645\n",
            "RMSE: 0.7432\n",
            "Unbiased accuracy on Test: 0.7432\n",
            "--------item Done --------\n",
            "-------------- user start --------------------\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.4448\n",
            "Biased accuracy on Train :0.4448\n",
            "RMSE: 0.7668\n",
            "Unbiased accuracy on Test: 0.7668\n",
            "--------user Done --------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sc0WWBADEEz",
        "outputId": "dc4f1b65-40c6-48ed-fa51-76e3470762b7"
      },
      "source": [
        "#1) user_based 추천\n",
        "\n",
        "model = KNN['basic']['user']['model']\n",
        "predictions = model.test(testset)\n",
        "test_loss = round(accuracy.rmse(predictions),4)\n",
        "top_n = get_top_n(predictions,5)\n",
        "Recommend('도윤',top_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.7668\n",
            "<도윤 님을 위한 추천곡>\n",
            "\n",
            "...Baby One More Time__Britney Spears => 100 %의 확률로 좋아하실 거에요\n",
            "Beat It - Single Version__Michael Jackson => 100 %의 확률로 좋아하실 거에요\n",
            "If I Were a Boy__Beyoncé => 100 %의 확률로 좋아하실 거에요\n",
            "Shape of You (Major Lazer Remix) [feat. Nyla & Kranium]__Ed Sheeran => 100 %의 확률로 좋아하실 거에요\n",
            "Counting Stars__OneRepublic => 100 %의 확률로 좋아하실 거에요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIdluV64DEEz",
        "outputId": "44c242cb-24be-47be-ba18-23593f0dc921"
      },
      "source": [
        "#1) item_based 추천\n",
        "\n",
        "model = KNN['basic']['item']['model']\n",
        "predictions = model.test(testset)\n",
        "test_loss = round(accuracy.rmse(predictions),4)\n",
        "top_n = get_top_n(predictions,5)\n",
        "Recommend('도윤',top_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.7432\n",
            "<도윤 님을 위한 추천곡>\n",
            "\n",
            "Counting Stars__OneRepublic => 61.48 %의 확률로 좋아하실 거에요\n",
            "Remember the Time__Michael Jackson => 60.62 %의 확률로 좋아하실 거에요\n",
            "Black or White - Single Version__Michael Jackson => 45.61 %의 확률로 좋아하실 거에요\n",
            "Rollin__Calvin Harris => 43.66 %의 확률로 좋아하실 거에요\n",
            "Beat It - Single Version__Michael Jackson => 40.84 %의 확률로 좋아하실 거에요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz7scpJeDeIC"
      },
      "source": [
        "### Means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV1hmjcCDEE0",
        "outputId": "d6b150c0-6c2a-40a2-d053-25ac90f1aee4"
      },
      "source": [
        "#'name' ={'cosine','msd','pearson'}\n",
        "from collections import defaultdict\n",
        "sim_option_user_based=[]\n",
        "sim_option_item_based=[]\n",
        "for support in list(np.linspace(10,20,10,dtype=int)):\n",
        "  for name in ['cosine','msd','pearson']:\n",
        "      sim_options = {'name': name,\n",
        "                'user_based': True,# compute  similarities between Users\n",
        "                'min_support':support\n",
        "        }\n",
        "      sim_option_user_based.append(sim_options)\n",
        "# Select your best algo with grid search.\n",
        "item_param_grid = {'k': np.linspace(20,80,7,dtype=int),\n",
        "              'sim_options': {'name': ['pearson','msd', 'cosine'],\n",
        "                              'min_support': [1, 5],\n",
        "                              'user_based': [False],\n",
        "                              'min_support':[2,3,4,5]}\n",
        "              }\n",
        "\n",
        "user_param_grid = {'k': [3,4,5,6],\n",
        "              'sim_options': {'name': ['pearson','msd', 'cosine'],\n",
        "                              'min_support': [1, 5],\n",
        "                              'user_based': [True],\n",
        "                              'min_support':np.linspace(10,20,10,dtype=int)}\n",
        "              }\n",
        "\n",
        "\n",
        "param_grid={'item':item_param_grid,'user':user_param_grid}\n",
        "\n",
        "results={}\n",
        "\n",
        "for key,value in param_grid.items():\n",
        "  print('--------------',key,'start --------------------')\n",
        "  grid_search = GridSearchCV(KNNWithMeans, value, measures=['rmse'], cv=5)\n",
        "  grid_search.fit(Trainset) \n",
        "\n",
        "  algo = grid_search.best_estimator['rmse']\n",
        "  algo.fit(trainset)\n",
        "  best_params = grid_search.best_params \n",
        "  results_df = pd.DataFrame.from_dict(grid_search.cv_results)\n",
        "  user=[trainset.to_raw_uid(i) for i in range(trainset.n_users)]\n",
        "  item=[trainset.to_raw_iid(i) for i in range(trainset.n_items)]\n",
        "  sim_matrix = algo.compute_similarities()\n",
        "  if key=='user':\n",
        "    sim_matrix = pd.DataFrame(sim_matrix,index=user,columns=user)\n",
        "  elif key=='item':\n",
        "    sim_matrix = pd.DataFrame(sim_matrix,index=item,columns=item)\n",
        "  #display(results_df) # 행의 개수는 grid search 한 경우의 수\n",
        "\n",
        "  # Compute biased accuracy on trainset\n",
        "  predictions = algo.test(trainset.build_testset())\n",
        "  train_loss = round(accuracy.rmse(predictions),4)\n",
        "  print(f'Biased accuracy on Train :{train_loss}')\n",
        "  \n",
        "\n",
        "  # Compute unbiased accuracy on teset\n",
        "  predictions = algo.test(testset)\n",
        "  test_loss = round(accuracy.rmse(predictions),4)\n",
        "  print(f'Unbiased accuracy on Test: {test_loss}')\n",
        "  \n",
        "\n",
        "  results[key]={}\n",
        "  results[key]['model'] = algo\n",
        "  results[key]['best_params']=best_params\n",
        "  results[key]['results_df']=results_df\n",
        "  results[key]['loss']={'train_loss':train_loss,'test_loss':test_loss}\n",
        "  results[key]['sim_matrix']=sim_matrix\n",
        "  print(f'--------{key} Done --------')\n",
        "\n",
        "KNN['w_means']=results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------- item start --------------------\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.4972\n",
            "Biased accuracy on Train :0.4972\n",
            "RMSE: 0.7381\n",
            "Unbiased accuracy on Test: 0.7381\n",
            "--------item Done --------\n",
            "-------------- user start --------------------\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.3925\n",
            "Biased accuracy on Train :0.3925\n",
            "RMSE: 0.7514\n",
            "Unbiased accuracy on Test: 0.7514\n",
            "--------user Done --------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRNQVidSDEE0",
        "outputId": "d4a7e37b-c723-4297-9731-99b2278dab51"
      },
      "source": [
        "#1) user_based 추천\n",
        "\n",
        "model = KNN['w_means']['user']['model']\n",
        "predictions = model.test(testset)\n",
        "test_loss = round(accuracy.rmse(predictions),4)\n",
        "top_n = get_top_n(predictions,5)\n",
        "Recommend('도윤',top_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.7514\n",
            "<도윤 님을 위한 추천곡>\n",
            "\n",
            "Black or White - Single Version__Michael Jackson => 100 %의 확률로 좋아하실 거에요\n",
            "Rollin__Calvin Harris => 100 %의 확률로 좋아하실 거에요\n",
            "Beat It - Single Version__Michael Jackson => 100 %의 확률로 좋아하실 거에요\n",
            "Counting Stars__OneRepublic => 100 %의 확률로 좋아하실 거에요\n",
            "SexyBack__Justin Timberlake => 95.9 %의 확률로 좋아하실 거에요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLlj4SWNDEE1",
        "outputId": "34964cb5-75c8-4aca-d40e-6b45a3009040"
      },
      "source": [
        "#1) item_based 추천\n",
        "\n",
        "model = KNN['w_means']['item']['model']\n",
        "predictions = model.test(testset)\n",
        "test_loss = round(accuracy.rmse(predictions),4)\n",
        "top_n = get_top_n(predictions,5)\n",
        "Recommend('도윤',top_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.7381\n",
            "<도윤 님을 위한 추천곡>\n",
            "\n",
            "Counting Stars__OneRepublic => 72.58 %의 확률로 좋아하실 거에요\n",
            "Black or White - Single Version__Michael Jackson => 61.78 %의 확률로 좋아하실 거에요\n",
            "Beat It - Single Version__Michael Jackson => 61.18 %의 확률로 좋아하실 거에요\n",
            "...Baby One More Time__Britney Spears => 61.08 %의 확률로 좋아하실 거에요\n",
            "If I Were a Boy__Beyoncé => 58.81 %의 확률로 좋아하실 거에요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jv8gcbVDhZh"
      },
      "source": [
        "### Z- Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV1RrGTcDEE3",
        "outputId": "2fdee061-55cc-41e4-c79a-f9b81d6a1144"
      },
      "source": [
        "#'name' ={'cosine','msd','pearson'}\n",
        "from collections import defaultdict\n",
        "sim_option_user_based=[]\n",
        "sim_option_item_based=[]\n",
        "for support in list(np.linspace(10,20,10,dtype=int)):\n",
        "  for name in ['cosine','msd','pearson']:\n",
        "      sim_options = {'name': name,\n",
        "                'user_based': True,# compute  similarities between Users\n",
        "                'min_support':support\n",
        "        }\n",
        "      sim_option_user_based.append(sim_options)\n",
        "# Select your best algo with grid search.\n",
        "item_param_grid = {'k': np.linspace(20,80,7,dtype=int),\n",
        "              'sim_options': {'name': ['pearson','msd', 'cosine'],\n",
        "                              'min_support': [1, 5],\n",
        "                              'user_based': [False],\n",
        "                              'min_support':[2,3,4,5]}\n",
        "              }\n",
        "\n",
        "user_param_grid = {'k': [3,4,5,6],\n",
        "              'sim_options': {'name': ['pearson','msd', 'cosine'],\n",
        "                              'min_support': [1, 5],\n",
        "                              'user_based': [True],\n",
        "                              'min_support':np.linspace(10,20,10,dtype=int)}\n",
        "              }\n",
        "\n",
        "\n",
        "param_grid={'item':item_param_grid,'user':user_param_grid}\n",
        "\n",
        "results={}\n",
        "\n",
        "for key,value in param_grid.items():\n",
        "  print('--------------',key,'start --------------------')\n",
        "  grid_search = GridSearchCV(KNNWithZScore, value, measures=['rmse'], cv=5)\n",
        "  grid_search.fit(dataset) # Grid search 할때는 dataset 전체로 우선 해줌\n",
        "\n",
        "  algo = grid_search.best_estimator['rmse']\n",
        "  algo.fit(trainset)\n",
        "  best_params = grid_search.best_params \n",
        "  results_df = pd.DataFrame.from_dict(grid_search.cv_results)\n",
        "  user=[trainset.to_raw_uid(i) for i in range(trainset.n_users)]\n",
        "  item=[trainset.to_raw_iid(i) for i in range(trainset.n_items)]\n",
        "  sim_matrix = algo.compute_similarities()\n",
        "  if key=='user':\n",
        "    sim_matrix = pd.DataFrame(sim_matrix,index=user,columns=user)\n",
        "  elif key=='item':\n",
        "    sim_matrix = pd.DataFrame(sim_matrix,index=item,columns=item)\n",
        "  #display(results_df) # 행의 개수는 grid search 한 경우의 수\n",
        "\n",
        "  # Compute biased accuracy on trainset\n",
        "  predictions = algo.test(trainset.build_testset())\n",
        "  train_loss = round(accuracy.rmse(predictions),4)\n",
        "  print(f'Biased accuracy on Train :{train_loss}')\n",
        "  \n",
        "\n",
        "  # Compute unbiased accuracy on teset\n",
        "  predictions = algo.test(testset)\n",
        "  test_loss = round(accuracy.rmse(predictions),4)\n",
        "  print(f'Unbiased accuracy on Test: {test_loss}')\n",
        "  \n",
        "\n",
        "  results[key]={}\n",
        "  results[key]['model'] = algo\n",
        "  results[key]['best_params']=best_params\n",
        "  results[key]['results_df']=results_df\n",
        "  results[key]['loss']={'train_loss':train_loss,'test_loss':test_loss}\n",
        "  results[key]['sim_matrix']=sim_matrix\n",
        "  print(f'--------{key} Done --------')\n",
        "\n",
        "KNN['w_zscore']=results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------- item start --------------------\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.4924\n",
            "Biased accuracy on Train :0.4924\n",
            "RMSE: 0.7405\n",
            "Unbiased accuracy on Test: 0.7405\n",
            "--------item Done --------\n",
            "-------------- user start --------------------\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.3896\n",
            "Biased accuracy on Train :0.3896\n",
            "RMSE: 0.7494\n",
            "Unbiased accuracy on Test: 0.7494\n",
            "--------user Done --------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2BISyVFDEE4",
        "outputId": "4b39d08a-7266-426a-870e-b67727b30e15"
      },
      "source": [
        "#1) user_based 추천\n",
        "\n",
        "model = KNN['w_zscore']['user']['model']\n",
        "predictions = model.test(testset)\n",
        "test_loss = round(accuracy.rmse(predictions),4)\n",
        "top_n = get_top_n(predictions,5)\n",
        "Recommend('도윤',top_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.7494\n",
            "<도윤 님을 위한 추천곡>\n",
            "\n",
            "Counting Stars__OneRepublic => 89.25 %의 확률로 좋아하실 거에요\n",
            "Beat It - Single Version__Michael Jackson => 88.38 %의 확률로 좋아하실 거에요\n",
            "Black or White - Single Version__Michael Jackson => 84.01 %의 확률로 좋아하실 거에요\n",
            "Rollin__Calvin Harris => 80.91 %의 확률로 좋아하실 거에요\n",
            "SexyBack__Justin Timberlake => 79.46 %의 확률로 좋아하실 거에요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnHrqcXyDEE4",
        "outputId": "c505c594-6a64-4f92-fa7e-36ff14d8fe4b"
      },
      "source": [
        "#1) item_based 추천\n",
        "\n",
        "model = KNN['w_zscore']['item']['model']\n",
        "predictions = model.test(testset)\n",
        "test_loss = round(accuracy.rmse(predictions),4)\n",
        "top_n = get_top_n(predictions,5)\n",
        "Recommend('도윤',top_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.7405\n",
            "<도윤 님을 위한 추천곡>\n",
            "\n",
            "Counting Stars__OneRepublic => 69.99 %의 확률로 좋아하실 거에요\n",
            "Something Just Like This - Don Diablo Remix__The Chainsmokers => 62.58 %의 확률로 좋아하실 거에요\n",
            "Beat It - Single Version__Michael Jackson => 61.19 %의 확률로 좋아하실 거에요\n",
            "Black or White - Single Version__Michael Jackson => 59.04 %의 확률로 좋아하실 거에요\n",
            "...Baby One More Time__Britney Spears => 58.14 %의 확률로 좋아하실 거에요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qCeUyvDDlN6"
      },
      "source": [
        "### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl080yBBDEE5",
        "outputId": "168f8e97-0752-487b-ea8d-089d4978d116"
      },
      "source": [
        "from collections import defaultdict\n",
        "sim_option_user_based=[]\n",
        "sim_option_item_based=[]\n",
        "for support in list(np.linspace(10,20,10,dtype=int)):\n",
        "  for name in ['cosine','msd','pearson']:\n",
        "      sim_options = {'name': name,\n",
        "                'user_based': True,# compute  similarities between Users\n",
        "                'min_support':support\n",
        "        }\n",
        "      sim_option_user_based.append(sim_options)\n",
        "    \n",
        "# Select your best algo with grid search.\n",
        "item_param_grid = {'k': np.linspace(20,80,7,dtype=int),\n",
        "              'sim_options': {'name': ['pearson','msd', 'cosine'],\n",
        "                              'min_support': [1, 5],\n",
        "                              'user_based': [False],\n",
        "                              'min_support':[2,3,4,5]}\n",
        "              }\n",
        "\n",
        "user_param_grid = {'k': [3,4,5,6],\n",
        "              'sim_options': {'name': ['pearson','msd', 'cosine'],\n",
        "                              'min_support': [1, 5],\n",
        "                              'user_based': [True],\n",
        "                              'min_support':np.linspace(10,20,10,dtype=int)}\n",
        "              }\n",
        "\n",
        "\n",
        "param_grid={'item':item_param_grid,'user':user_param_grid}\n",
        "\n",
        "results={}\n",
        "\n",
        "for key,value in param_grid.items():\n",
        "  print('--------------',key,'start --------------------')\n",
        "  grid_search = GridSearchCV(KNNBaseline, value, measures=['rmse'], cv=5)\n",
        "  grid_search.fit(dataset) # Grid search 할때는 dataset 전체로 우선 해줌\n",
        "\n",
        "  algo = grid_search.best_estimator['rmse']\n",
        "  algo.fit(trainset)\n",
        "  best_params = grid_search.best_params \n",
        "  results_df = pd.DataFrame.from_dict(grid_search.cv_results)\n",
        "  user=[trainset.to_raw_uid(i) for i in range(trainset.n_users)]\n",
        "  item=[trainset.to_raw_iid(i) for i in range(trainset.n_items)]\n",
        "  sim_matrix = algo.compute_similarities()\n",
        "  if key=='user':\n",
        "    sim_matrix = pd.DataFrame(sim_matrix,index=user,columns=user)\n",
        "  elif key=='item':\n",
        "    sim_matrix = pd.DataFrame(sim_matrix,index=item,columns=item)\n",
        "  #display(results_df) # 행의 개수는 grid search 한 경우의 수\n",
        "\n",
        "  # Compute biased accuracy on trainset\n",
        "  predictions = algo.test(trainset.build_testset())\n",
        "  train_loss = round(accuracy.rmse(predictions),4)\n",
        "  print(f'Biased accuracy on Train :{train_loss}')\n",
        "  \n",
        "\n",
        "  # Compute unbiased accuracy on teset\n",
        "  predictions = algo.test(testset)\n",
        "  test_loss = round(accuracy.rmse(predictions),4)\n",
        "  print(f'Unbiased accuracy on Test: {test_loss}')\n",
        "  \n",
        "\n",
        "  results[key]={}\n",
        "  results[key]['model'] = algo\n",
        "  results[key]['best_params']=best_params\n",
        "  results[key]['results_df']=results_df\n",
        "  results[key]['loss']={'train_loss':train_loss,'test_loss':test_loss}\n",
        "  results[key]['sim_matrix']=sim_matrix\n",
        "  print(f'--------{key} Done --------')\n",
        "\n",
        "KNN['w_baseline']=results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------- item start --------------------\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.4417\n",
            "Biased accuracy on Train :0.4417\n",
            "RMSE: 0.7317\n",
            "Unbiased accuracy on Test: 0.7317\n",
            "--------item Done --------\n",
            "-------------- user start --------------------\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the pearson similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Estimating biases using als...\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.5208\n",
            "Biased accuracy on Train :0.5208\n",
            "RMSE: 0.7545\n",
            "Unbiased accuracy on Test: 0.7545\n",
            "--------user Done --------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRvMhPrhDEE9",
        "outputId": "e5662c81-d894-491f-c804-27b06210b4d7"
      },
      "source": [
        "#1) user_based 추천\n",
        "model = KNN['w_baseline']['user']['model']\n",
        "predictions = model.test(testset)\n",
        "test_loss = round(accuracy.rmse(predictions),4)\n",
        "top_n = get_top_n(predictions,5)\n",
        "Recommend('창현',top_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.7545\n",
            "<창현 님을 위한 추천곡>\n",
            "\n",
            "If I Were a Boy__Beyoncé => 92.72 %의 확률로 좋아하실 거에요\n",
            "Black or White - Single Version__Michael Jackson => 90.56 %의 확률로 좋아하실 거에요\n",
            "Maps__Maroon 5 => 88.67 %의 확률로 좋아하실 거에요\n",
            "So Sick__Ne-Yo => 79.27 %의 확률로 좋아하실 거에요\n",
            "Know Yourself__Drake => 66.23 %의 확률로 좋아하실 거에요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuJavPEBDEE9",
        "outputId": "9cb52d95-d0de-494b-9038-1457cfc1cd50"
      },
      "source": [
        "#1) item_based 추천\n",
        "model = KNN['w_baseline']['item']['model']\n",
        "predictions = model.test(testset)\n",
        "test_loss = round(accuracy.rmse(predictions),4)\n",
        "top_n = get_top_n(predictions,5)\n",
        "Recommend('창현',top_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.7317\n",
            "<창현 님을 위한 추천곡>\n",
            "\n",
            "So Sick__Ne-Yo => 93.55 %의 확률로 좋아하실 거에요\n",
            "If I Were a Boy__Beyoncé => 79.19 %의 확률로 좋아하실 거에요\n",
            "Hotline Bling__Drake => 69.86 %의 확률로 좋아하실 거에요\n",
            "Black or White - Single Version__Michael Jackson => 69.17 %의 확률로 좋아하실 거에요\n",
            "I'm the One__DJ Khaled => 69.07 %의 확률로 좋아하실 거에요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvAvEfmHDEE-",
        "outputId": "aeaedc23-17f8-49fb-84f8-aed10529feb6"
      },
      "source": [
        "## KNN Result\n",
        "\n",
        "print('KNN_user_Basic_User :',KNN['basic']['user']['loss'])\n",
        "print('KNN_user_With_Means_User :', KNN['w_means']['user']['loss'])\n",
        "print('KNN_user_With_Z-Score_User:', KNN['w_zscore']['user']['loss'])\n",
        "print('KNN_user_With_Baseline_User:', KNN['w_baseline']['user']['loss'])\n",
        "print()\n",
        "print('KNN_item_Basic_Item :',KNN['basic']['item']['loss'])\n",
        "print('KNN_item_With_Means_Item :', KNN['w_means']['item']['loss'])\n",
        "print('KNN_item_With_Z-Score_Item:', KNN['w_zscore']['item']['loss'])\n",
        "print('KNN_user_With_Baseline_Item:', KNN['w_baseline']['item']['loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN_user_Basic_User : {'train_loss': 0.4448, 'test_loss': 0.7668}\n",
            "KNN_user_With_Means_User : {'train_loss': 0.3925, 'test_loss': 0.7514}\n",
            "KNN_user_With_Z-Score_User: {'train_loss': 0.3896, 'test_loss': 0.7494}\n",
            "KNN_user_With_Baseline_User: {'train_loss': 0.5208, 'test_loss': 0.7545}\n",
            "\n",
            "KNN_item_Basic_Item : {'train_loss': 0.4645, 'test_loss': 0.7432}\n",
            "KNN_item_With_Means_Item : {'train_loss': 0.4972, 'test_loss': 0.7381}\n",
            "KNN_item_With_Z-Score_Item: {'train_loss': 0.4924, 'test_loss': 0.7405}\n",
            "KNN_user_With_Baseline_Item: {'train_loss': 0.4417, 'test_loss': 0.7317}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZiTqXfsDEFB"
      },
      "source": [
        "data_dict = pd.read_excel(r\"C:\\Users\\82102\\Desktop\\KU-BIG\\Spotify\\Spotify_Data.xlsx\",sheet_name=None,header=0,index_col=0)\n",
        "user_name = data_dict['Ratings'].iloc[:,0]\n",
        "track_name = data_dict['Track_Features'].iloc[:,0]\n",
        "#ratings = data_dict['Ratings'].drop(['users'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoCsS3DKFR0u"
      },
      "source": [
        "# 2. Matrix Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZandB3oIR-zc"
      },
      "source": [
        "## 1) SVD(MF) : with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLnhrbMZuD0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f420ee-8099-4a64-89e7-98daec29e6f3"
      },
      "source": [
        "# Select your best algo with grid search.\n",
        "print('Grid Search...')\n",
        "param_grid = {'n_epochs': [18,19,20,22,24,26,28,30], 'lr_all': np.linspace(0.04,0.05,10)}\n",
        "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5,n_jobs=-1)\n",
        "grid_search.fit(Trainset) # Grid search 할때는 dataset 전체로 우선 해줌\n",
        "\n",
        "algo = grid_search.best_estimator['rmse']\n",
        "algo.fit(trainset)\n",
        "\n",
        "print(grid_search.best_params)\n",
        "results_df = pd.DataFrame.from_dict(grid_search.cv_results)\n",
        "display(results_df) # 행의 개수는 grid search 한 경우의 수\n",
        "\n",
        "# Compute unbiased accuracy on teset\n",
        "predictions = algo.test(testset)\n",
        "print('Unbiased accuracy on Test,', end=' ')\n",
        "round(accuracy.rmse(predictions),4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grid Search...\n",
            "{'rmse': {'n_epochs': 30, 'lr_all': 0.044444444444444446}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split0_test_rmse</th>\n",
              "      <th>split1_test_rmse</th>\n",
              "      <th>split2_test_rmse</th>\n",
              "      <th>split3_test_rmse</th>\n",
              "      <th>split4_test_rmse</th>\n",
              "      <th>mean_test_rmse</th>\n",
              "      <th>std_test_rmse</th>\n",
              "      <th>rank_test_rmse</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_test_time</th>\n",
              "      <th>std_test_time</th>\n",
              "      <th>params</th>\n",
              "      <th>param_n_epochs</th>\n",
              "      <th>param_lr_all</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.751864</td>\n",
              "      <td>0.738493</td>\n",
              "      <td>0.748530</td>\n",
              "      <td>0.732345</td>\n",
              "      <td>0.756419</td>\n",
              "      <td>0.745530</td>\n",
              "      <td>0.008844</td>\n",
              "      <td>71</td>\n",
              "      <td>0.338199</td>\n",
              "      <td>0.055390</td>\n",
              "      <td>0.011220</td>\n",
              "      <td>0.004643</td>\n",
              "      <td>{'n_epochs': 18, 'lr_all': 0.04}</td>\n",
              "      <td>18</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.767146</td>\n",
              "      <td>0.742116</td>\n",
              "      <td>0.745589</td>\n",
              "      <td>0.732128</td>\n",
              "      <td>0.756362</td>\n",
              "      <td>0.748668</td>\n",
              "      <td>0.012055</td>\n",
              "      <td>80</td>\n",
              "      <td>0.350087</td>\n",
              "      <td>0.029314</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>{'n_epochs': 18, 'lr_all': 0.04111111111111111}</td>\n",
              "      <td>18</td>\n",
              "      <td>0.041111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.766766</td>\n",
              "      <td>0.723610</td>\n",
              "      <td>0.742865</td>\n",
              "      <td>0.732878</td>\n",
              "      <td>0.757534</td>\n",
              "      <td>0.744731</td>\n",
              "      <td>0.015745</td>\n",
              "      <td>67</td>\n",
              "      <td>0.367143</td>\n",
              "      <td>0.020522</td>\n",
              "      <td>0.010429</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>{'n_epochs': 18, 'lr_all': 0.042222222222222223}</td>\n",
              "      <td>18</td>\n",
              "      <td>0.042222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.773473</td>\n",
              "      <td>0.729934</td>\n",
              "      <td>0.743548</td>\n",
              "      <td>0.736350</td>\n",
              "      <td>0.744480</td>\n",
              "      <td>0.745557</td>\n",
              "      <td>0.014922</td>\n",
              "      <td>72</td>\n",
              "      <td>0.376354</td>\n",
              "      <td>0.030468</td>\n",
              "      <td>0.008550</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>{'n_epochs': 18, 'lr_all': 0.043333333333333335}</td>\n",
              "      <td>18</td>\n",
              "      <td>0.043333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.770497</td>\n",
              "      <td>0.735165</td>\n",
              "      <td>0.748224</td>\n",
              "      <td>0.716155</td>\n",
              "      <td>0.760427</td>\n",
              "      <td>0.746094</td>\n",
              "      <td>0.019085</td>\n",
              "      <td>76</td>\n",
              "      <td>0.373049</td>\n",
              "      <td>0.010408</td>\n",
              "      <td>0.009954</td>\n",
              "      <td>0.002575</td>\n",
              "      <td>{'n_epochs': 18, 'lr_all': 0.044444444444444446}</td>\n",
              "      <td>18</td>\n",
              "      <td>0.044444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.757142</td>\n",
              "      <td>0.722798</td>\n",
              "      <td>0.727886</td>\n",
              "      <td>0.728157</td>\n",
              "      <td>0.742586</td>\n",
              "      <td>0.735714</td>\n",
              "      <td>0.012582</td>\n",
              "      <td>3</td>\n",
              "      <td>0.581038</td>\n",
              "      <td>0.039139</td>\n",
              "      <td>0.008969</td>\n",
              "      <td>0.000538</td>\n",
              "      <td>{'n_epochs': 30, 'lr_all': 0.04555555555555556}</td>\n",
              "      <td>30</td>\n",
              "      <td>0.045556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.755847</td>\n",
              "      <td>0.728046</td>\n",
              "      <td>0.747244</td>\n",
              "      <td>0.731528</td>\n",
              "      <td>0.738566</td>\n",
              "      <td>0.740246</td>\n",
              "      <td>0.010198</td>\n",
              "      <td>31</td>\n",
              "      <td>0.583882</td>\n",
              "      <td>0.033989</td>\n",
              "      <td>0.009639</td>\n",
              "      <td>0.001811</td>\n",
              "      <td>{'n_epochs': 30, 'lr_all': 0.04666666666666667}</td>\n",
              "      <td>30</td>\n",
              "      <td>0.046667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.757271</td>\n",
              "      <td>0.719771</td>\n",
              "      <td>0.736893</td>\n",
              "      <td>0.722119</td>\n",
              "      <td>0.744579</td>\n",
              "      <td>0.736127</td>\n",
              "      <td>0.014020</td>\n",
              "      <td>5</td>\n",
              "      <td>0.579106</td>\n",
              "      <td>0.026221</td>\n",
              "      <td>0.010371</td>\n",
              "      <td>0.002751</td>\n",
              "      <td>{'n_epochs': 30, 'lr_all': 0.04777777777777778}</td>\n",
              "      <td>30</td>\n",
              "      <td>0.047778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.753864</td>\n",
              "      <td>0.724427</td>\n",
              "      <td>0.734568</td>\n",
              "      <td>0.721100</td>\n",
              "      <td>0.747199</td>\n",
              "      <td>0.736232</td>\n",
              "      <td>0.012667</td>\n",
              "      <td>6</td>\n",
              "      <td>0.582613</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.009020</td>\n",
              "      <td>0.000567</td>\n",
              "      <td>{'n_epochs': 30, 'lr_all': 0.04888888888888889}</td>\n",
              "      <td>30</td>\n",
              "      <td>0.048889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.753800</td>\n",
              "      <td>0.725011</td>\n",
              "      <td>0.736806</td>\n",
              "      <td>0.725840</td>\n",
              "      <td>0.742094</td>\n",
              "      <td>0.736710</td>\n",
              "      <td>0.010734</td>\n",
              "      <td>7</td>\n",
              "      <td>0.517066</td>\n",
              "      <td>0.107393</td>\n",
              "      <td>0.008099</td>\n",
              "      <td>0.001548</td>\n",
              "      <td>{'n_epochs': 30, 'lr_all': 0.05}</td>\n",
              "      <td>30</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    split0_test_rmse  split1_test_rmse  ...  param_n_epochs  param_lr_all\n",
              "0           0.751864          0.738493  ...              18      0.040000\n",
              "1           0.767146          0.742116  ...              18      0.041111\n",
              "2           0.766766          0.723610  ...              18      0.042222\n",
              "3           0.773473          0.729934  ...              18      0.043333\n",
              "4           0.770497          0.735165  ...              18      0.044444\n",
              "..               ...               ...  ...             ...           ...\n",
              "75          0.757142          0.722798  ...              30      0.045556\n",
              "76          0.755847          0.728046  ...              30      0.046667\n",
              "77          0.757271          0.719771  ...              30      0.047778\n",
              "78          0.753864          0.724427  ...              30      0.048889\n",
              "79          0.753800          0.725011  ...              30      0.050000\n",
              "\n",
              "[80 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Unbiased accuracy on Test, RMSE: 0.7138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7138"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o__ff4l5KnPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e605bee-39f6-426d-8c0c-b0d4a405c3f2"
      },
      "source": [
        "import pickle\n",
        "from sklearn.externals import joblib\n",
        "directory=Directory('SVD')\n",
        "fname=f'{directory}{today()}_SVD.pkl'\n",
        "print(fname)\n",
        "joblib.dump(algo, fname) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Colab Notebooks/Spotify Recommendation/Recommendation Models/SVD/20210404_17시39분_SVD.pkl\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/gdrive/MyDrive/Colab Notebooks/Spotify Recommendation/Recommendation Models/SVD/20210404_17시39분_SVD.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSJ8fH7SP3ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132302b9-2b91-48dc-c18a-eae390d658ba"
      },
      "source": [
        "fname='/gdrive/MyDrive/Colab Notebooks/Spotify Recommendation/Recommendation Models/SVD/20210404_11시40분_SVD.pkl'\n",
        "\n",
        "rcm_from_joblib = joblib.load(fname)\n",
        "predictions = rcm_from_joblib.test(testset)\n",
        "test_loss = round(accuracy.rmse(predictions),4)\n",
        "\n",
        "top_n = get_top_n(predictions,5)\n",
        "Recommend('윤지현',top_n)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.2656\n",
            "<윤지현 (a.k.a. 윤영롱) 님을 위한 추천곡>\n",
            "\n",
            "Maps__Maroon 5 => 97.3 %의 확률로 좋아하실 거에요\n",
            "Black or White - Single Version__Michael Jackson => 96.22 %의 확률로 좋아하실 거에요\n",
            "Shape of You__Ed Sheeran => 95.61 %의 확률로 좋아하실 거에요\n",
            "Get Lucky - Radio Edit__Daft Punk => 95.11 %의 확률로 좋아하실 거에요\n",
            "...Baby One More Time__Britney Spears => 94.36 %의 확률로 좋아하실 거에요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azm8o7_o3Nl7"
      },
      "source": [
        "## 2) Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqilmHoK32P_"
      },
      "source": [
        "from tensorflow.keras import layers,Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dot, Add, Flatten ,ReLU, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.regularizers import l2,l1,l1_l2\n",
        "from tensorflow.keras.optimizers import SGD, Adamax,Adam\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAHdP2s56DUh"
      },
      "source": [
        "### 데이터셋 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s59uGDGeDhFS"
      },
      "source": [
        "DATA_PATH = '/gdrive/Shareddrives/Spotify Recommendation/Spotify_Data.xlsx'\n",
        "data_dict = pd.read_excel(DATA_PATH,sheet_name=None,header=0,index_col=None)\n",
        "\n",
        "#display(data_dict['Ratings'])\n",
        "user_name = data_dict['Ratings'].iloc[:,0]\n",
        "track_name = data_dict['Track_Features'].iloc[:,0]\n",
        "ratings = data_dict['Ratings'].drop(['users'],axis=1)\n",
        "ratings = ratings.set_axis(np.linspace(0,161,162,dtype=int),axis=1)\n",
        "tracks = data_dict['Track_Features'].drop(['Unnamed: 0'],axis=1)\n",
        "# track,index\n",
        "track2idx={name:i for i, name in enumerate(list(track_ft['track_name']))}\n",
        "idx2track={i:name for i, name in enumerate(list(track_ft['track_name']))}\n",
        "\n",
        "user2idx={name:i for i, name in enumerate(user_name)}\n",
        "idx2user={i:name for i, name in enumerate(user_name)}\n",
        "\n",
        "\n",
        "display(ratings)\n",
        "\n",
        "track2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDwJVpEdhu9K",
        "outputId": "b87714b2-79b0-42a6-c710-401d2cd657b5"
      },
      "source": [
        "track_ft['track_name']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1                                     Bye Bye Bye__NSYNC\n",
              "2                 This I Promise You - Radio Edit__NSYNC\n",
              "3                                It's Gonna Be Me__NSYNC\n",
              "4      God Must Have Spent a Little More Time on You ...\n",
              "5                    I Want You Back - Radio Edit__NSYNC\n",
              "                             ...                        \n",
              "158             Can I Touch You...There?__Michael Bolton\n",
              "159    Shape of You (Major Lazer Remix) [feat. Nyla &...\n",
              "160       Paris - Pegboard Nerds Remix__The Chainsmokers\n",
              "161    Don't Let Me Down - Zomboy Remix__The Chainsmo...\n",
              "162    Something Just Like This - ARMNHMR Remix__The ...\n",
              "Name: track_name, Length: 162, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EROoUK13cL3x",
        "outputId": "4804ba49-76b5-415b-dbb9-aa788727cea9"
      },
      "source": [
        "track2idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'$ave Dat Money (feat. Fetty Wap & Rich Homie Quan)__Lil Dicky': 128,\n",
              " '(You Drive Me) Crazy__Britney Spears': 27,\n",
              " '...Baby One More Time__Britney Spears': 25,\n",
              " '0 To 100 / The Catch Up__Drake': 70,\n",
              " 'A Song For Mama__Boyz II Men': 14,\n",
              " 'Against All Odds (Take a Look at Me Now)__Mariah Carey': 155,\n",
              " 'All Day__Kanye West': 41,\n",
              " 'All I Have to Give__Backstreet Boys': 18,\n",
              " 'Antidote__Travis Scott': 54,\n",
              " 'As Long as You Love Me__Backstreet Boys': 15,\n",
              " 'Bangarang (feat. Sirah)__Skrillex': 103,\n",
              " 'Beat It - Single Version__Michael Jackson': 143,\n",
              " 'Best I Ever Had__Drake': 100,\n",
              " 'Big Rings__Drake': 42,\n",
              " 'Billie Jean__Michael Jackson': 142,\n",
              " 'Black Skinhead__Kanye West': 75,\n",
              " 'Black or White - Single Version__Michael Jackson': 144,\n",
              " 'Blood On The Leaves__Kanye West': 49,\n",
              " 'Bounce It__Juicy J': 64,\n",
              " 'Bound 2__Kanye West': 69,\n",
              " 'Break Up Every Night__The Chainsmokers': 84,\n",
              " \"Breakn' A Sweat__Skrillex\": 104,\n",
              " 'Bye Bye Bye__NSYNC': 0,\n",
              " 'Can I Touch You...There?__Michael Bolton': 157,\n",
              " 'Chicken Soup__Skrillex': 112,\n",
              " 'Childs Play__Drake': 31,\n",
              " 'Ciao Adios__Anne-Marie': 7,\n",
              " 'Clique__JAY Z': 90,\n",
              " 'Cold__Maroon 5': 129,\n",
              " 'Complicated__Avril Lavigne': 29,\n",
              " 'Confident__Demi Lovato': 117,\n",
              " 'Cool for the Summer__Demi Lovato': 118,\n",
              " 'Counting Stars__OneRepublic': 97,\n",
              " \"Don't Let Me Down - Zomboy Remix__The Chainsmokers\": 160,\n",
              " \"Don't Love You No More (I'm Sorry)__Craig David\": 141,\n",
              " 'Double Bubble Trouble__M.I.A.': 48,\n",
              " 'Earth Song - Remastered Version__Michael Jackson': 156,\n",
              " 'End Of The Road__Boyz II Men': 11,\n",
              " 'Escapade__Janet Jackson': 133,\n",
              " \"Everybody (Backstreet's Back) - Radio Edit__Backstreet Boys\": 28,\n",
              " 'Fame (2007 Remastered)__David Bowie': 45,\n",
              " 'Fantasy__Mariah Carey': 153,\n",
              " 'Father Stretch My Hands Pt. 1__Kanye West': 77,\n",
              " 'For Free__DJ Khaled': 73,\n",
              " 'For You__Demi Lovato': 120,\n",
              " 'Forever__Chris Brown': 149,\n",
              " 'GDFR (feat. Sage The Gemini & Lookas)__Flo Rida': 93,\n",
              " \"Gary's Theme - Remastered__Bill Evans\": 8,\n",
              " 'Get Lucky - Daft Punk Remix__Daft Punk': 63,\n",
              " 'Get Lucky - Radio Edit__Daft Punk': 61,\n",
              " 'Get Lucky__Daft Punk': 98,\n",
              " 'God Must Have Spent a Little More Time on You - Remix__NSYNC': 3,\n",
              " 'Gone, Gone__Juicy J': 60,\n",
              " 'Got To Give It Up (Part 1)__Marvin Gaye': 51,\n",
              " 'Headlines__Drake': 58,\n",
              " \"Hold On, We're Going Home__Drake\": 68,\n",
              " 'Hotline Bling__Drake': 39,\n",
              " \"I Know There's Gonna Be (Good Times)__Jamie xx\": 38,\n",
              " 'I See the Light - From \"Tangled\"/Soundtrack Version__Mandy Moore': 151,\n",
              " 'I Wanna Be With You__DJ Khaled': 65,\n",
              " 'I Want It That Way__Backstreet Boys': 26,\n",
              " 'I Want You - Single Version__Marvin Gaye': 52,\n",
              " 'I Want You Back - Radio Edit__NSYNC': 4,\n",
              " 'I Wish__Stevie Wonder': 57,\n",
              " \"I'll Make Love To You__Boyz II Men\": 12,\n",
              " \"I'll Never Break Your Heart__Backstreet Boys\": 17,\n",
              " \"I'm the One__DJ Khaled\": 114,\n",
              " 'If I Were a Boy__Beyoncé': 24,\n",
              " \"It's Gonna Be Me__NSYNC\": 2,\n",
              " 'Jumpman__Drake': 32,\n",
              " 'Kingdom Come__Demi Lovato': 122,\n",
              " 'Know Yourself__Drake': 43,\n",
              " 'Kush__Lil Wayne': 40,\n",
              " 'Kyoto (feat. Sirah)__Skrillex': 107,\n",
              " \"Let's Dance (Single Version) [2002 Remastered Version]__David Bowie\": 76,\n",
              " 'Levels - Radio Edit__Avicii': 9,\n",
              " 'Live Your Life - feat. Rihanna__T.I.': 91,\n",
              " 'Lollipop__Lil Wayne': 89,\n",
              " 'Loop__NELL': 110,\n",
              " 'Loud Places__Jamie xx': 36,\n",
              " 'Lucky__Britney Spears': 30,\n",
              " 'Malibu__Miley Cyrus': 80,\n",
              " 'Mamacita__Travis Scott': 56,\n",
              " 'Man In The Mirror__Michael Jackson': 146,\n",
              " 'Maps__Maroon 5': 96,\n",
              " 'Mary Jane (Jamie Xx - Girl Remix)__Jamie xx': 71,\n",
              " 'Mercy__Kanye West': 47,\n",
              " 'Monster__Kanye West': 33,\n",
              " 'My Boo__Usher': 140,\n",
              " 'My House__Flo Rida': 99,\n",
              " 'New Slaves__Kanye West': 46,\n",
              " 'No New Friends - SFTB Remix__DJ Khaled': 67,\n",
              " 'No Vacancy__OneRepublic': 79,\n",
              " 'No Woman__Whitney': 72,\n",
              " 'Nosedive__Dynamic Duo': 109,\n",
              " 'Old Ways__Demi Lovato': 119,\n",
              " 'On Bended Knee__Boyz II Men': 10,\n",
              " 'One of Those Nights__Juicy J': 62,\n",
              " 'P.Y.T. (Pretty Young Thing)__Michael Jackson': 147,\n",
              " 'Paris - Pegboard Nerds Remix__The Chainsmokers': 159,\n",
              " 'Passionfruit__Drake': 86,\n",
              " 'Pop - Radio Version__NSYNC': 5,\n",
              " 'Popular Song__MIKA': 130,\n",
              " 'Quit Playing Games (With My Heart)__Backstreet Boys': 16,\n",
              " 'Remember the Time__Michael Jackson': 154,\n",
              " 'Return Of The Mack - C & J Street Mix__Mark Morrison': 34,\n",
              " 'Right In__Skrillex': 102,\n",
              " 'Right On Time__Skrillex': 106,\n",
              " 'Rollin__Calvin Harris': 82,\n",
              " 'Sexy Bitch (feat. Akon) - Featuring Akon;explicit__David Guetta': 95,\n",
              " 'Sexy Love__Ne-Yo': 137,\n",
              " 'SexyBack__Justin Timberlake': 152,\n",
              " 'Shape of My Heart__Backstreet Boys': 22,\n",
              " 'Shape of You (Major Lazer Remix) [feat. Nyla & Kranium]__Ed Sheeran': 158,\n",
              " 'Shape of You__Ed Sheeran': 87,\n",
              " 'Show Me the Meaning of Being Lonely__Backstreet Boys': 21,\n",
              " 'Skepta Interlude__Drake': 78,\n",
              " 'Skrillex Orchestral Suite By Varien - Bonus Track__Skrillex': 108,\n",
              " 'Slow Jam__Usher': 138,\n",
              " 'So Sick__Ne-Yo': 135,\n",
              " 'Someone To Call My Lover__Janet Jackson': 132,\n",
              " 'Something Just Like This - ARMNHMR Remix__The Chainsmokers': 161,\n",
              " 'Something Just Like This - Don Diablo Remix__The Chainsmokers': 113,\n",
              " 'Started From the Bottom__Drake': 59,\n",
              " 'Stay (with Alessia Cara)__Zedd': 81,\n",
              " 'Stay__Ne-Yo': 134,\n",
              " 'Stone Cold__Demi Lovato': 121,\n",
              " 'String Quintet in C Major, Op. 29: II. Adagio molto espressivo__Ludwig van Beethoven': 125,\n",
              " \"Sugah Daddy__D'Angelo\": 37,\n",
              " 'Swords__M.I.A.': 53,\n",
              " 'Take Care__Drake': 101,\n",
              " 'Take Me There - Remix__Blackstreet': 23,\n",
              " \"Tearin' up My Heart - Radio Edit__NSYNC\": 6,\n",
              " 'The Call__Backstreet Boys': 19,\n",
              " \"The Devil's Den__Skrillex\": 105,\n",
              " 'The Monster__Eminem': 94,\n",
              " 'The One__The Chainsmokers': 85,\n",
              " 'The Way You Make Me Feel__Michael Jackson': 145,\n",
              " 'This I Promise You - Radio Edit__NSYNC': 1,\n",
              " 'Till I Collapse__Eminem': 127,\n",
              " 'Titanium (feat. Sia)__David Guetta': 92,\n",
              " 'Together Again__Janet Jackson': 131,\n",
              " 'U Got It Bad__Usher': 139,\n",
              " 'Unbreakable__Janet Jackson': 55,\n",
              " 'Upper Echelon__Travis Scott': 50,\n",
              " 'Waitin for You__Demi Lovato': 123,\n",
              " 'Water Runs Dry__Boyz II Men': 13,\n",
              " 'What 2 Do__DEAN': 111,\n",
              " 'When Will My Life Begin - From \"Tangled\"/Soundtrack Version__Mandy Moore': 150,\n",
              " \"When You're Mad__Ne-Yo\": 136,\n",
              " 'Whippin (feat. Felix Snow)__Kiiara': 83,\n",
              " 'Wild Ones (feat. Sia)__Flo Rida': 88,\n",
              " 'Wildfire__Demi Lovato': 124,\n",
              " 'With You__Chris Brown': 148,\n",
              " 'Wu-Tang Forever__Drake': 44,\n",
              " 'XO TOUR Llif3__Lil Uzi Vert': 116,\n",
              " 'You Make Me Wanna...__Usher': 126,\n",
              " 'Young__The Chainsmokers': 115}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ga1vIhsOGhEZ",
        "outputId": "3d7aea10-7eab-42ac-ad2c-cb6b5d86987a"
      },
      "source": [
        "users=[]\n",
        "for user in ratings.index:\n",
        "  users.extend([user]*len(ratings.columns))\n",
        " \n",
        "tracks = list(ratings.columns)*len(ratings.index)\n",
        "score=[]\n",
        "for i in range(len(ratings.index)):\n",
        "  for j in range(len(ratings.columns)):\n",
        "    score.append(float(ratings.iloc[i,j]))\n",
        "temp={'users':users,'tracks':tracks,'score':score}\n",
        "data = pd.DataFrame(temp)\n",
        "display(data.head(50))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>users</th>\n",
              "      <th>tracks</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0</td>\n",
              "      <td>47</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    users  tracks  score\n",
              "0       0       0    1.0\n",
              "1       0       1    0.0\n",
              "2       0       2    1.0\n",
              "3       0       3    1.0\n",
              "4       0       4    0.0\n",
              "5       0       5    1.0\n",
              "6       0       6    1.0\n",
              "7       0       7    0.0\n",
              "8       0       8    1.0\n",
              "9       0       9    0.0\n",
              "10      0      10    1.0\n",
              "11      0      11    1.0\n",
              "12      0      12    1.0\n",
              "13      0      13    0.0\n",
              "14      0      14    1.0\n",
              "15      0      15    1.0\n",
              "16      0      16    0.0\n",
              "17      0      17    0.0\n",
              "18      0      18    1.0\n",
              "19      0      19    0.0\n",
              "20      0      20    1.0\n",
              "21      0      21    0.0\n",
              "22      0      22    1.0\n",
              "23      0      23    0.0\n",
              "24      0      24    0.0\n",
              "25      0      25    1.0\n",
              "26      0      26    1.0\n",
              "27      0      27    0.0\n",
              "28      0      28    0.0\n",
              "29      0      29    1.0\n",
              "30      0      30    0.0\n",
              "31      0      31    0.0\n",
              "32      0      32    0.0\n",
              "33      0      33    0.0\n",
              "34      0      34    0.0\n",
              "35      0      35    0.0\n",
              "36      0      36    0.0\n",
              "37      0      37    1.0\n",
              "38      0      38    0.0\n",
              "39      0      39    1.0\n",
              "40      0      40    0.0\n",
              "41      0      41    0.0\n",
              "42      0      42    0.0\n",
              "43      0      43    0.0\n",
              "44      0      44    0.0\n",
              "45      0      45    0.0\n",
              "46      0      46    0.0\n",
              "47      0      47    0.0\n",
              "48      0      48    0.0\n",
              "49      0      49    0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CdCq4wBH4bJC",
        "outputId": "193b7bd8-26dd-4e09-92e3-95a623524010"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "TRAIN_SIZE = 0.8\n",
        "data = shuffle(data,random_state=3)\n",
        "data = shuffle(data,random_state=33)\n",
        "\n",
        "test_cutoff = int(TRAIN_SIZE * len(data))\n",
        "ratings_train = data.iloc[:test_cutoff]\n",
        "ratings_test = data.iloc[test_cutoff:]\n",
        "\n",
        "valid_cutoff = int(0.5*len(ratings_test))\n",
        "ratings_test=shuffle(ratings_test,random_state=33)\n",
        "ratings_valid = ratings_test.iloc[:valid_cutoff]\n",
        "ratings_test = ratings_test.iloc[valid_cutoff:]\n",
        "display(ratings_train,ratings_test,ratings_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>users</th>\n",
              "      <th>tracks</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3797</th>\n",
              "      <td>23</td>\n",
              "      <td>71</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2162</th>\n",
              "      <td>13</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1297</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2825</th>\n",
              "      <td>17</td>\n",
              "      <td>71</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1199</th>\n",
              "      <td>7</td>\n",
              "      <td>65</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2752</th>\n",
              "      <td>16</td>\n",
              "      <td>160</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>2</td>\n",
              "      <td>40</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>3</td>\n",
              "      <td>132</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4371</th>\n",
              "      <td>26</td>\n",
              "      <td>159</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4465</th>\n",
              "      <td>27</td>\n",
              "      <td>91</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3758 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      users  tracks  score\n",
              "3797     23      71   -1.0\n",
              "2162     13      56    0.0\n",
              "1297      8       1   -1.0\n",
              "2825     17      71   -1.0\n",
              "1199      7      65    1.0\n",
              "...     ...     ...    ...\n",
              "2752     16     160    1.0\n",
              "364       2      40   -1.0\n",
              "618       3     132    1.0\n",
              "4371     26     159    1.0\n",
              "4465     27      91    0.0\n",
              "\n",
              "[3758 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>users</th>\n",
              "      <th>tracks</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4108</th>\n",
              "      <td>25</td>\n",
              "      <td>58</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4401</th>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1961</th>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1167</th>\n",
              "      <td>7</td>\n",
              "      <td>33</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2730</th>\n",
              "      <td>16</td>\n",
              "      <td>138</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3197</th>\n",
              "      <td>19</td>\n",
              "      <td>119</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4148</th>\n",
              "      <td>25</td>\n",
              "      <td>98</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4204</th>\n",
              "      <td>25</td>\n",
              "      <td>154</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4243</th>\n",
              "      <td>26</td>\n",
              "      <td>31</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3608</th>\n",
              "      <td>22</td>\n",
              "      <td>44</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>470 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      users  tracks  score\n",
              "4108     25      58    1.0\n",
              "4401     27      27    1.0\n",
              "1961     12      17   -1.0\n",
              "1167      7      33    0.0\n",
              "2730     16     138    0.0\n",
              "...     ...     ...    ...\n",
              "3197     19     119    1.0\n",
              "4148     25      98    1.0\n",
              "4204     25     154   -1.0\n",
              "4243     26      31    1.0\n",
              "3608     22      44    0.0\n",
              "\n",
              "[470 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>users</th>\n",
              "      <th>tracks</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3091</th>\n",
              "      <td>19</td>\n",
              "      <td>13</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>669</th>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>0</td>\n",
              "      <td>117</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4575</th>\n",
              "      <td>28</td>\n",
              "      <td>39</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2420</th>\n",
              "      <td>14</td>\n",
              "      <td>152</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0</td>\n",
              "      <td>97</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4012</th>\n",
              "      <td>24</td>\n",
              "      <td>124</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2297</th>\n",
              "      <td>14</td>\n",
              "      <td>29</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>1</td>\n",
              "      <td>71</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>944</th>\n",
              "      <td>5</td>\n",
              "      <td>134</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>470 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      users  tracks  score\n",
              "3091     19      13    0.0\n",
              "669       4      21    0.0\n",
              "117       0     117    0.0\n",
              "4575     28      39   -1.0\n",
              "2420     14     152    1.0\n",
              "...     ...     ...    ...\n",
              "97        0      97    0.0\n",
              "4012     24     124    1.0\n",
              "2297     14      29    1.0\n",
              "233       1      71    0.0\n",
              "944       5     134    1.0\n",
              "\n",
              "[470 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppdXe9Dz34-Z",
        "outputId": "7c93795f-6509-402d-aca0-5825b1761861"
      },
      "source": [
        "# Variable 초기화                      # Latent factor 수 \n",
        "mu = round(ratings_train.score.mean(),4)  # 전체 평균 \n",
        "M = len(ratings.index)    # Number of users\n",
        "N = len(ratings.columns) # Number of movies\n",
        "mu,M,N      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2743, 29, 162)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZMtJ9hB5sip"
      },
      "source": [
        "# Defining RMSE measure\n",
        "def RMSE(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhhDAV7A8Q2l"
      },
      "source": [
        "### Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWtTroykRyrj"
      },
      "source": [
        "### (1) Baseline "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uMgXOE7s8PvM",
        "outputId": "36af00b4-d511-4fed-fc2a-be6ff622c69f"
      },
      "source": [
        "NN_Models=defaultdict(dict)\n",
        "#1차 범위 : 5,10,15,..,30 : 10,15가 최소\n",
        "#2차 범위 : 10,11,12,13,14,15 : 13이 최소 => 에폭수 1000으로 설정했음 : 50부터 증가 하는데 그래서 100까지로 삼으면 될듯 싶음\n",
        "\n",
        "epochs = 300\n",
        "PREVIOUS=10\n",
        "for initializer in ['GlorotNormal','GlorotUniform']:\n",
        "  for K in range(10,15+1):  \n",
        "    print('------------------------------------------------------------------')\n",
        "    print(f'Init = {initializer}')\n",
        "    # Keras model\n",
        "    user = Input(shape=(1, ))                                               # User input\n",
        "    item = Input(shape=(1, ))                                               # Item input\n",
        "    P_embedding = Embedding(M, K,embeddings_initializer=initializer, embeddings_regularizer=l2())(user)        # (M, 1, K)\n",
        "    Q_embedding = Embedding(N, K,embeddings_initializer=initializer, embeddings_regularizer=l2())(item)        # (N, 1, K)\n",
        "    user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)          # User bias term (M, 1, )\n",
        "    item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)          # Item bias term (N, 1, )\n",
        "\n",
        "    #R = Dot(axes=2)([P_embedding, Q_embedding)\n",
        "    R = layers.dot([P_embedding, Q_embedding], axes=2)                      # (1, 1, 1)\n",
        "    #R = Add()([R, user_bias, item_bias])\n",
        "    R = layers.add([R, user_bias, item_bias])\n",
        "    R = Flatten()(R)                                                        # (1, 1)\n",
        "\n",
        "\n",
        "    # Model setting\n",
        "    starter_learning_rate = 0.1\n",
        "    end_learning_rate = 0.0001\n",
        "    decay_steps = int(0.1*epochs)\n",
        "    learning_rate_fn = PolynomialDecay(\n",
        "        starter_learning_rate,\n",
        "        decay_steps,\n",
        "        end_learning_rate,\n",
        "        power=0.1)\n",
        "    model = Model(inputs=[user, item], outputs=R)\n",
        "\n",
        "    model.compile(\n",
        "      loss=RMSE,\n",
        "      optimizer=SGD(learning_rate=learning_rate_fn,momentum=0.9),\n",
        "      #optimizer=Adamax(),\n",
        "      metrics=[RMSE],\n",
        "      \n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    # Model fitting\n",
        "    result = model.fit(\n",
        "      x=[ratings_train.users.values, ratings_train.tracks.values],\n",
        "      y=ratings_train.score.values - mu,\n",
        "      epochs=epochs,\n",
        "      batch_size=64,\n",
        "      validation_data=(\n",
        "        [ratings_valid.users.values, ratings_valid.tracks.values],\n",
        "        ratings_valid.score.values - mu\n",
        "      )\n",
        "    )\n",
        "\n",
        "      # Plot RMSE\n",
        "    plt.plot(result.history['RMSE'], label=\"Train RMSE\")\n",
        "    plt.plot(result.history['val_RMSE'], label=\"Test RMSE\")\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    MIN= min(result.history['val_RMSE'])\n",
        "    \n",
        "    if MIN < PREVIOUS:\n",
        "      try:  \n",
        "        NN_Models[initializer]['K']=K\n",
        "        NN_Models[initializer]['model']= model\n",
        "        NN_Models[initializer]['loss'] = result.history['val_RMSE']\n",
        "        NN_Models[initializer]['min_loss'] = MIN\n",
        "        PREVIOUS = MIN\n",
        "      except:\n",
        "        continue\n",
        "    \n",
        "    print(f'K={K} / Init = {initializer} / min_loss = {MIN}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------\n",
            "Init = GlorotNormal\n",
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_23 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_24 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_44 (Embedding)        (None, 1, 10)        290         input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_45 (Embedding)        (None, 1, 10)        1620        input_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_11 (Dot)                    (None, 1, 1)         0           embedding_44[0][0]               \n",
            "                                                                 embedding_45[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_46 (Embedding)        (None, 1, 1)         29          input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_47 (Embedding)        (None, 1, 1)         162         input_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 1, 1)         0           dot_11[0][0]                     \n",
            "                                                                 embedding_46[0][0]               \n",
            "                                                                 embedding_47[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 1)            0           add_11[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,101\n",
            "Trainable params: 2,101\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.0238 - RMSE: 0.7906 - val_loss: 0.8709 - val_RMSE: 0.7771\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8548 - RMSE: 0.7590 - val_loss: 0.8690 - val_RMSE: 0.7770\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8480 - RMSE: 0.7530 - val_loss: 0.8687 - val_RMSE: 0.7769\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8543 - RMSE: 0.7596 - val_loss: 0.8685 - val_RMSE: 0.7769\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8585 - RMSE: 0.7639 - val_loss: 0.8682 - val_RMSE: 0.7768\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8614 - RMSE: 0.7670 - val_loss: 0.8680 - val_RMSE: 0.7768\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8540 - RMSE: 0.7599 - val_loss: 0.8677 - val_RMSE: 0.7767\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8549 - RMSE: 0.7610 - val_loss: 0.8675 - val_RMSE: 0.7767\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8567 - RMSE: 0.7629 - val_loss: 0.8672 - val_RMSE: 0.7766\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8590 - RMSE: 0.7655 - val_loss: 0.8670 - val_RMSE: 0.7766\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8505 - RMSE: 0.7571 - val_loss: 0.8668 - val_RMSE: 0.7765\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8512 - RMSE: 0.7580 - val_loss: 0.8665 - val_RMSE: 0.7765\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8490 - RMSE: 0.7560 - val_loss: 0.8663 - val_RMSE: 0.7764\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8517 - RMSE: 0.7589 - val_loss: 0.8660 - val_RMSE: 0.7764\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8555 - RMSE: 0.7628 - val_loss: 0.8658 - val_RMSE: 0.7764\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8457 - RMSE: 0.7532 - val_loss: 0.8656 - val_RMSE: 0.7763\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8474 - RMSE: 0.7551 - val_loss: 0.8653 - val_RMSE: 0.7763\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8562 - RMSE: 0.7642 - val_loss: 0.8651 - val_RMSE: 0.7762\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8380 - RMSE: 0.7462 - val_loss: 0.8649 - val_RMSE: 0.7762\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8628 - RMSE: 0.7711 - val_loss: 0.8646 - val_RMSE: 0.7761\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8507 - RMSE: 0.7593 - val_loss: 0.8644 - val_RMSE: 0.7761\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8466 - RMSE: 0.7554 - val_loss: 0.8642 - val_RMSE: 0.7760\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8540 - RMSE: 0.7629 - val_loss: 0.8639 - val_RMSE: 0.7760\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8464 - RMSE: 0.7555 - val_loss: 0.8637 - val_RMSE: 0.7759\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8535 - RMSE: 0.7628 - val_loss: 0.8635 - val_RMSE: 0.7759\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8477 - RMSE: 0.7572 - val_loss: 0.8633 - val_RMSE: 0.7759\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8476 - RMSE: 0.7573 - val_loss: 0.8630 - val_RMSE: 0.7758\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8509 - RMSE: 0.7607 - val_loss: 0.8628 - val_RMSE: 0.7758\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8441 - RMSE: 0.7541 - val_loss: 0.8626 - val_RMSE: 0.7757\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8448 - RMSE: 0.7550 - val_loss: 0.8623 - val_RMSE: 0.7757\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8572 - RMSE: 0.7676 - val_loss: 0.8621 - val_RMSE: 0.7756\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8498 - RMSE: 0.7604 - val_loss: 0.8619 - val_RMSE: 0.7756\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8484 - RMSE: 0.7592 - val_loss: 0.8617 - val_RMSE: 0.7756\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8417 - RMSE: 0.7527 - val_loss: 0.8614 - val_RMSE: 0.7755\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8487 - RMSE: 0.7599 - val_loss: 0.8612 - val_RMSE: 0.7755\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8459 - RMSE: 0.7572 - val_loss: 0.8610 - val_RMSE: 0.7754\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8394 - RMSE: 0.7509 - val_loss: 0.8608 - val_RMSE: 0.7754\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8421 - RMSE: 0.7538 - val_loss: 0.8606 - val_RMSE: 0.7754\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8471 - RMSE: 0.7590 - val_loss: 0.8603 - val_RMSE: 0.7753\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8385 - RMSE: 0.7505 - val_loss: 0.8601 - val_RMSE: 0.7753\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8472 - RMSE: 0.7595 - val_loss: 0.8599 - val_RMSE: 0.7752\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8375 - RMSE: 0.7499 - val_loss: 0.8597 - val_RMSE: 0.7752\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8427 - RMSE: 0.7553 - val_loss: 0.8595 - val_RMSE: 0.7751\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8505 - RMSE: 0.7633 - val_loss: 0.8592 - val_RMSE: 0.7751\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8497 - RMSE: 0.7627 - val_loss: 0.8590 - val_RMSE: 0.7751\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8380 - RMSE: 0.7511 - val_loss: 0.8588 - val_RMSE: 0.7750\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8436 - RMSE: 0.7570 - val_loss: 0.8586 - val_RMSE: 0.7750\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8400 - RMSE: 0.7535 - val_loss: 0.8584 - val_RMSE: 0.7749\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8399 - RMSE: 0.7536 - val_loss: 0.8582 - val_RMSE: 0.7749\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8434 - RMSE: 0.7573 - val_loss: 0.8579 - val_RMSE: 0.7749\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8430 - RMSE: 0.7571 - val_loss: 0.8577 - val_RMSE: 0.7748\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8427 - RMSE: 0.7569 - val_loss: 0.8575 - val_RMSE: 0.7748\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8436 - RMSE: 0.7580 - val_loss: 0.8573 - val_RMSE: 0.7747\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8414 - RMSE: 0.7559 - val_loss: 0.8571 - val_RMSE: 0.7747\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8460 - RMSE: 0.7607 - val_loss: 0.8569 - val_RMSE: 0.7747\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8447 - RMSE: 0.7596 - val_loss: 0.8567 - val_RMSE: 0.7746\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8375 - RMSE: 0.7526 - val_loss: 0.8565 - val_RMSE: 0.7746\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8417 - RMSE: 0.7570 - val_loss: 0.8562 - val_RMSE: 0.7745\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8350 - RMSE: 0.7504 - val_loss: 0.8560 - val_RMSE: 0.7745\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8403 - RMSE: 0.7559 - val_loss: 0.8558 - val_RMSE: 0.7745\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8484 - RMSE: 0.7642 - val_loss: 0.8556 - val_RMSE: 0.7744\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8413 - RMSE: 0.7573 - val_loss: 0.8554 - val_RMSE: 0.7744\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8426 - RMSE: 0.7587 - val_loss: 0.8552 - val_RMSE: 0.7744\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8349 - RMSE: 0.7512 - val_loss: 0.8550 - val_RMSE: 0.7743\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8495 - RMSE: 0.7660 - val_loss: 0.8548 - val_RMSE: 0.7743\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8516 - RMSE: 0.7682 - val_loss: 0.8546 - val_RMSE: 0.7742\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8519 - RMSE: 0.7687 - val_loss: 0.8544 - val_RMSE: 0.7742\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8427 - RMSE: 0.7596 - val_loss: 0.8542 - val_RMSE: 0.7742\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8419 - RMSE: 0.7591 - val_loss: 0.8540 - val_RMSE: 0.7741\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8334 - RMSE: 0.7507 - val_loss: 0.8538 - val_RMSE: 0.7741\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8400 - RMSE: 0.7575 - val_loss: 0.8536 - val_RMSE: 0.7741\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8421 - RMSE: 0.7597 - val_loss: 0.8534 - val_RMSE: 0.7740\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8392 - RMSE: 0.7570 - val_loss: 0.8532 - val_RMSE: 0.7740\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8394 - RMSE: 0.7574 - val_loss: 0.8530 - val_RMSE: 0.7740\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8485 - RMSE: 0.7667 - val_loss: 0.8528 - val_RMSE: 0.7739\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8365 - RMSE: 0.7548 - val_loss: 0.8526 - val_RMSE: 0.7739\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8311 - RMSE: 0.7495 - val_loss: 0.8524 - val_RMSE: 0.7738\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8427 - RMSE: 0.7613 - val_loss: 0.8522 - val_RMSE: 0.7738\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8330 - RMSE: 0.7518 - val_loss: 0.8520 - val_RMSE: 0.7738\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8373 - RMSE: 0.7562 - val_loss: 0.8518 - val_RMSE: 0.7737\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8318 - RMSE: 0.7509 - val_loss: 0.8516 - val_RMSE: 0.7737\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8368 - RMSE: 0.7561 - val_loss: 0.8514 - val_RMSE: 0.7737\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8453 - RMSE: 0.7648 - val_loss: 0.8512 - val_RMSE: 0.7736\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8376 - RMSE: 0.7572 - val_loss: 0.8510 - val_RMSE: 0.7736\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8280 - RMSE: 0.7477 - val_loss: 0.8508 - val_RMSE: 0.7736\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8363 - RMSE: 0.7562 - val_loss: 0.8506 - val_RMSE: 0.7735\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8423 - RMSE: 0.7623 - val_loss: 0.8504 - val_RMSE: 0.7735\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8335 - RMSE: 0.7537 - val_loss: 0.8502 - val_RMSE: 0.7735\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8364 - RMSE: 0.7568 - val_loss: 0.8500 - val_RMSE: 0.7734\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8332 - RMSE: 0.7538 - val_loss: 0.8499 - val_RMSE: 0.7734\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8320 - RMSE: 0.7528 - val_loss: 0.8497 - val_RMSE: 0.7734\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8348 - RMSE: 0.7556 - val_loss: 0.8495 - val_RMSE: 0.7733\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8376 - RMSE: 0.7586 - val_loss: 0.8493 - val_RMSE: 0.7733\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8388 - RMSE: 0.7600 - val_loss: 0.8491 - val_RMSE: 0.7733\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8378 - RMSE: 0.7591 - val_loss: 0.8489 - val_RMSE: 0.7732\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8364 - RMSE: 0.7579 - val_loss: 0.8487 - val_RMSE: 0.7732\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8290 - RMSE: 0.7506 - val_loss: 0.8485 - val_RMSE: 0.7732\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8283 - RMSE: 0.7501 - val_loss: 0.8483 - val_RMSE: 0.7731\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8352 - RMSE: 0.7572 - val_loss: 0.8482 - val_RMSE: 0.7731\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8293 - RMSE: 0.7514 - val_loss: 0.8480 - val_RMSE: 0.7731\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8259 - RMSE: 0.7482 - val_loss: 0.8478 - val_RMSE: 0.7730\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8334 - RMSE: 0.7558 - val_loss: 0.8476 - val_RMSE: 0.7730\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8328 - RMSE: 0.7554 - val_loss: 0.8474 - val_RMSE: 0.7730\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8284 - RMSE: 0.7511 - val_loss: 0.8472 - val_RMSE: 0.7729\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8383 - RMSE: 0.7612 - val_loss: 0.8471 - val_RMSE: 0.7729\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8320 - RMSE: 0.7551 - val_loss: 0.8469 - val_RMSE: 0.7729\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8260 - RMSE: 0.7491 - val_loss: 0.8467 - val_RMSE: 0.7728\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8324 - RMSE: 0.7558 - val_loss: 0.8465 - val_RMSE: 0.7728\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8333 - RMSE: 0.7568 - val_loss: 0.8463 - val_RMSE: 0.7728\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8412 - RMSE: 0.7648 - val_loss: 0.8462 - val_RMSE: 0.7728\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8314 - RMSE: 0.7552 - val_loss: 0.8460 - val_RMSE: 0.7727\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8310 - RMSE: 0.7550 - val_loss: 0.8458 - val_RMSE: 0.7727\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8295 - RMSE: 0.7536 - val_loss: 0.8456 - val_RMSE: 0.7727\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8407 - RMSE: 0.7649 - val_loss: 0.8454 - val_RMSE: 0.7726\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8301 - RMSE: 0.7545 - val_loss: 0.8453 - val_RMSE: 0.7726\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8340 - RMSE: 0.7585 - val_loss: 0.8451 - val_RMSE: 0.7726\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8222 - RMSE: 0.7469 - val_loss: 0.8449 - val_RMSE: 0.7725\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8233 - RMSE: 0.7482 - val_loss: 0.8447 - val_RMSE: 0.7725\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8326 - RMSE: 0.7576 - val_loss: 0.8446 - val_RMSE: 0.7725\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8282 - RMSE: 0.7533 - val_loss: 0.8444 - val_RMSE: 0.7725\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8328 - RMSE: 0.7581 - val_loss: 0.8442 - val_RMSE: 0.7724\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8433 - RMSE: 0.7688 - val_loss: 0.8440 - val_RMSE: 0.7724\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8220 - RMSE: 0.7476 - val_loss: 0.8439 - val_RMSE: 0.7724\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8257 - RMSE: 0.7514 - val_loss: 0.8437 - val_RMSE: 0.7723\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8388 - RMSE: 0.7647 - val_loss: 0.8435 - val_RMSE: 0.7723\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8094 - RMSE: 0.7354 - val_loss: 0.8433 - val_RMSE: 0.7723\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8309 - RMSE: 0.7571 - val_loss: 0.8432 - val_RMSE: 0.7722\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8332 - RMSE: 0.7595 - val_loss: 0.8430 - val_RMSE: 0.7722\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8310 - RMSE: 0.7574 - val_loss: 0.8428 - val_RMSE: 0.7722\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8244 - RMSE: 0.7509 - val_loss: 0.8426 - val_RMSE: 0.7722\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8243 - RMSE: 0.7510 - val_loss: 0.8425 - val_RMSE: 0.7721\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8362 - RMSE: 0.7631 - val_loss: 0.8423 - val_RMSE: 0.7721\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8211 - RMSE: 0.7481 - val_loss: 0.8421 - val_RMSE: 0.7721\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8272 - RMSE: 0.7543 - val_loss: 0.8420 - val_RMSE: 0.7721\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8327 - RMSE: 0.7600 - val_loss: 0.8418 - val_RMSE: 0.7720\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8372 - RMSE: 0.7646 - val_loss: 0.8416 - val_RMSE: 0.7720\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8269 - RMSE: 0.7545 - val_loss: 0.8415 - val_RMSE: 0.7720\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8282 - RMSE: 0.7559 - val_loss: 0.8413 - val_RMSE: 0.7719\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8295 - RMSE: 0.7574 - val_loss: 0.8411 - val_RMSE: 0.7719\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8269 - RMSE: 0.7549 - val_loss: 0.8410 - val_RMSE: 0.7719\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8299 - RMSE: 0.7580 - val_loss: 0.8408 - val_RMSE: 0.7719\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8287 - RMSE: 0.7570 - val_loss: 0.8406 - val_RMSE: 0.7718\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8370 - RMSE: 0.7654 - val_loss: 0.8405 - val_RMSE: 0.7718\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8238 - RMSE: 0.7524 - val_loss: 0.8403 - val_RMSE: 0.7718\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8247 - RMSE: 0.7534 - val_loss: 0.8401 - val_RMSE: 0.7717\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8209 - RMSE: 0.7498 - val_loss: 0.8400 - val_RMSE: 0.7717\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8184 - RMSE: 0.7473 - val_loss: 0.8398 - val_RMSE: 0.7717\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8259 - RMSE: 0.7550 - val_loss: 0.8397 - val_RMSE: 0.7717\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8127 - RMSE: 0.7419 - val_loss: 0.8395 - val_RMSE: 0.7716\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8255 - RMSE: 0.7549 - val_loss: 0.8393 - val_RMSE: 0.7716\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8175 - RMSE: 0.7470 - val_loss: 0.8392 - val_RMSE: 0.7716\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8298 - RMSE: 0.7595 - val_loss: 0.8390 - val_RMSE: 0.7716\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8238 - RMSE: 0.7536 - val_loss: 0.8388 - val_RMSE: 0.7715\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8191 - RMSE: 0.7490 - val_loss: 0.8387 - val_RMSE: 0.7715\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8275 - RMSE: 0.7575 - val_loss: 0.8385 - val_RMSE: 0.7715\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8273 - RMSE: 0.7575 - val_loss: 0.8384 - val_RMSE: 0.7715\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8252 - RMSE: 0.7555 - val_loss: 0.8382 - val_RMSE: 0.7714\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8294 - RMSE: 0.7598 - val_loss: 0.8381 - val_RMSE: 0.7714\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8292 - RMSE: 0.7598 - val_loss: 0.8379 - val_RMSE: 0.7714\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8199 - RMSE: 0.7506 - val_loss: 0.8377 - val_RMSE: 0.7713\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8301 - RMSE: 0.7610 - val_loss: 0.8376 - val_RMSE: 0.7713\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8217 - RMSE: 0.7526 - val_loss: 0.8374 - val_RMSE: 0.7713\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8151 - RMSE: 0.7462 - val_loss: 0.8373 - val_RMSE: 0.7713\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8273 - RMSE: 0.7585 - val_loss: 0.8371 - val_RMSE: 0.7712\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8240 - RMSE: 0.7554 - val_loss: 0.8370 - val_RMSE: 0.7712\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8293 - RMSE: 0.7608 - val_loss: 0.8368 - val_RMSE: 0.7712\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8173 - RMSE: 0.7490 - val_loss: 0.8367 - val_RMSE: 0.7712\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8253 - RMSE: 0.7571 - val_loss: 0.8365 - val_RMSE: 0.7711\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8260 - RMSE: 0.7579 - val_loss: 0.8363 - val_RMSE: 0.7711\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8253 - RMSE: 0.7573 - val_loss: 0.8362 - val_RMSE: 0.7711\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8194 - RMSE: 0.7515 - val_loss: 0.8360 - val_RMSE: 0.7711\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8301 - RMSE: 0.7624 - val_loss: 0.8359 - val_RMSE: 0.7710\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8090 - RMSE: 0.7414 - val_loss: 0.8357 - val_RMSE: 0.7710\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8130 - RMSE: 0.7456 - val_loss: 0.8356 - val_RMSE: 0.7710\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8226 - RMSE: 0.7553 - val_loss: 0.8354 - val_RMSE: 0.7710\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8145 - RMSE: 0.7473 - val_loss: 0.8353 - val_RMSE: 0.7709\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8174 - RMSE: 0.7503 - val_loss: 0.8351 - val_RMSE: 0.7709\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8207 - RMSE: 0.7538 - val_loss: 0.8350 - val_RMSE: 0.7709\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8233 - RMSE: 0.7565 - val_loss: 0.8348 - val_RMSE: 0.7709\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8033 - RMSE: 0.7366 - val_loss: 0.8347 - val_RMSE: 0.7708\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8217 - RMSE: 0.7551 - val_loss: 0.8345 - val_RMSE: 0.7708\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8217 - RMSE: 0.7552 - val_loss: 0.8344 - val_RMSE: 0.7708\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8207 - RMSE: 0.7544 - val_loss: 0.8342 - val_RMSE: 0.7708\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8177 - RMSE: 0.7515 - val_loss: 0.8341 - val_RMSE: 0.7707\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8155 - RMSE: 0.7494 - val_loss: 0.8339 - val_RMSE: 0.7707\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8211 - RMSE: 0.7551 - val_loss: 0.8338 - val_RMSE: 0.7707\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8301 - RMSE: 0.7643 - val_loss: 0.8337 - val_RMSE: 0.7707\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8183 - RMSE: 0.7526 - val_loss: 0.8335 - val_RMSE: 0.7707\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8213 - RMSE: 0.7557 - val_loss: 0.8334 - val_RMSE: 0.7706\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8207 - RMSE: 0.7552 - val_loss: 0.8332 - val_RMSE: 0.7706\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8204 - RMSE: 0.7551 - val_loss: 0.8331 - val_RMSE: 0.7706\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8173 - RMSE: 0.7521 - val_loss: 0.8329 - val_RMSE: 0.7706\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8220 - RMSE: 0.7569 - val_loss: 0.8328 - val_RMSE: 0.7705\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8074 - RMSE: 0.7424 - val_loss: 0.8326 - val_RMSE: 0.7705\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8276 - RMSE: 0.7627 - val_loss: 0.8325 - val_RMSE: 0.7705\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8162 - RMSE: 0.7515 - val_loss: 0.8324 - val_RMSE: 0.7705\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8242 - RMSE: 0.7596 - val_loss: 0.8322 - val_RMSE: 0.7705\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8308 - RMSE: 0.7663 - val_loss: 0.8321 - val_RMSE: 0.7704\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8124 - RMSE: 0.7480 - val_loss: 0.8319 - val_RMSE: 0.7704\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8102 - RMSE: 0.7459 - val_loss: 0.8318 - val_RMSE: 0.7704\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8145 - RMSE: 0.7504 - val_loss: 0.8317 - val_RMSE: 0.7704\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8124 - RMSE: 0.7484 - val_loss: 0.8315 - val_RMSE: 0.7703\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8226 - RMSE: 0.7587 - val_loss: 0.8314 - val_RMSE: 0.7703\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8155 - RMSE: 0.7517 - val_loss: 0.8312 - val_RMSE: 0.7703\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8144 - RMSE: 0.7507 - val_loss: 0.8311 - val_RMSE: 0.7703\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8239 - RMSE: 0.7603 - val_loss: 0.8309 - val_RMSE: 0.7702\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8129 - RMSE: 0.7495 - val_loss: 0.8308 - val_RMSE: 0.7702\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8254 - RMSE: 0.7621 - val_loss: 0.8307 - val_RMSE: 0.7702\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8121 - RMSE: 0.7490 - val_loss: 0.8305 - val_RMSE: 0.7702\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8196 - RMSE: 0.7566 - val_loss: 0.8304 - val_RMSE: 0.7702\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8201 - RMSE: 0.7572 - val_loss: 0.8303 - val_RMSE: 0.7701\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8145 - RMSE: 0.7517 - val_loss: 0.8301 - val_RMSE: 0.7701\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8104 - RMSE: 0.7477 - val_loss: 0.8300 - val_RMSE: 0.7701\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8160 - RMSE: 0.7534 - val_loss: 0.8299 - val_RMSE: 0.7701\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8201 - RMSE: 0.7576 - val_loss: 0.8297 - val_RMSE: 0.7701\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8266 - RMSE: 0.7642 - val_loss: 0.8296 - val_RMSE: 0.7700\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8213 - RMSE: 0.7591 - val_loss: 0.8295 - val_RMSE: 0.7700\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8183 - RMSE: 0.7562 - val_loss: 0.8293 - val_RMSE: 0.7700\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8193 - RMSE: 0.7573 - val_loss: 0.8292 - val_RMSE: 0.7700\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8196 - RMSE: 0.7577 - val_loss: 0.8290 - val_RMSE: 0.7700\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8127 - RMSE: 0.7509 - val_loss: 0.8289 - val_RMSE: 0.7699\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8196 - RMSE: 0.7579 - val_loss: 0.8288 - val_RMSE: 0.7699\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8105 - RMSE: 0.7489 - val_loss: 0.8286 - val_RMSE: 0.7699\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8107 - RMSE: 0.7493 - val_loss: 0.8285 - val_RMSE: 0.7699\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8077 - RMSE: 0.7463 - val_loss: 0.8284 - val_RMSE: 0.7698\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8057 - RMSE: 0.7444 - val_loss: 0.8283 - val_RMSE: 0.7698\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8159 - RMSE: 0.7548 - val_loss: 0.8281 - val_RMSE: 0.7698\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8177 - RMSE: 0.7567 - val_loss: 0.8280 - val_RMSE: 0.7698\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8186 - RMSE: 0.7577 - val_loss: 0.8279 - val_RMSE: 0.7698\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8132 - RMSE: 0.7524 - val_loss: 0.8277 - val_RMSE: 0.7698\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8169 - RMSE: 0.7562 - val_loss: 0.8276 - val_RMSE: 0.7697\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8247 - RMSE: 0.7641 - val_loss: 0.8275 - val_RMSE: 0.7697\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8228 - RMSE: 0.7623 - val_loss: 0.8273 - val_RMSE: 0.7697\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8194 - RMSE: 0.7590 - val_loss: 0.8272 - val_RMSE: 0.7697\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8213 - RMSE: 0.7610 - val_loss: 0.8271 - val_RMSE: 0.7696\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8085 - RMSE: 0.7483 - val_loss: 0.8269 - val_RMSE: 0.7696\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8143 - RMSE: 0.7543 - val_loss: 0.8268 - val_RMSE: 0.7696\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8162 - RMSE: 0.7563 - val_loss: 0.8267 - val_RMSE: 0.7696\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8091 - RMSE: 0.7492 - val_loss: 0.8266 - val_RMSE: 0.7696\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8118 - RMSE: 0.7521 - val_loss: 0.8264 - val_RMSE: 0.7695\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8110 - RMSE: 0.7514 - val_loss: 0.8263 - val_RMSE: 0.7695\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8155 - RMSE: 0.7560 - val_loss: 0.8262 - val_RMSE: 0.7695\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8131 - RMSE: 0.7537 - val_loss: 0.8261 - val_RMSE: 0.7695\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8177 - RMSE: 0.7584 - val_loss: 0.8259 - val_RMSE: 0.7695\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8261 - RMSE: 0.7669 - val_loss: 0.8258 - val_RMSE: 0.7695\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8256 - RMSE: 0.7665 - val_loss: 0.8257 - val_RMSE: 0.7694\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8125 - RMSE: 0.7535 - val_loss: 0.8256 - val_RMSE: 0.7694\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8178 - RMSE: 0.7589 - val_loss: 0.8254 - val_RMSE: 0.7694\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8115 - RMSE: 0.7527 - val_loss: 0.8253 - val_RMSE: 0.7694\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8227 - RMSE: 0.7640 - val_loss: 0.8252 - val_RMSE: 0.7694\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8090 - RMSE: 0.7505 - val_loss: 0.8251 - val_RMSE: 0.7693\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8088 - RMSE: 0.7504 - val_loss: 0.8250 - val_RMSE: 0.7693\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8181 - RMSE: 0.7598 - val_loss: 0.8248 - val_RMSE: 0.7693\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8081 - RMSE: 0.7499 - val_loss: 0.8247 - val_RMSE: 0.7693\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8140 - RMSE: 0.7558 - val_loss: 0.8246 - val_RMSE: 0.7693\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8109 - RMSE: 0.7528 - val_loss: 0.8245 - val_RMSE: 0.7693\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8044 - RMSE: 0.7465 - val_loss: 0.8244 - val_RMSE: 0.7692\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8065 - RMSE: 0.7487 - val_loss: 0.8242 - val_RMSE: 0.7692\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8085 - RMSE: 0.7508 - val_loss: 0.8241 - val_RMSE: 0.7692\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8039 - RMSE: 0.7463 - val_loss: 0.8240 - val_RMSE: 0.7692\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8028 - RMSE: 0.7453 - val_loss: 0.8239 - val_RMSE: 0.7692\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8087 - RMSE: 0.7513 - val_loss: 0.8237 - val_RMSE: 0.7691\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8140 - RMSE: 0.7567 - val_loss: 0.8236 - val_RMSE: 0.7691\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8045 - RMSE: 0.7473 - val_loss: 0.8235 - val_RMSE: 0.7691\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8090 - RMSE: 0.7519 - val_loss: 0.8234 - val_RMSE: 0.7691\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8059 - RMSE: 0.7489 - val_loss: 0.8233 - val_RMSE: 0.7691\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8081 - RMSE: 0.7512 - val_loss: 0.8232 - val_RMSE: 0.7691\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8055 - RMSE: 0.7487 - val_loss: 0.8230 - val_RMSE: 0.7690\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8103 - RMSE: 0.7536 - val_loss: 0.8229 - val_RMSE: 0.7690\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8081 - RMSE: 0.7515 - val_loss: 0.8228 - val_RMSE: 0.7690\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8056 - RMSE: 0.7491 - val_loss: 0.8227 - val_RMSE: 0.7690\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8119 - RMSE: 0.7555 - val_loss: 0.8226 - val_RMSE: 0.7690\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8147 - RMSE: 0.7585 - val_loss: 0.8225 - val_RMSE: 0.7690\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8138 - RMSE: 0.7576 - val_loss: 0.8223 - val_RMSE: 0.7689\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8030 - RMSE: 0.7469 - val_loss: 0.8222 - val_RMSE: 0.7689\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8119 - RMSE: 0.7559 - val_loss: 0.8221 - val_RMSE: 0.7689\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8178 - RMSE: 0.7619 - val_loss: 0.8220 - val_RMSE: 0.7689\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8112 - RMSE: 0.7554 - val_loss: 0.8219 - val_RMSE: 0.7689\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7979 - RMSE: 0.7422 - val_loss: 0.8218 - val_RMSE: 0.7688\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8131 - RMSE: 0.7575 - val_loss: 0.8217 - val_RMSE: 0.7688\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8095 - RMSE: 0.7540 - val_loss: 0.8215 - val_RMSE: 0.7688\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8138 - RMSE: 0.7584 - val_loss: 0.8214 - val_RMSE: 0.7688\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8056 - RMSE: 0.7503 - val_loss: 0.8213 - val_RMSE: 0.7688\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8155 - RMSE: 0.7603 - val_loss: 0.8212 - val_RMSE: 0.7688\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8127 - RMSE: 0.7576 - val_loss: 0.8211 - val_RMSE: 0.7688\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8061 - RMSE: 0.7510 - val_loss: 0.8210 - val_RMSE: 0.7687\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8116 - RMSE: 0.7566 - val_loss: 0.8209 - val_RMSE: 0.7687\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8062 - RMSE: 0.7514 - val_loss: 0.8208 - val_RMSE: 0.7687\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8224 - RMSE: 0.7677 - val_loss: 0.8206 - val_RMSE: 0.7687\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8140 - RMSE: 0.7593 - val_loss: 0.8205 - val_RMSE: 0.7687\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8177 - RMSE: 0.7631 - val_loss: 0.8204 - val_RMSE: 0.7687\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8133 - RMSE: 0.7589 - val_loss: 0.8203 - val_RMSE: 0.7686\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8023 - RMSE: 0.7479 - val_loss: 0.8202 - val_RMSE: 0.7686\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8145 - RMSE: 0.7602 - val_loss: 0.8201 - val_RMSE: 0.7686\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8087 - RMSE: 0.7545 - val_loss: 0.8200 - val_RMSE: 0.7686\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8004 - RMSE: 0.7463 - val_loss: 0.8199 - val_RMSE: 0.7686\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8079 - RMSE: 0.7539 - val_loss: 0.8198 - val_RMSE: 0.7686\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8009 - RMSE: 0.7470 - val_loss: 0.8197 - val_RMSE: 0.7685\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8092 - RMSE: 0.7554 - val_loss: 0.8195 - val_RMSE: 0.7685\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8108 - RMSE: 0.7571 - val_loss: 0.8194 - val_RMSE: 0.7685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=10 / Init = GlorotNormal / min_loss = 0.768501877784729\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotNormal\n",
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_26 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_48 (Embedding)        (None, 1, 11)        319         input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_49 (Embedding)        (None, 1, 11)        1782        input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_12 (Dot)                    (None, 1, 1)         0           embedding_48[0][0]               \n",
            "                                                                 embedding_49[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_50 (Embedding)        (None, 1, 1)         29          input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_51 (Embedding)        (None, 1, 1)         162         input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 1, 1)         0           dot_12[0][0]                     \n",
            "                                                                 embedding_50[0][0]               \n",
            "                                                                 embedding_51[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 1)            0           add_12[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,292\n",
            "Trainable params: 2,292\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 5ms/step - loss: 1.0784 - RMSE: 0.7963 - val_loss: 0.8756 - val_RMSE: 0.7640\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8708 - RMSE: 0.7567 - val_loss: 0.8734 - val_RMSE: 0.7640\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8745 - RMSE: 0.7614 - val_loss: 0.8732 - val_RMSE: 0.7640\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8589 - RMSE: 0.7460 - val_loss: 0.8729 - val_RMSE: 0.7640\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8734 - RMSE: 0.7608 - val_loss: 0.8727 - val_RMSE: 0.7640\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8672 - RMSE: 0.7548 - val_loss: 0.8724 - val_RMSE: 0.7640\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8784 - RMSE: 0.7662 - val_loss: 0.8722 - val_RMSE: 0.7640\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8695 - RMSE: 0.7576 - val_loss: 0.8719 - val_RMSE: 0.7640\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8610 - RMSE: 0.7494 - val_loss: 0.8717 - val_RMSE: 0.7640\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8690 - RMSE: 0.7576 - val_loss: 0.8714 - val_RMSE: 0.7639\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8746 - RMSE: 0.7634 - val_loss: 0.8712 - val_RMSE: 0.7639\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8685 - RMSE: 0.7575 - val_loss: 0.8709 - val_RMSE: 0.7639\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8622 - RMSE: 0.7515 - val_loss: 0.8707 - val_RMSE: 0.7639\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8717 - RMSE: 0.7612 - val_loss: 0.8705 - val_RMSE: 0.7639\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8666 - RMSE: 0.7563 - val_loss: 0.8702 - val_RMSE: 0.7639\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8652 - RMSE: 0.7552 - val_loss: 0.8700 - val_RMSE: 0.7639\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8647 - RMSE: 0.7549 - val_loss: 0.8697 - val_RMSE: 0.7639\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8672 - RMSE: 0.7577 - val_loss: 0.8695 - val_RMSE: 0.7639\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8806 - RMSE: 0.7713 - val_loss: 0.8693 - val_RMSE: 0.7639\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8673 - RMSE: 0.7583 - val_loss: 0.8690 - val_RMSE: 0.7639\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8707 - RMSE: 0.7618 - val_loss: 0.8688 - val_RMSE: 0.7638\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8797 - RMSE: 0.7711 - val_loss: 0.8686 - val_RMSE: 0.7638\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8746 - RMSE: 0.7662 - val_loss: 0.8683 - val_RMSE: 0.7638\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8685 - RMSE: 0.7603 - val_loss: 0.8681 - val_RMSE: 0.7638\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8625 - RMSE: 0.7546 - val_loss: 0.8678 - val_RMSE: 0.7638\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8609 - RMSE: 0.7532 - val_loss: 0.8676 - val_RMSE: 0.7638\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8539 - RMSE: 0.7465 - val_loss: 0.8674 - val_RMSE: 0.7638\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8644 - RMSE: 0.7571 - val_loss: 0.8672 - val_RMSE: 0.7638\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8609 - RMSE: 0.7539 - val_loss: 0.8669 - val_RMSE: 0.7638\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8584 - RMSE: 0.7516 - val_loss: 0.8667 - val_RMSE: 0.7638\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8602 - RMSE: 0.7536 - val_loss: 0.8665 - val_RMSE: 0.7638\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8571 - RMSE: 0.7508 - val_loss: 0.8662 - val_RMSE: 0.7638\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8661 - RMSE: 0.7600 - val_loss: 0.8660 - val_RMSE: 0.7637\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8665 - RMSE: 0.7606 - val_loss: 0.8658 - val_RMSE: 0.7637\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8599 - RMSE: 0.7543 - val_loss: 0.8655 - val_RMSE: 0.7637\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8585 - RMSE: 0.7530 - val_loss: 0.8653 - val_RMSE: 0.7637\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8587 - RMSE: 0.7535 - val_loss: 0.8651 - val_RMSE: 0.7637\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8650 - RMSE: 0.7600 - val_loss: 0.8649 - val_RMSE: 0.7637\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8605 - RMSE: 0.7557 - val_loss: 0.8646 - val_RMSE: 0.7637\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8742 - RMSE: 0.7697 - val_loss: 0.8644 - val_RMSE: 0.7637\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8608 - RMSE: 0.7565 - val_loss: 0.8642 - val_RMSE: 0.7637\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8503 - RMSE: 0.7462 - val_loss: 0.8640 - val_RMSE: 0.7637\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8620 - RMSE: 0.7581 - val_loss: 0.8637 - val_RMSE: 0.7637\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8502 - RMSE: 0.7466 - val_loss: 0.8635 - val_RMSE: 0.7637\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8707 - RMSE: 0.7673 - val_loss: 0.8633 - val_RMSE: 0.7637\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8599 - RMSE: 0.7567 - val_loss: 0.8631 - val_RMSE: 0.7637\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8591 - RMSE: 0.7562 - val_loss: 0.8629 - val_RMSE: 0.7637\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8555 - RMSE: 0.7527 - val_loss: 0.8626 - val_RMSE: 0.7637\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8627 - RMSE: 0.7602 - val_loss: 0.8624 - val_RMSE: 0.7636\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8570 - RMSE: 0.7546 - val_loss: 0.8622 - val_RMSE: 0.7636\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8539 - RMSE: 0.7518 - val_loss: 0.8620 - val_RMSE: 0.7636\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8524 - RMSE: 0.7505 - val_loss: 0.8618 - val_RMSE: 0.7636\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8586 - RMSE: 0.7569 - val_loss: 0.8616 - val_RMSE: 0.7636\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8569 - RMSE: 0.7554 - val_loss: 0.8613 - val_RMSE: 0.7636\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8531 - RMSE: 0.7518 - val_loss: 0.8611 - val_RMSE: 0.7636\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8541 - RMSE: 0.7530 - val_loss: 0.8609 - val_RMSE: 0.7636\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8510 - RMSE: 0.7502 - val_loss: 0.8607 - val_RMSE: 0.7636\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8596 - RMSE: 0.7590 - val_loss: 0.8605 - val_RMSE: 0.7636\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8556 - RMSE: 0.7551 - val_loss: 0.8603 - val_RMSE: 0.7636\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8566 - RMSE: 0.7563 - val_loss: 0.8601 - val_RMSE: 0.7636\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8544 - RMSE: 0.7543 - val_loss: 0.8598 - val_RMSE: 0.7636\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8546 - RMSE: 0.7548 - val_loss: 0.8596 - val_RMSE: 0.7636\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8695 - RMSE: 0.7699 - val_loss: 0.8594 - val_RMSE: 0.7636\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8494 - RMSE: 0.7500 - val_loss: 0.8592 - val_RMSE: 0.7636\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8485 - RMSE: 0.7494 - val_loss: 0.8590 - val_RMSE: 0.7635\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8633 - RMSE: 0.7643 - val_loss: 0.8588 - val_RMSE: 0.7635\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8517 - RMSE: 0.7530 - val_loss: 0.8586 - val_RMSE: 0.7635\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8548 - RMSE: 0.7562 - val_loss: 0.8584 - val_RMSE: 0.7635\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8530 - RMSE: 0.7546 - val_loss: 0.8582 - val_RMSE: 0.7635\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8597 - RMSE: 0.7615 - val_loss: 0.8580 - val_RMSE: 0.7635\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8532 - RMSE: 0.7553 - val_loss: 0.8578 - val_RMSE: 0.7635\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8572 - RMSE: 0.7594 - val_loss: 0.8576 - val_RMSE: 0.7635\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8477 - RMSE: 0.7502 - val_loss: 0.8574 - val_RMSE: 0.7635\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8641 - RMSE: 0.7667 - val_loss: 0.8571 - val_RMSE: 0.7635\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8515 - RMSE: 0.7544 - val_loss: 0.8569 - val_RMSE: 0.7635\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8414 - RMSE: 0.7445 - val_loss: 0.8567 - val_RMSE: 0.7635\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8484 - RMSE: 0.7516 - val_loss: 0.8565 - val_RMSE: 0.7635\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8474 - RMSE: 0.7508 - val_loss: 0.8563 - val_RMSE: 0.7635\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8456 - RMSE: 0.7493 - val_loss: 0.8561 - val_RMSE: 0.7635\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8482 - RMSE: 0.7520 - val_loss: 0.8559 - val_RMSE: 0.7635\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8480 - RMSE: 0.7521 - val_loss: 0.8557 - val_RMSE: 0.7635\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8528 - RMSE: 0.7570 - val_loss: 0.8555 - val_RMSE: 0.7635\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8461 - RMSE: 0.7506 - val_loss: 0.8553 - val_RMSE: 0.7635\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8444 - RMSE: 0.7490 - val_loss: 0.8551 - val_RMSE: 0.7635\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8478 - RMSE: 0.7527 - val_loss: 0.8549 - val_RMSE: 0.7634\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8566 - RMSE: 0.7617 - val_loss: 0.8548 - val_RMSE: 0.7635\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8425 - RMSE: 0.7478 - val_loss: 0.8545 - val_RMSE: 0.7634\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8502 - RMSE: 0.7556 - val_loss: 0.8544 - val_RMSE: 0.7634\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8567 - RMSE: 0.7623 - val_loss: 0.8542 - val_RMSE: 0.7634\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8515 - RMSE: 0.7573 - val_loss: 0.8540 - val_RMSE: 0.7634\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8417 - RMSE: 0.7477 - val_loss: 0.8538 - val_RMSE: 0.7634\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8447 - RMSE: 0.7509 - val_loss: 0.8536 - val_RMSE: 0.7634\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8377 - RMSE: 0.7441 - val_loss: 0.8534 - val_RMSE: 0.7634\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8491 - RMSE: 0.7557 - val_loss: 0.8532 - val_RMSE: 0.7634\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8578 - RMSE: 0.7646 - val_loss: 0.8530 - val_RMSE: 0.7634\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8470 - RMSE: 0.7539 - val_loss: 0.8528 - val_RMSE: 0.7634\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8399 - RMSE: 0.7470 - val_loss: 0.8526 - val_RMSE: 0.7634\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8482 - RMSE: 0.7555 - val_loss: 0.8524 - val_RMSE: 0.7634\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8435 - RMSE: 0.7511 - val_loss: 0.8522 - val_RMSE: 0.7634\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8529 - RMSE: 0.7606 - val_loss: 0.8520 - val_RMSE: 0.7634\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8527 - RMSE: 0.7606 - val_loss: 0.8519 - val_RMSE: 0.7634\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8551 - RMSE: 0.7632 - val_loss: 0.8517 - val_RMSE: 0.7634\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8483 - RMSE: 0.7566 - val_loss: 0.8515 - val_RMSE: 0.7634\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8480 - RMSE: 0.7565 - val_loss: 0.8513 - val_RMSE: 0.7634\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8468 - RMSE: 0.7555 - val_loss: 0.8511 - val_RMSE: 0.7634\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8410 - RMSE: 0.7499 - val_loss: 0.8509 - val_RMSE: 0.7634\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8469 - RMSE: 0.7560 - val_loss: 0.8507 - val_RMSE: 0.7634\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8518 - RMSE: 0.7610 - val_loss: 0.8505 - val_RMSE: 0.7634\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8418 - RMSE: 0.7512 - val_loss: 0.8504 - val_RMSE: 0.7634\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8467 - RMSE: 0.7563 - val_loss: 0.8502 - val_RMSE: 0.7634\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8440 - RMSE: 0.7537 - val_loss: 0.8500 - val_RMSE: 0.7634\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8460 - RMSE: 0.7560 - val_loss: 0.8498 - val_RMSE: 0.7634\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8405 - RMSE: 0.7507 - val_loss: 0.8496 - val_RMSE: 0.7634\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8435 - RMSE: 0.7538 - val_loss: 0.8494 - val_RMSE: 0.7634\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8460 - RMSE: 0.7565 - val_loss: 0.8493 - val_RMSE: 0.7633\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8373 - RMSE: 0.7480 - val_loss: 0.8491 - val_RMSE: 0.7633\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8477 - RMSE: 0.7586 - val_loss: 0.8489 - val_RMSE: 0.7633\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8485 - RMSE: 0.7595 - val_loss: 0.8487 - val_RMSE: 0.7633\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8410 - RMSE: 0.7522 - val_loss: 0.8485 - val_RMSE: 0.7633\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8510 - RMSE: 0.7624 - val_loss: 0.8484 - val_RMSE: 0.7633\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8370 - RMSE: 0.7486 - val_loss: 0.8482 - val_RMSE: 0.7633\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8386 - RMSE: 0.7504 - val_loss: 0.8480 - val_RMSE: 0.7633\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8408 - RMSE: 0.7527 - val_loss: 0.8478 - val_RMSE: 0.7633\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8363 - RMSE: 0.7485 - val_loss: 0.8476 - val_RMSE: 0.7633\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8424 - RMSE: 0.7547 - val_loss: 0.8475 - val_RMSE: 0.7633\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8451 - RMSE: 0.7576 - val_loss: 0.8473 - val_RMSE: 0.7633\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8451 - RMSE: 0.7578 - val_loss: 0.8471 - val_RMSE: 0.7633\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8386 - RMSE: 0.7514 - val_loss: 0.8469 - val_RMSE: 0.7633\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8495 - RMSE: 0.7625 - val_loss: 0.8468 - val_RMSE: 0.7633\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8384 - RMSE: 0.7516 - val_loss: 0.8466 - val_RMSE: 0.7633\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8417 - RMSE: 0.7551 - val_loss: 0.8464 - val_RMSE: 0.7633\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8416 - RMSE: 0.7551 - val_loss: 0.8462 - val_RMSE: 0.7633\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8382 - RMSE: 0.7519 - val_loss: 0.8461 - val_RMSE: 0.7633\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8377 - RMSE: 0.7516 - val_loss: 0.8459 - val_RMSE: 0.7633\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8533 - RMSE: 0.7673 - val_loss: 0.8457 - val_RMSE: 0.7633\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8444 - RMSE: 0.7587 - val_loss: 0.8455 - val_RMSE: 0.7633\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8473 - RMSE: 0.7616 - val_loss: 0.8454 - val_RMSE: 0.7633\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8442 - RMSE: 0.7588 - val_loss: 0.8452 - val_RMSE: 0.7633\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8326 - RMSE: 0.7473 - val_loss: 0.8450 - val_RMSE: 0.7633\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8211 - RMSE: 0.7360 - val_loss: 0.8449 - val_RMSE: 0.7633\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8427 - RMSE: 0.7578 - val_loss: 0.8447 - val_RMSE: 0.7633\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8330 - RMSE: 0.7483 - val_loss: 0.8445 - val_RMSE: 0.7633\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8412 - RMSE: 0.7566 - val_loss: 0.8444 - val_RMSE: 0.7633\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8375 - RMSE: 0.7531 - val_loss: 0.8442 - val_RMSE: 0.7633\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8367 - RMSE: 0.7525 - val_loss: 0.8440 - val_RMSE: 0.7633\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8529 - RMSE: 0.7688 - val_loss: 0.8439 - val_RMSE: 0.7633\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8410 - RMSE: 0.7571 - val_loss: 0.8437 - val_RMSE: 0.7633\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8426 - RMSE: 0.7588 - val_loss: 0.8435 - val_RMSE: 0.7633\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8371 - RMSE: 0.7535 - val_loss: 0.8434 - val_RMSE: 0.7633\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8388 - RMSE: 0.7554 - val_loss: 0.8432 - val_RMSE: 0.7633\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8316 - RMSE: 0.7484 - val_loss: 0.8430 - val_RMSE: 0.7632\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8373 - RMSE: 0.7543 - val_loss: 0.8429 - val_RMSE: 0.7633\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8445 - RMSE: 0.7616 - val_loss: 0.8427 - val_RMSE: 0.7633\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8288 - RMSE: 0.7461 - val_loss: 0.8425 - val_RMSE: 0.7632\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8362 - RMSE: 0.7536 - val_loss: 0.8424 - val_RMSE: 0.7633\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8304 - RMSE: 0.7480 - val_loss: 0.8422 - val_RMSE: 0.7632\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8408 - RMSE: 0.7586 - val_loss: 0.8420 - val_RMSE: 0.7632\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8372 - RMSE: 0.7551 - val_loss: 0.8419 - val_RMSE: 0.7632\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8362 - RMSE: 0.7543 - val_loss: 0.8417 - val_RMSE: 0.7632\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8346 - RMSE: 0.7528 - val_loss: 0.8416 - val_RMSE: 0.7632\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8319 - RMSE: 0.7503 - val_loss: 0.8414 - val_RMSE: 0.7632\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8361 - RMSE: 0.7546 - val_loss: 0.8412 - val_RMSE: 0.7632\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8374 - RMSE: 0.7561 - val_loss: 0.8411 - val_RMSE: 0.7632\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8339 - RMSE: 0.7527 - val_loss: 0.8409 - val_RMSE: 0.7632\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8300 - RMSE: 0.7490 - val_loss: 0.8408 - val_RMSE: 0.7632\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8310 - RMSE: 0.7502 - val_loss: 0.8406 - val_RMSE: 0.7632\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8226 - RMSE: 0.7420 - val_loss: 0.8404 - val_RMSE: 0.7632\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8391 - RMSE: 0.7586 - val_loss: 0.8403 - val_RMSE: 0.7632\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8331 - RMSE: 0.7527 - val_loss: 0.8401 - val_RMSE: 0.7632\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8322 - RMSE: 0.7521 - val_loss: 0.8400 - val_RMSE: 0.7632\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8351 - RMSE: 0.7551 - val_loss: 0.8398 - val_RMSE: 0.7632\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8345 - RMSE: 0.7547 - val_loss: 0.8397 - val_RMSE: 0.7632\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8274 - RMSE: 0.7477 - val_loss: 0.8395 - val_RMSE: 0.7632\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8392 - RMSE: 0.7596 - val_loss: 0.8394 - val_RMSE: 0.7632\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8246 - RMSE: 0.7452 - val_loss: 0.8392 - val_RMSE: 0.7632\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8342 - RMSE: 0.7550 - val_loss: 0.8390 - val_RMSE: 0.7632\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8306 - RMSE: 0.7516 - val_loss: 0.8389 - val_RMSE: 0.7632\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8369 - RMSE: 0.7579 - val_loss: 0.8387 - val_RMSE: 0.7632\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8373 - RMSE: 0.7585 - val_loss: 0.8386 - val_RMSE: 0.7632\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8362 - RMSE: 0.7576 - val_loss: 0.8384 - val_RMSE: 0.7632\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8293 - RMSE: 0.7508 - val_loss: 0.8383 - val_RMSE: 0.7632\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8319 - RMSE: 0.7536 - val_loss: 0.8381 - val_RMSE: 0.7632\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8279 - RMSE: 0.7497 - val_loss: 0.8380 - val_RMSE: 0.7632\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8342 - RMSE: 0.7562 - val_loss: 0.8378 - val_RMSE: 0.7632\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8258 - RMSE: 0.7479 - val_loss: 0.8377 - val_RMSE: 0.7632\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8364 - RMSE: 0.7587 - val_loss: 0.8375 - val_RMSE: 0.7632\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8207 - RMSE: 0.7432 - val_loss: 0.8374 - val_RMSE: 0.7632\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8342 - RMSE: 0.7568 - val_loss: 0.8372 - val_RMSE: 0.7632\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8243 - RMSE: 0.7471 - val_loss: 0.8371 - val_RMSE: 0.7632\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8318 - RMSE: 0.7546 - val_loss: 0.8369 - val_RMSE: 0.7632\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8299 - RMSE: 0.7529 - val_loss: 0.8368 - val_RMSE: 0.7632\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8360 - RMSE: 0.7592 - val_loss: 0.8366 - val_RMSE: 0.7632\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8311 - RMSE: 0.7545 - val_loss: 0.8365 - val_RMSE: 0.7632\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8289 - RMSE: 0.7524 - val_loss: 0.8363 - val_RMSE: 0.7632\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8319 - RMSE: 0.7556 - val_loss: 0.8362 - val_RMSE: 0.7632\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8294 - RMSE: 0.7532 - val_loss: 0.8360 - val_RMSE: 0.7632\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8295 - RMSE: 0.7534 - val_loss: 0.8359 - val_RMSE: 0.7632\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8228 - RMSE: 0.7469 - val_loss: 0.8357 - val_RMSE: 0.7632\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8195 - RMSE: 0.7437 - val_loss: 0.8356 - val_RMSE: 0.7632\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8180 - RMSE: 0.7424 - val_loss: 0.8355 - val_RMSE: 0.7632\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8312 - RMSE: 0.7557 - val_loss: 0.8353 - val_RMSE: 0.7632\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8352 - RMSE: 0.7599 - val_loss: 0.8352 - val_RMSE: 0.7632\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8384 - RMSE: 0.7633 - val_loss: 0.8350 - val_RMSE: 0.7632\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8325 - RMSE: 0.7574 - val_loss: 0.8349 - val_RMSE: 0.7632\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8201 - RMSE: 0.7452 - val_loss: 0.8347 - val_RMSE: 0.7632\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8371 - RMSE: 0.7624 - val_loss: 0.8346 - val_RMSE: 0.7632\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8269 - RMSE: 0.7523 - val_loss: 0.8344 - val_RMSE: 0.7632\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8279 - RMSE: 0.7535 - val_loss: 0.8343 - val_RMSE: 0.7632\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8332 - RMSE: 0.7589 - val_loss: 0.8342 - val_RMSE: 0.7632\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8305 - RMSE: 0.7563 - val_loss: 0.8340 - val_RMSE: 0.7632\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8128 - RMSE: 0.7388 - val_loss: 0.8339 - val_RMSE: 0.7632\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8237 - RMSE: 0.7498 - val_loss: 0.8337 - val_RMSE: 0.7632\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8321 - RMSE: 0.7584 - val_loss: 0.8336 - val_RMSE: 0.7632\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8262 - RMSE: 0.7526 - val_loss: 0.8335 - val_RMSE: 0.7632\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8300 - RMSE: 0.7565 - val_loss: 0.8333 - val_RMSE: 0.7632\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8246 - RMSE: 0.7512 - val_loss: 0.8332 - val_RMSE: 0.7632\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8261 - RMSE: 0.7529 - val_loss: 0.8330 - val_RMSE: 0.7632\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8247 - RMSE: 0.7516 - val_loss: 0.8329 - val_RMSE: 0.7632\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8312 - RMSE: 0.7583 - val_loss: 0.8328 - val_RMSE: 0.7632\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8308 - RMSE: 0.7581 - val_loss: 0.8326 - val_RMSE: 0.7632\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8243 - RMSE: 0.7516 - val_loss: 0.8325 - val_RMSE: 0.7632\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8174 - RMSE: 0.7449 - val_loss: 0.8323 - val_RMSE: 0.7632\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8169 - RMSE: 0.7445 - val_loss: 0.8322 - val_RMSE: 0.7632\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8216 - RMSE: 0.7494 - val_loss: 0.8321 - val_RMSE: 0.7632\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8250 - RMSE: 0.7529 - val_loss: 0.8319 - val_RMSE: 0.7632\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8234 - RMSE: 0.7515 - val_loss: 0.8318 - val_RMSE: 0.7632\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8300 - RMSE: 0.7582 - val_loss: 0.8317 - val_RMSE: 0.7632\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8163 - RMSE: 0.7446 - val_loss: 0.8315 - val_RMSE: 0.7632\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8295 - RMSE: 0.7580 - val_loss: 0.8314 - val_RMSE: 0.7632\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8228 - RMSE: 0.7514 - val_loss: 0.8313 - val_RMSE: 0.7632\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8252 - RMSE: 0.7539 - val_loss: 0.8311 - val_RMSE: 0.7631\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8246 - RMSE: 0.7535 - val_loss: 0.8310 - val_RMSE: 0.7631\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8247 - RMSE: 0.7537 - val_loss: 0.8309 - val_RMSE: 0.7631\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8327 - RMSE: 0.7618 - val_loss: 0.8307 - val_RMSE: 0.7631\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8207 - RMSE: 0.7499 - val_loss: 0.8306 - val_RMSE: 0.7631\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8187 - RMSE: 0.7481 - val_loss: 0.8305 - val_RMSE: 0.7631\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8215 - RMSE: 0.7510 - val_loss: 0.8303 - val_RMSE: 0.7631\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8255 - RMSE: 0.7552 - val_loss: 0.8302 - val_RMSE: 0.7631\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8101 - RMSE: 0.7399 - val_loss: 0.8301 - val_RMSE: 0.7631\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8113 - RMSE: 0.7412 - val_loss: 0.8299 - val_RMSE: 0.7631\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8220 - RMSE: 0.7521 - val_loss: 0.8298 - val_RMSE: 0.7631\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8276 - RMSE: 0.7579 - val_loss: 0.8297 - val_RMSE: 0.7631\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8236 - RMSE: 0.7539 - val_loss: 0.8295 - val_RMSE: 0.7631\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8157 - RMSE: 0.7462 - val_loss: 0.8294 - val_RMSE: 0.7631\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8159 - RMSE: 0.7465 - val_loss: 0.8293 - val_RMSE: 0.7631\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8200 - RMSE: 0.7507 - val_loss: 0.8292 - val_RMSE: 0.7631\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8182 - RMSE: 0.7490 - val_loss: 0.8290 - val_RMSE: 0.7631\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8150 - RMSE: 0.7459 - val_loss: 0.8289 - val_RMSE: 0.7631\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8235 - RMSE: 0.7546 - val_loss: 0.8288 - val_RMSE: 0.7631\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8092 - RMSE: 0.7404 - val_loss: 0.8286 - val_RMSE: 0.7631\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8316 - RMSE: 0.7630 - val_loss: 0.8285 - val_RMSE: 0.7631\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8200 - RMSE: 0.7515 - val_loss: 0.8284 - val_RMSE: 0.7631\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8260 - RMSE: 0.7576 - val_loss: 0.8283 - val_RMSE: 0.7631\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8182 - RMSE: 0.7500 - val_loss: 0.8281 - val_RMSE: 0.7631\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8193 - RMSE: 0.7512 - val_loss: 0.8280 - val_RMSE: 0.7631\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8218 - RMSE: 0.7538 - val_loss: 0.8279 - val_RMSE: 0.7631\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8156 - RMSE: 0.7478 - val_loss: 0.8278 - val_RMSE: 0.7631\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8264 - RMSE: 0.7587 - val_loss: 0.8276 - val_RMSE: 0.7631\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8265 - RMSE: 0.7588 - val_loss: 0.8275 - val_RMSE: 0.7631\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8238 - RMSE: 0.7564 - val_loss: 0.8274 - val_RMSE: 0.7631\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8262 - RMSE: 0.7588 - val_loss: 0.8273 - val_RMSE: 0.7631\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8163 - RMSE: 0.7491 - val_loss: 0.8271 - val_RMSE: 0.7631\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8244 - RMSE: 0.7573 - val_loss: 0.8270 - val_RMSE: 0.7631\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8292 - RMSE: 0.7622 - val_loss: 0.8269 - val_RMSE: 0.7631\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8221 - RMSE: 0.7553 - val_loss: 0.8268 - val_RMSE: 0.7631\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8195 - RMSE: 0.7528 - val_loss: 0.8266 - val_RMSE: 0.7631\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8283 - RMSE: 0.7617 - val_loss: 0.8265 - val_RMSE: 0.7631\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8206 - RMSE: 0.7541 - val_loss: 0.8264 - val_RMSE: 0.7631\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8150 - RMSE: 0.7486 - val_loss: 0.8263 - val_RMSE: 0.7631\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8235 - RMSE: 0.7572 - val_loss: 0.8262 - val_RMSE: 0.7631\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8099 - RMSE: 0.7437 - val_loss: 0.8260 - val_RMSE: 0.7631\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8255 - RMSE: 0.7595 - val_loss: 0.8259 - val_RMSE: 0.7631\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8167 - RMSE: 0.7508 - val_loss: 0.8258 - val_RMSE: 0.7631\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8154 - RMSE: 0.7496 - val_loss: 0.8257 - val_RMSE: 0.7631\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8222 - RMSE: 0.7565 - val_loss: 0.8256 - val_RMSE: 0.7631\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8094 - RMSE: 0.7439 - val_loss: 0.8254 - val_RMSE: 0.7631\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8117 - RMSE: 0.7463 - val_loss: 0.8253 - val_RMSE: 0.7631\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8182 - RMSE: 0.7529 - val_loss: 0.8252 - val_RMSE: 0.7631\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8159 - RMSE: 0.7507 - val_loss: 0.8251 - val_RMSE: 0.7631\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8135 - RMSE: 0.7485 - val_loss: 0.8250 - val_RMSE: 0.7631\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8232 - RMSE: 0.7583 - val_loss: 0.8248 - val_RMSE: 0.7631\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8215 - RMSE: 0.7567 - val_loss: 0.8247 - val_RMSE: 0.7631\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8159 - RMSE: 0.7512 - val_loss: 0.8246 - val_RMSE: 0.7631\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8245 - RMSE: 0.7599 - val_loss: 0.8245 - val_RMSE: 0.7631\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8191 - RMSE: 0.7546 - val_loss: 0.8244 - val_RMSE: 0.7631\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8186 - RMSE: 0.7543 - val_loss: 0.8243 - val_RMSE: 0.7631\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8211 - RMSE: 0.7569 - val_loss: 0.8241 - val_RMSE: 0.7631\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8244 - RMSE: 0.7603 - val_loss: 0.8240 - val_RMSE: 0.7631\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8144 - RMSE: 0.7504 - val_loss: 0.8239 - val_RMSE: 0.7631\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8119 - RMSE: 0.7480 - val_loss: 0.8238 - val_RMSE: 0.7631\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8155 - RMSE: 0.7518 - val_loss: 0.8237 - val_RMSE: 0.7631\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8102 - RMSE: 0.7465 - val_loss: 0.8236 - val_RMSE: 0.7631\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8192 - RMSE: 0.7557 - val_loss: 0.8234 - val_RMSE: 0.7631\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8121 - RMSE: 0.7487 - val_loss: 0.8233 - val_RMSE: 0.7631\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8210 - RMSE: 0.7577 - val_loss: 0.8232 - val_RMSE: 0.7631\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8154 - RMSE: 0.7522 - val_loss: 0.8231 - val_RMSE: 0.7631\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8152 - RMSE: 0.7522 - val_loss: 0.8230 - val_RMSE: 0.7631\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8221 - RMSE: 0.7592 - val_loss: 0.8229 - val_RMSE: 0.7631\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8096 - RMSE: 0.7468 - val_loss: 0.8228 - val_RMSE: 0.7631\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8123 - RMSE: 0.7495 - val_loss: 0.8227 - val_RMSE: 0.7631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=11 / Init = GlorotNormal / min_loss = 0.763107180595398\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotNormal\n",
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_27 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_28 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_52 (Embedding)        (None, 1, 12)        348         input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_53 (Embedding)        (None, 1, 12)        1944        input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_13 (Dot)                    (None, 1, 1)         0           embedding_52[0][0]               \n",
            "                                                                 embedding_53[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_54 (Embedding)        (None, 1, 1)         29          input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_55 (Embedding)        (None, 1, 1)         162         input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 1, 1)         0           dot_13[0][0]                     \n",
            "                                                                 embedding_54[0][0]               \n",
            "                                                                 embedding_55[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_13 (Flatten)            (None, 1)            0           add_13[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,483\n",
            "Trainable params: 2,483\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.0834 - RMSE: 0.7822 - val_loss: 0.8946 - val_RMSE: 0.7714\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8832 - RMSE: 0.7585 - val_loss: 0.8923 - val_RMSE: 0.7715\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8840 - RMSE: 0.7604 - val_loss: 0.8920 - val_RMSE: 0.7714\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8836 - RMSE: 0.7603 - val_loss: 0.8917 - val_RMSE: 0.7714\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8801 - RMSE: 0.7570 - val_loss: 0.8914 - val_RMSE: 0.7713\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8755 - RMSE: 0.7527 - val_loss: 0.8911 - val_RMSE: 0.7713\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8759 - RMSE: 0.7533 - val_loss: 0.8908 - val_RMSE: 0.7712\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8729 - RMSE: 0.7506 - val_loss: 0.8905 - val_RMSE: 0.7712\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8763 - RMSE: 0.7543 - val_loss: 0.8902 - val_RMSE: 0.7712\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8757 - RMSE: 0.7540 - val_loss: 0.8899 - val_RMSE: 0.7711\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8790 - RMSE: 0.7575 - val_loss: 0.8896 - val_RMSE: 0.7711\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8780 - RMSE: 0.7568 - val_loss: 0.8893 - val_RMSE: 0.7711\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8706 - RMSE: 0.7497 - val_loss: 0.8890 - val_RMSE: 0.7710\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8710 - RMSE: 0.7503 - val_loss: 0.8887 - val_RMSE: 0.7710\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8708 - RMSE: 0.7504 - val_loss: 0.8884 - val_RMSE: 0.7710\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8679 - RMSE: 0.7478 - val_loss: 0.8881 - val_RMSE: 0.7709\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8799 - RMSE: 0.7599 - val_loss: 0.8878 - val_RMSE: 0.7709\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8721 - RMSE: 0.7524 - val_loss: 0.8876 - val_RMSE: 0.7708\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8734 - RMSE: 0.7540 - val_loss: 0.8873 - val_RMSE: 0.7708\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8738 - RMSE: 0.7546 - val_loss: 0.8870 - val_RMSE: 0.7708\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8858 - RMSE: 0.7669 - val_loss: 0.8867 - val_RMSE: 0.7707\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8745 - RMSE: 0.7558 - val_loss: 0.8864 - val_RMSE: 0.7707\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8775 - RMSE: 0.7591 - val_loss: 0.8861 - val_RMSE: 0.7707\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8718 - RMSE: 0.7537 - val_loss: 0.8858 - val_RMSE: 0.7706\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8764 - RMSE: 0.7585 - val_loss: 0.8855 - val_RMSE: 0.7706\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8677 - RMSE: 0.7501 - val_loss: 0.8853 - val_RMSE: 0.7706\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8781 - RMSE: 0.7607 - val_loss: 0.8850 - val_RMSE: 0.7705\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8731 - RMSE: 0.7560 - val_loss: 0.8847 - val_RMSE: 0.7705\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8762 - RMSE: 0.7593 - val_loss: 0.8844 - val_RMSE: 0.7705\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8652 - RMSE: 0.7486 - val_loss: 0.8841 - val_RMSE: 0.7704\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8662 - RMSE: 0.7498 - val_loss: 0.8839 - val_RMSE: 0.7704\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8786 - RMSE: 0.7624 - val_loss: 0.8836 - val_RMSE: 0.7703\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8738 - RMSE: 0.7579 - val_loss: 0.8833 - val_RMSE: 0.7703\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8800 - RMSE: 0.7644 - val_loss: 0.8830 - val_RMSE: 0.7703\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8664 - RMSE: 0.7510 - val_loss: 0.8827 - val_RMSE: 0.7702\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8618 - RMSE: 0.7466 - val_loss: 0.8825 - val_RMSE: 0.7702\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8664 - RMSE: 0.7515 - val_loss: 0.8822 - val_RMSE: 0.7702\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8654 - RMSE: 0.7508 - val_loss: 0.8819 - val_RMSE: 0.7701\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8789 - RMSE: 0.7645 - val_loss: 0.8816 - val_RMSE: 0.7701\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8584 - RMSE: 0.7442 - val_loss: 0.8814 - val_RMSE: 0.7701\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8640 - RMSE: 0.7501 - val_loss: 0.8811 - val_RMSE: 0.7700\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8694 - RMSE: 0.7557 - val_loss: 0.8808 - val_RMSE: 0.7700\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8674 - RMSE: 0.7540 - val_loss: 0.8805 - val_RMSE: 0.7700\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8737 - RMSE: 0.7604 - val_loss: 0.8803 - val_RMSE: 0.7699\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8693 - RMSE: 0.7563 - val_loss: 0.8800 - val_RMSE: 0.7699\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8740 - RMSE: 0.7612 - val_loss: 0.8797 - val_RMSE: 0.7699\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8737 - RMSE: 0.7611 - val_loss: 0.8795 - val_RMSE: 0.7699\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8677 - RMSE: 0.7554 - val_loss: 0.8792 - val_RMSE: 0.7698\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8685 - RMSE: 0.7565 - val_loss: 0.8789 - val_RMSE: 0.7698\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8573 - RMSE: 0.7455 - val_loss: 0.8787 - val_RMSE: 0.7698\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8667 - RMSE: 0.7552 - val_loss: 0.8784 - val_RMSE: 0.7697\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8658 - RMSE: 0.7544 - val_loss: 0.8781 - val_RMSE: 0.7697\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8535 - RMSE: 0.7424 - val_loss: 0.8779 - val_RMSE: 0.7697\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8581 - RMSE: 0.7472 - val_loss: 0.8776 - val_RMSE: 0.7696\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8605 - RMSE: 0.7499 - val_loss: 0.8774 - val_RMSE: 0.7696\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8694 - RMSE: 0.7590 - val_loss: 0.8771 - val_RMSE: 0.7696\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8544 - RMSE: 0.7442 - val_loss: 0.8768 - val_RMSE: 0.7695\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8639 - RMSE: 0.7539 - val_loss: 0.8766 - val_RMSE: 0.7695\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8715 - RMSE: 0.7618 - val_loss: 0.8763 - val_RMSE: 0.7695\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8570 - RMSE: 0.7476 - val_loss: 0.8761 - val_RMSE: 0.7695\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8553 - RMSE: 0.7460 - val_loss: 0.8758 - val_RMSE: 0.7694\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8607 - RMSE: 0.7517 - val_loss: 0.8756 - val_RMSE: 0.7694\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8589 - RMSE: 0.7501 - val_loss: 0.8753 - val_RMSE: 0.7694\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8563 - RMSE: 0.7477 - val_loss: 0.8750 - val_RMSE: 0.7693\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8492 - RMSE: 0.7408 - val_loss: 0.8748 - val_RMSE: 0.7693\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8577 - RMSE: 0.7496 - val_loss: 0.8745 - val_RMSE: 0.7693\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8547 - RMSE: 0.7468 - val_loss: 0.8743 - val_RMSE: 0.7692\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8594 - RMSE: 0.7517 - val_loss: 0.8740 - val_RMSE: 0.7692\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8603 - RMSE: 0.7529 - val_loss: 0.8738 - val_RMSE: 0.7692\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8638 - RMSE: 0.7566 - val_loss: 0.8735 - val_RMSE: 0.7692\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8612 - RMSE: 0.7542 - val_loss: 0.8733 - val_RMSE: 0.7691\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8596 - RMSE: 0.7528 - val_loss: 0.8730 - val_RMSE: 0.7691\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8683 - RMSE: 0.7617 - val_loss: 0.8728 - val_RMSE: 0.7691\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8614 - RMSE: 0.7551 - val_loss: 0.8725 - val_RMSE: 0.7690\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8700 - RMSE: 0.7639 - val_loss: 0.8723 - val_RMSE: 0.7690\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8618 - RMSE: 0.7559 - val_loss: 0.8720 - val_RMSE: 0.7690\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8410 - RMSE: 0.7353 - val_loss: 0.8718 - val_RMSE: 0.7690\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8688 - RMSE: 0.7634 - val_loss: 0.8715 - val_RMSE: 0.7689\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8689 - RMSE: 0.7637 - val_loss: 0.8713 - val_RMSE: 0.7689\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8557 - RMSE: 0.7507 - val_loss: 0.8711 - val_RMSE: 0.7689\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8564 - RMSE: 0.7516 - val_loss: 0.8708 - val_RMSE: 0.7688\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8580 - RMSE: 0.7535 - val_loss: 0.8706 - val_RMSE: 0.7688\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8558 - RMSE: 0.7514 - val_loss: 0.8703 - val_RMSE: 0.7688\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8541 - RMSE: 0.7500 - val_loss: 0.8701 - val_RMSE: 0.7688\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8575 - RMSE: 0.7535 - val_loss: 0.8699 - val_RMSE: 0.7687\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8604 - RMSE: 0.7567 - val_loss: 0.8696 - val_RMSE: 0.7687\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8589 - RMSE: 0.7554 - val_loss: 0.8694 - val_RMSE: 0.7687\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8524 - RMSE: 0.7491 - val_loss: 0.8691 - val_RMSE: 0.7687\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8694 - RMSE: 0.7663 - val_loss: 0.8689 - val_RMSE: 0.7686\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8545 - RMSE: 0.7517 - val_loss: 0.8687 - val_RMSE: 0.7686\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8559 - RMSE: 0.7532 - val_loss: 0.8684 - val_RMSE: 0.7686\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8526 - RMSE: 0.7501 - val_loss: 0.8682 - val_RMSE: 0.7686\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8570 - RMSE: 0.7548 - val_loss: 0.8679 - val_RMSE: 0.7685\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8591 - RMSE: 0.7571 - val_loss: 0.8677 - val_RMSE: 0.7685\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8518 - RMSE: 0.7500 - val_loss: 0.8675 - val_RMSE: 0.7685\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8550 - RMSE: 0.7534 - val_loss: 0.8672 - val_RMSE: 0.7685\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8435 - RMSE: 0.7421 - val_loss: 0.8670 - val_RMSE: 0.7684\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8470 - RMSE: 0.7458 - val_loss: 0.8668 - val_RMSE: 0.7684\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8490 - RMSE: 0.7480 - val_loss: 0.8666 - val_RMSE: 0.7684\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8470 - RMSE: 0.7462 - val_loss: 0.8663 - val_RMSE: 0.7684\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8567 - RMSE: 0.7561 - val_loss: 0.8661 - val_RMSE: 0.7683\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8506 - RMSE: 0.7503 - val_loss: 0.8659 - val_RMSE: 0.7683\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8534 - RMSE: 0.7533 - val_loss: 0.8656 - val_RMSE: 0.7683\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8550 - RMSE: 0.7551 - val_loss: 0.8654 - val_RMSE: 0.7683\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8528 - RMSE: 0.7531 - val_loss: 0.8652 - val_RMSE: 0.7682\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8523 - RMSE: 0.7528 - val_loss: 0.8650 - val_RMSE: 0.7682\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8559 - RMSE: 0.7565 - val_loss: 0.8647 - val_RMSE: 0.7682\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8435 - RMSE: 0.7444 - val_loss: 0.8645 - val_RMSE: 0.7682\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8577 - RMSE: 0.7588 - val_loss: 0.8643 - val_RMSE: 0.7681\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8507 - RMSE: 0.7520 - val_loss: 0.8641 - val_RMSE: 0.7681\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8494 - RMSE: 0.7509 - val_loss: 0.8638 - val_RMSE: 0.7681\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8458 - RMSE: 0.7475 - val_loss: 0.8636 - val_RMSE: 0.7681\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8471 - RMSE: 0.7490 - val_loss: 0.8634 - val_RMSE: 0.7680\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8559 - RMSE: 0.7580 - val_loss: 0.8632 - val_RMSE: 0.7680\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8515 - RMSE: 0.7538 - val_loss: 0.8629 - val_RMSE: 0.7680\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8500 - RMSE: 0.7525 - val_loss: 0.8627 - val_RMSE: 0.7680\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8469 - RMSE: 0.7496 - val_loss: 0.8625 - val_RMSE: 0.7680\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8505 - RMSE: 0.7534 - val_loss: 0.8623 - val_RMSE: 0.7679\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8422 - RMSE: 0.7453 - val_loss: 0.8621 - val_RMSE: 0.7679\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8481 - RMSE: 0.7514 - val_loss: 0.8619 - val_RMSE: 0.7679\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8530 - RMSE: 0.7565 - val_loss: 0.8616 - val_RMSE: 0.7679\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8561 - RMSE: 0.7598 - val_loss: 0.8614 - val_RMSE: 0.7678\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8458 - RMSE: 0.7497 - val_loss: 0.8612 - val_RMSE: 0.7678\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8440 - RMSE: 0.7481 - val_loss: 0.8610 - val_RMSE: 0.7678\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8471 - RMSE: 0.7514 - val_loss: 0.8608 - val_RMSE: 0.7678\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8474 - RMSE: 0.7519 - val_loss: 0.8606 - val_RMSE: 0.7678\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8461 - RMSE: 0.7507 - val_loss: 0.8603 - val_RMSE: 0.7677\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8572 - RMSE: 0.7620 - val_loss: 0.8601 - val_RMSE: 0.7677\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8453 - RMSE: 0.7503 - val_loss: 0.8599 - val_RMSE: 0.7677\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8494 - RMSE: 0.7546 - val_loss: 0.8597 - val_RMSE: 0.7677\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8532 - RMSE: 0.7586 - val_loss: 0.8595 - val_RMSE: 0.7677\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8497 - RMSE: 0.7553 - val_loss: 0.8593 - val_RMSE: 0.7676\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8445 - RMSE: 0.7503 - val_loss: 0.8591 - val_RMSE: 0.7676\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8372 - RMSE: 0.7432 - val_loss: 0.8589 - val_RMSE: 0.7676\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8294 - RMSE: 0.7356 - val_loss: 0.8587 - val_RMSE: 0.7676\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8515 - RMSE: 0.7579 - val_loss: 0.8585 - val_RMSE: 0.7676\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8436 - RMSE: 0.7501 - val_loss: 0.8582 - val_RMSE: 0.7675\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8468 - RMSE: 0.7536 - val_loss: 0.8580 - val_RMSE: 0.7675\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8523 - RMSE: 0.7592 - val_loss: 0.8578 - val_RMSE: 0.7675\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8484 - RMSE: 0.7555 - val_loss: 0.8576 - val_RMSE: 0.7675\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8389 - RMSE: 0.7462 - val_loss: 0.8574 - val_RMSE: 0.7675\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8537 - RMSE: 0.7612 - val_loss: 0.8572 - val_RMSE: 0.7674\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8399 - RMSE: 0.7476 - val_loss: 0.8570 - val_RMSE: 0.7674\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8487 - RMSE: 0.7566 - val_loss: 0.8568 - val_RMSE: 0.7674\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8393 - RMSE: 0.7474 - val_loss: 0.8566 - val_RMSE: 0.7674\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8556 - RMSE: 0.7638 - val_loss: 0.8564 - val_RMSE: 0.7674\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8509 - RMSE: 0.7593 - val_loss: 0.8562 - val_RMSE: 0.7673\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8433 - RMSE: 0.7519 - val_loss: 0.8560 - val_RMSE: 0.7673\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8472 - RMSE: 0.7560 - val_loss: 0.8558 - val_RMSE: 0.7673\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8379 - RMSE: 0.7469 - val_loss: 0.8556 - val_RMSE: 0.7673\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8467 - RMSE: 0.7558 - val_loss: 0.8554 - val_RMSE: 0.7673\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8370 - RMSE: 0.7464 - val_loss: 0.8552 - val_RMSE: 0.7672\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8380 - RMSE: 0.7476 - val_loss: 0.8550 - val_RMSE: 0.7672\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8384 - RMSE: 0.7481 - val_loss: 0.8548 - val_RMSE: 0.7672\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8430 - RMSE: 0.7529 - val_loss: 0.8546 - val_RMSE: 0.7672\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8362 - RMSE: 0.7463 - val_loss: 0.8544 - val_RMSE: 0.7672\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8421 - RMSE: 0.7523 - val_loss: 0.8542 - val_RMSE: 0.7671\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8372 - RMSE: 0.7476 - val_loss: 0.8540 - val_RMSE: 0.7671\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8449 - RMSE: 0.7555 - val_loss: 0.8538 - val_RMSE: 0.7671\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8354 - RMSE: 0.7462 - val_loss: 0.8536 - val_RMSE: 0.7671\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8453 - RMSE: 0.7562 - val_loss: 0.8534 - val_RMSE: 0.7671\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8358 - RMSE: 0.7469 - val_loss: 0.8532 - val_RMSE: 0.7671\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8435 - RMSE: 0.7548 - val_loss: 0.8530 - val_RMSE: 0.7670\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8435 - RMSE: 0.7550 - val_loss: 0.8529 - val_RMSE: 0.7670\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8431 - RMSE: 0.7548 - val_loss: 0.8527 - val_RMSE: 0.7670\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8428 - RMSE: 0.7546 - val_loss: 0.8525 - val_RMSE: 0.7670\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8514 - RMSE: 0.7634 - val_loss: 0.8523 - val_RMSE: 0.7670\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8456 - RMSE: 0.7577 - val_loss: 0.8521 - val_RMSE: 0.7670\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8418 - RMSE: 0.7541 - val_loss: 0.8519 - val_RMSE: 0.7669\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8419 - RMSE: 0.7544 - val_loss: 0.8517 - val_RMSE: 0.7669\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8351 - RMSE: 0.7478 - val_loss: 0.8515 - val_RMSE: 0.7669\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8307 - RMSE: 0.7436 - val_loss: 0.8513 - val_RMSE: 0.7669\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8409 - RMSE: 0.7540 - val_loss: 0.8511 - val_RMSE: 0.7669\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8371 - RMSE: 0.7503 - val_loss: 0.8510 - val_RMSE: 0.7669\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8394 - RMSE: 0.7528 - val_loss: 0.8508 - val_RMSE: 0.7668\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8376 - RMSE: 0.7512 - val_loss: 0.8506 - val_RMSE: 0.7668\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8401 - RMSE: 0.7538 - val_loss: 0.8504 - val_RMSE: 0.7668\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8359 - RMSE: 0.7498 - val_loss: 0.8502 - val_RMSE: 0.7668\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8363 - RMSE: 0.7504 - val_loss: 0.8500 - val_RMSE: 0.7668\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8330 - RMSE: 0.7472 - val_loss: 0.8498 - val_RMSE: 0.7668\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8328 - RMSE: 0.7472 - val_loss: 0.8497 - val_RMSE: 0.7667\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8356 - RMSE: 0.7502 - val_loss: 0.8495 - val_RMSE: 0.7667\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8360 - RMSE: 0.7508 - val_loss: 0.8493 - val_RMSE: 0.7667\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8379 - RMSE: 0.7528 - val_loss: 0.8491 - val_RMSE: 0.7667\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8341 - RMSE: 0.7491 - val_loss: 0.8489 - val_RMSE: 0.7667\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8433 - RMSE: 0.7585 - val_loss: 0.8487 - val_RMSE: 0.7667\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8379 - RMSE: 0.7533 - val_loss: 0.8486 - val_RMSE: 0.7666\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8252 - RMSE: 0.7407 - val_loss: 0.8484 - val_RMSE: 0.7666\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8261 - RMSE: 0.7418 - val_loss: 0.8482 - val_RMSE: 0.7666\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8327 - RMSE: 0.7486 - val_loss: 0.8480 - val_RMSE: 0.7666\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8404 - RMSE: 0.7565 - val_loss: 0.8478 - val_RMSE: 0.7666\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8310 - RMSE: 0.7473 - val_loss: 0.8477 - val_RMSE: 0.7666\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8441 - RMSE: 0.7605 - val_loss: 0.8475 - val_RMSE: 0.7665\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8357 - RMSE: 0.7523 - val_loss: 0.8473 - val_RMSE: 0.7665\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8356 - RMSE: 0.7523 - val_loss: 0.8471 - val_RMSE: 0.7665\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8332 - RMSE: 0.7501 - val_loss: 0.8469 - val_RMSE: 0.7665\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8341 - RMSE: 0.7511 - val_loss: 0.8468 - val_RMSE: 0.7665\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8364 - RMSE: 0.7536 - val_loss: 0.8466 - val_RMSE: 0.7665\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8358 - RMSE: 0.7531 - val_loss: 0.8464 - val_RMSE: 0.7664\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8346 - RMSE: 0.7521 - val_loss: 0.8462 - val_RMSE: 0.7664\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8337 - RMSE: 0.7514 - val_loss: 0.8461 - val_RMSE: 0.7664\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8291 - RMSE: 0.7469 - val_loss: 0.8459 - val_RMSE: 0.7664\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8249 - RMSE: 0.7430 - val_loss: 0.8457 - val_RMSE: 0.7664\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8222 - RMSE: 0.7404 - val_loss: 0.8456 - val_RMSE: 0.7664\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8334 - RMSE: 0.7518 - val_loss: 0.8454 - val_RMSE: 0.7664\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8223 - RMSE: 0.7407 - val_loss: 0.8452 - val_RMSE: 0.7663\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8353 - RMSE: 0.7540 - val_loss: 0.8450 - val_RMSE: 0.7663\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8266 - RMSE: 0.7454 - val_loss: 0.8449 - val_RMSE: 0.7663\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8273 - RMSE: 0.7463 - val_loss: 0.8447 - val_RMSE: 0.7663\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8271 - RMSE: 0.7462 - val_loss: 0.8445 - val_RMSE: 0.7663\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8324 - RMSE: 0.7517 - val_loss: 0.8444 - val_RMSE: 0.7663\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8243 - RMSE: 0.7438 - val_loss: 0.8442 - val_RMSE: 0.7663\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8260 - RMSE: 0.7456 - val_loss: 0.8440 - val_RMSE: 0.7662\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8348 - RMSE: 0.7545 - val_loss: 0.8438 - val_RMSE: 0.7662\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8351 - RMSE: 0.7550 - val_loss: 0.8437 - val_RMSE: 0.7662\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8383 - RMSE: 0.7583 - val_loss: 0.8435 - val_RMSE: 0.7662\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8336 - RMSE: 0.7538 - val_loss: 0.8433 - val_RMSE: 0.7662\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8372 - RMSE: 0.7576 - val_loss: 0.8432 - val_RMSE: 0.7662\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8235 - RMSE: 0.7440 - val_loss: 0.8430 - val_RMSE: 0.7662\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8302 - RMSE: 0.7508 - val_loss: 0.8428 - val_RMSE: 0.7661\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8307 - RMSE: 0.7515 - val_loss: 0.8427 - val_RMSE: 0.7661\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8328 - RMSE: 0.7537 - val_loss: 0.8425 - val_RMSE: 0.7661\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8368 - RMSE: 0.7579 - val_loss: 0.8423 - val_RMSE: 0.7661\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8288 - RMSE: 0.7500 - val_loss: 0.8422 - val_RMSE: 0.7661\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8271 - RMSE: 0.7485 - val_loss: 0.8420 - val_RMSE: 0.7661\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8301 - RMSE: 0.7516 - val_loss: 0.8419 - val_RMSE: 0.7661\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8295 - RMSE: 0.7512 - val_loss: 0.8417 - val_RMSE: 0.7661\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8242 - RMSE: 0.7461 - val_loss: 0.8415 - val_RMSE: 0.7660\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8338 - RMSE: 0.7558 - val_loss: 0.8414 - val_RMSE: 0.7660\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8236 - RMSE: 0.7457 - val_loss: 0.8412 - val_RMSE: 0.7660\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8307 - RMSE: 0.7531 - val_loss: 0.8410 - val_RMSE: 0.7660\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8226 - RMSE: 0.7451 - val_loss: 0.8409 - val_RMSE: 0.7660\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8319 - RMSE: 0.7545 - val_loss: 0.8407 - val_RMSE: 0.7660\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8241 - RMSE: 0.7468 - val_loss: 0.8406 - val_RMSE: 0.7660\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8223 - RMSE: 0.7452 - val_loss: 0.8404 - val_RMSE: 0.7659\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8275 - RMSE: 0.7505 - val_loss: 0.8402 - val_RMSE: 0.7659\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8209 - RMSE: 0.7441 - val_loss: 0.8401 - val_RMSE: 0.7659\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8329 - RMSE: 0.7562 - val_loss: 0.8399 - val_RMSE: 0.7659\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8394 - RMSE: 0.7628 - val_loss: 0.8398 - val_RMSE: 0.7659\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8170 - RMSE: 0.7407 - val_loss: 0.8396 - val_RMSE: 0.7659\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8274 - RMSE: 0.7512 - val_loss: 0.8394 - val_RMSE: 0.7659\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8213 - RMSE: 0.7453 - val_loss: 0.8393 - val_RMSE: 0.7658\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8209 - RMSE: 0.7450 - val_loss: 0.8391 - val_RMSE: 0.7658\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8112 - RMSE: 0.7354 - val_loss: 0.8390 - val_RMSE: 0.7658\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8276 - RMSE: 0.7520 - val_loss: 0.8388 - val_RMSE: 0.7658\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8255 - RMSE: 0.7500 - val_loss: 0.8387 - val_RMSE: 0.7658\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8233 - RMSE: 0.7480 - val_loss: 0.8385 - val_RMSE: 0.7658\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8311 - RMSE: 0.7559 - val_loss: 0.8384 - val_RMSE: 0.7658\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8238 - RMSE: 0.7487 - val_loss: 0.8382 - val_RMSE: 0.7658\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8226 - RMSE: 0.7477 - val_loss: 0.8380 - val_RMSE: 0.7657\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8268 - RMSE: 0.7521 - val_loss: 0.8379 - val_RMSE: 0.7657\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8248 - RMSE: 0.7502 - val_loss: 0.8377 - val_RMSE: 0.7657\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8240 - RMSE: 0.7495 - val_loss: 0.8376 - val_RMSE: 0.7657\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8303 - RMSE: 0.7559 - val_loss: 0.8374 - val_RMSE: 0.7657\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8274 - RMSE: 0.7532 - val_loss: 0.8373 - val_RMSE: 0.7657\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8222 - RMSE: 0.7481 - val_loss: 0.8371 - val_RMSE: 0.7657\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8218 - RMSE: 0.7479 - val_loss: 0.8370 - val_RMSE: 0.7657\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8306 - RMSE: 0.7567 - val_loss: 0.8368 - val_RMSE: 0.7656\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8184 - RMSE: 0.7448 - val_loss: 0.8367 - val_RMSE: 0.7656\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8268 - RMSE: 0.7533 - val_loss: 0.8365 - val_RMSE: 0.7656\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8246 - RMSE: 0.7512 - val_loss: 0.8364 - val_RMSE: 0.7656\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8281 - RMSE: 0.7549 - val_loss: 0.8362 - val_RMSE: 0.7656\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8205 - RMSE: 0.7474 - val_loss: 0.8361 - val_RMSE: 0.7656\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8221 - RMSE: 0.7491 - val_loss: 0.8359 - val_RMSE: 0.7656\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8235 - RMSE: 0.7507 - val_loss: 0.8358 - val_RMSE: 0.7656\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8304 - RMSE: 0.7577 - val_loss: 0.8356 - val_RMSE: 0.7656\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8241 - RMSE: 0.7515 - val_loss: 0.8355 - val_RMSE: 0.7655\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8319 - RMSE: 0.7595 - val_loss: 0.8354 - val_RMSE: 0.7655\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8326 - RMSE: 0.7603 - val_loss: 0.8352 - val_RMSE: 0.7655\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8156 - RMSE: 0.7434 - val_loss: 0.8351 - val_RMSE: 0.7655\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8205 - RMSE: 0.7485 - val_loss: 0.8349 - val_RMSE: 0.7655\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8325 - RMSE: 0.7606 - val_loss: 0.8348 - val_RMSE: 0.7655\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8201 - RMSE: 0.7483 - val_loss: 0.8346 - val_RMSE: 0.7655\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8010 - RMSE: 0.7294 - val_loss: 0.8345 - val_RMSE: 0.7655\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8249 - RMSE: 0.7534 - val_loss: 0.8343 - val_RMSE: 0.7654\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8272 - RMSE: 0.7558 - val_loss: 0.8342 - val_RMSE: 0.7654\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8241 - RMSE: 0.7529 - val_loss: 0.8341 - val_RMSE: 0.7654\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8310 - RMSE: 0.7600 - val_loss: 0.8339 - val_RMSE: 0.7654\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8114 - RMSE: 0.7405 - val_loss: 0.8338 - val_RMSE: 0.7654\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8276 - RMSE: 0.7568 - val_loss: 0.8336 - val_RMSE: 0.7654\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8108 - RMSE: 0.7402 - val_loss: 0.8335 - val_RMSE: 0.7654\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8169 - RMSE: 0.7463 - val_loss: 0.8333 - val_RMSE: 0.7654\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8147 - RMSE: 0.7443 - val_loss: 0.8332 - val_RMSE: 0.7654\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8149 - RMSE: 0.7446 - val_loss: 0.8331 - val_RMSE: 0.7654\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8232 - RMSE: 0.7531 - val_loss: 0.8329 - val_RMSE: 0.7653\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8113 - RMSE: 0.7413 - val_loss: 0.8328 - val_RMSE: 0.7653\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8145 - RMSE: 0.7446 - val_loss: 0.8326 - val_RMSE: 0.7653\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8188 - RMSE: 0.7491 - val_loss: 0.8325 - val_RMSE: 0.7653\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8192 - RMSE: 0.7496 - val_loss: 0.8324 - val_RMSE: 0.7653\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8111 - RMSE: 0.7416 - val_loss: 0.8322 - val_RMSE: 0.7653\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8139 - RMSE: 0.7446 - val_loss: 0.8321 - val_RMSE: 0.7653\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8141 - RMSE: 0.7449 - val_loss: 0.8319 - val_RMSE: 0.7653\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8151 - RMSE: 0.7460 - val_loss: 0.8318 - val_RMSE: 0.7653\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8122 - RMSE: 0.7432 - val_loss: 0.8317 - val_RMSE: 0.7652\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8111 - RMSE: 0.7422 - val_loss: 0.8315 - val_RMSE: 0.7652\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8107 - RMSE: 0.7420 - val_loss: 0.8314 - val_RMSE: 0.7652\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8237 - RMSE: 0.7551 - val_loss: 0.8313 - val_RMSE: 0.7652\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8145 - RMSE: 0.7460 - val_loss: 0.8311 - val_RMSE: 0.7652\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8220 - RMSE: 0.7536 - val_loss: 0.8310 - val_RMSE: 0.7652\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8105 - RMSE: 0.7423 - val_loss: 0.8309 - val_RMSE: 0.7652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=12 / Init = GlorotNormal / min_loss = 0.7651875615119934\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotNormal\n",
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_29 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_30 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_56 (Embedding)        (None, 1, 13)        377         input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_57 (Embedding)        (None, 1, 13)        2106        input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_14 (Dot)                    (None, 1, 1)         0           embedding_56[0][0]               \n",
            "                                                                 embedding_57[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_58 (Embedding)        (None, 1, 1)         29          input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_59 (Embedding)        (None, 1, 1)         162         input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 1, 1)         0           dot_14[0][0]                     \n",
            "                                                                 embedding_58[0][0]               \n",
            "                                                                 embedding_59[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_14 (Flatten)            (None, 1)            0           add_14[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,674\n",
            "Trainable params: 2,674\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.0994 - RMSE: 0.7889 - val_loss: 0.8867 - val_RMSE: 0.7630\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8952 - RMSE: 0.7704 - val_loss: 0.8840 - val_RMSE: 0.7627\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8781 - RMSE: 0.7545 - val_loss: 0.8837 - val_RMSE: 0.7627\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8724 - RMSE: 0.7490 - val_loss: 0.8834 - val_RMSE: 0.7627\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8711 - RMSE: 0.7480 - val_loss: 0.8832 - val_RMSE: 0.7627\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8797 - RMSE: 0.7569 - val_loss: 0.8829 - val_RMSE: 0.7627\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8782 - RMSE: 0.7556 - val_loss: 0.8826 - val_RMSE: 0.7626\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8870 - RMSE: 0.7646 - val_loss: 0.8824 - val_RMSE: 0.7627\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8783 - RMSE: 0.7563 - val_loss: 0.8821 - val_RMSE: 0.7626\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8789 - RMSE: 0.7571 - val_loss: 0.8818 - val_RMSE: 0.7626\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8755 - RMSE: 0.7540 - val_loss: 0.8816 - val_RMSE: 0.7626\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8686 - RMSE: 0.7474 - val_loss: 0.8813 - val_RMSE: 0.7626\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8737 - RMSE: 0.7527 - val_loss: 0.8810 - val_RMSE: 0.7626\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8755 - RMSE: 0.7548 - val_loss: 0.8808 - val_RMSE: 0.7626\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8738 - RMSE: 0.7533 - val_loss: 0.8805 - val_RMSE: 0.7626\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8802 - RMSE: 0.7600 - val_loss: 0.8803 - val_RMSE: 0.7626\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8821 - RMSE: 0.7622 - val_loss: 0.8800 - val_RMSE: 0.7626\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8831 - RMSE: 0.7634 - val_loss: 0.8797 - val_RMSE: 0.7626\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8785 - RMSE: 0.7591 - val_loss: 0.8795 - val_RMSE: 0.7626\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8775 - RMSE: 0.7583 - val_loss: 0.8792 - val_RMSE: 0.7626\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8752 - RMSE: 0.7562 - val_loss: 0.8790 - val_RMSE: 0.7626\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8725 - RMSE: 0.7538 - val_loss: 0.8787 - val_RMSE: 0.7626\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8765 - RMSE: 0.7580 - val_loss: 0.8784 - val_RMSE: 0.7626\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8811 - RMSE: 0.7629 - val_loss: 0.8782 - val_RMSE: 0.7626\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8692 - RMSE: 0.7513 - val_loss: 0.8779 - val_RMSE: 0.7626\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8790 - RMSE: 0.7613 - val_loss: 0.8777 - val_RMSE: 0.7626\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8823 - RMSE: 0.7649 - val_loss: 0.8774 - val_RMSE: 0.7626\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8783 - RMSE: 0.7611 - val_loss: 0.8772 - val_RMSE: 0.7626\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8773 - RMSE: 0.7604 - val_loss: 0.8769 - val_RMSE: 0.7626\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8800 - RMSE: 0.7633 - val_loss: 0.8767 - val_RMSE: 0.7626\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8678 - RMSE: 0.7513 - val_loss: 0.8764 - val_RMSE: 0.7626\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8651 - RMSE: 0.7490 - val_loss: 0.8762 - val_RMSE: 0.7626\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8654 - RMSE: 0.7495 - val_loss: 0.8759 - val_RMSE: 0.7626\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8797 - RMSE: 0.7640 - val_loss: 0.8757 - val_RMSE: 0.7626\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8712 - RMSE: 0.7557 - val_loss: 0.8754 - val_RMSE: 0.7626\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8750 - RMSE: 0.7599 - val_loss: 0.8752 - val_RMSE: 0.7626\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8733 - RMSE: 0.7584 - val_loss: 0.8749 - val_RMSE: 0.7626\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8783 - RMSE: 0.7636 - val_loss: 0.8747 - val_RMSE: 0.7626\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8854 - RMSE: 0.7709 - val_loss: 0.8744 - val_RMSE: 0.7626\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8633 - RMSE: 0.7491 - val_loss: 0.8742 - val_RMSE: 0.7626\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8729 - RMSE: 0.7589 - val_loss: 0.8739 - val_RMSE: 0.7626\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8722 - RMSE: 0.7585 - val_loss: 0.8737 - val_RMSE: 0.7626\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8748 - RMSE: 0.7613 - val_loss: 0.8735 - val_RMSE: 0.7626\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8776 - RMSE: 0.7644 - val_loss: 0.8732 - val_RMSE: 0.7626\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8622 - RMSE: 0.7492 - val_loss: 0.8730 - val_RMSE: 0.7626\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8689 - RMSE: 0.7561 - val_loss: 0.8727 - val_RMSE: 0.7626\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8826 - RMSE: 0.7701 - val_loss: 0.8725 - val_RMSE: 0.7626\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8674 - RMSE: 0.7551 - val_loss: 0.8723 - val_RMSE: 0.7626\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8676 - RMSE: 0.7555 - val_loss: 0.8720 - val_RMSE: 0.7626\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8599 - RMSE: 0.7481 - val_loss: 0.8718 - val_RMSE: 0.7626\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8734 - RMSE: 0.7618 - val_loss: 0.8715 - val_RMSE: 0.7626\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8636 - RMSE: 0.7523 - val_loss: 0.8713 - val_RMSE: 0.7626\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8701 - RMSE: 0.7590 - val_loss: 0.8711 - val_RMSE: 0.7626\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8637 - RMSE: 0.7529 - val_loss: 0.8708 - val_RMSE: 0.7626\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8671 - RMSE: 0.7565 - val_loss: 0.8706 - val_RMSE: 0.7626\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8651 - RMSE: 0.7547 - val_loss: 0.8704 - val_RMSE: 0.7626\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8658 - RMSE: 0.7556 - val_loss: 0.8701 - val_RMSE: 0.7626\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8686 - RMSE: 0.7587 - val_loss: 0.8699 - val_RMSE: 0.7626\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8715 - RMSE: 0.7619 - val_loss: 0.8697 - val_RMSE: 0.7626\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8693 - RMSE: 0.7599 - val_loss: 0.8694 - val_RMSE: 0.7626\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8624 - RMSE: 0.7532 - val_loss: 0.8692 - val_RMSE: 0.7626\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8674 - RMSE: 0.7584 - val_loss: 0.8690 - val_RMSE: 0.7626\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8736 - RMSE: 0.7649 - val_loss: 0.8687 - val_RMSE: 0.7626\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8620 - RMSE: 0.7535 - val_loss: 0.8685 - val_RMSE: 0.7626\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8634 - RMSE: 0.7551 - val_loss: 0.8683 - val_RMSE: 0.7627\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8632 - RMSE: 0.7551 - val_loss: 0.8680 - val_RMSE: 0.7626\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8607 - RMSE: 0.7529 - val_loss: 0.8678 - val_RMSE: 0.7626\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8627 - RMSE: 0.7551 - val_loss: 0.8676 - val_RMSE: 0.7626\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8699 - RMSE: 0.7625 - val_loss: 0.8674 - val_RMSE: 0.7626\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8684 - RMSE: 0.7613 - val_loss: 0.8671 - val_RMSE: 0.7626\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8606 - RMSE: 0.7536 - val_loss: 0.8669 - val_RMSE: 0.7627\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8562 - RMSE: 0.7495 - val_loss: 0.8667 - val_RMSE: 0.7627\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8630 - RMSE: 0.7565 - val_loss: 0.8665 - val_RMSE: 0.7627\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8566 - RMSE: 0.7503 - val_loss: 0.8662 - val_RMSE: 0.7626\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8603 - RMSE: 0.7543 - val_loss: 0.8660 - val_RMSE: 0.7627\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8595 - RMSE: 0.7537 - val_loss: 0.8658 - val_RMSE: 0.7627\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8657 - RMSE: 0.7601 - val_loss: 0.8656 - val_RMSE: 0.7627\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8535 - RMSE: 0.7481 - val_loss: 0.8653 - val_RMSE: 0.7627\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8460 - RMSE: 0.7409 - val_loss: 0.8651 - val_RMSE: 0.7627\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8632 - RMSE: 0.7583 - val_loss: 0.8649 - val_RMSE: 0.7627\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8583 - RMSE: 0.7536 - val_loss: 0.8647 - val_RMSE: 0.7627\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8601 - RMSE: 0.7556 - val_loss: 0.8645 - val_RMSE: 0.7627\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8698 - RMSE: 0.7656 - val_loss: 0.8642 - val_RMSE: 0.7627\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8531 - RMSE: 0.7491 - val_loss: 0.8640 - val_RMSE: 0.7627\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8557 - RMSE: 0.7518 - val_loss: 0.8638 - val_RMSE: 0.7627\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8564 - RMSE: 0.7527 - val_loss: 0.8636 - val_RMSE: 0.7627\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8656 - RMSE: 0.7623 - val_loss: 0.8634 - val_RMSE: 0.7627\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8621 - RMSE: 0.7590 - val_loss: 0.8632 - val_RMSE: 0.7627\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8565 - RMSE: 0.7536 - val_loss: 0.8630 - val_RMSE: 0.7627\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8567 - RMSE: 0.7539 - val_loss: 0.8627 - val_RMSE: 0.7627\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8620 - RMSE: 0.7594 - val_loss: 0.8625 - val_RMSE: 0.7627\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8671 - RMSE: 0.7648 - val_loss: 0.8623 - val_RMSE: 0.7627\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8645 - RMSE: 0.7624 - val_loss: 0.8621 - val_RMSE: 0.7627\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8578 - RMSE: 0.7558 - val_loss: 0.8619 - val_RMSE: 0.7627\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8679 - RMSE: 0.7662 - val_loss: 0.8617 - val_RMSE: 0.7627\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8529 - RMSE: 0.7514 - val_loss: 0.8615 - val_RMSE: 0.7627\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8547 - RMSE: 0.7534 - val_loss: 0.8613 - val_RMSE: 0.7627\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8589 - RMSE: 0.7578 - val_loss: 0.8610 - val_RMSE: 0.7627\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8560 - RMSE: 0.7551 - val_loss: 0.8608 - val_RMSE: 0.7627\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8522 - RMSE: 0.7515 - val_loss: 0.8606 - val_RMSE: 0.7627\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8636 - RMSE: 0.7632 - val_loss: 0.8604 - val_RMSE: 0.7627\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8523 - RMSE: 0.7520 - val_loss: 0.8602 - val_RMSE: 0.7627\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8527 - RMSE: 0.7526 - val_loss: 0.8600 - val_RMSE: 0.7627\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8541 - RMSE: 0.7543 - val_loss: 0.8598 - val_RMSE: 0.7627\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8577 - RMSE: 0.7580 - val_loss: 0.8596 - val_RMSE: 0.7627\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8543 - RMSE: 0.7548 - val_loss: 0.8594 - val_RMSE: 0.7627\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8608 - RMSE: 0.7616 - val_loss: 0.8592 - val_RMSE: 0.7627\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8528 - RMSE: 0.7538 - val_loss: 0.8590 - val_RMSE: 0.7627\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8573 - RMSE: 0.7585 - val_loss: 0.8588 - val_RMSE: 0.7627\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8495 - RMSE: 0.7509 - val_loss: 0.8586 - val_RMSE: 0.7627\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8486 - RMSE: 0.7502 - val_loss: 0.8584 - val_RMSE: 0.7627\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8507 - RMSE: 0.7525 - val_loss: 0.8582 - val_RMSE: 0.7627\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8537 - RMSE: 0.7557 - val_loss: 0.8580 - val_RMSE: 0.7627\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8560 - RMSE: 0.7582 - val_loss: 0.8578 - val_RMSE: 0.7627\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8508 - RMSE: 0.7532 - val_loss: 0.8576 - val_RMSE: 0.7627\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8590 - RMSE: 0.7616 - val_loss: 0.8574 - val_RMSE: 0.7627\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8437 - RMSE: 0.7465 - val_loss: 0.8572 - val_RMSE: 0.7627\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8472 - RMSE: 0.7502 - val_loss: 0.8570 - val_RMSE: 0.7627\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8577 - RMSE: 0.7609 - val_loss: 0.8568 - val_RMSE: 0.7627\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8512 - RMSE: 0.7546 - val_loss: 0.8566 - val_RMSE: 0.7627\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8442 - RMSE: 0.7478 - val_loss: 0.8564 - val_RMSE: 0.7627\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8416 - RMSE: 0.7454 - val_loss: 0.8562 - val_RMSE: 0.7627\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8531 - RMSE: 0.7571 - val_loss: 0.8560 - val_RMSE: 0.7627\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8510 - RMSE: 0.7552 - val_loss: 0.8558 - val_RMSE: 0.7627\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8521 - RMSE: 0.7564 - val_loss: 0.8556 - val_RMSE: 0.7627\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8559 - RMSE: 0.7605 - val_loss: 0.8554 - val_RMSE: 0.7627\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8552 - RMSE: 0.7600 - val_loss: 0.8552 - val_RMSE: 0.7627\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8532 - RMSE: 0.7582 - val_loss: 0.8550 - val_RMSE: 0.7627\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8365 - RMSE: 0.7417 - val_loss: 0.8548 - val_RMSE: 0.7627\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8450 - RMSE: 0.7503 - val_loss: 0.8546 - val_RMSE: 0.7628\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8458 - RMSE: 0.7514 - val_loss: 0.8544 - val_RMSE: 0.7627\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8531 - RMSE: 0.7588 - val_loss: 0.8542 - val_RMSE: 0.7627\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8448 - RMSE: 0.7508 - val_loss: 0.8540 - val_RMSE: 0.7627\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8482 - RMSE: 0.7543 - val_loss: 0.8539 - val_RMSE: 0.7627\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8511 - RMSE: 0.7574 - val_loss: 0.8537 - val_RMSE: 0.7628\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8519 - RMSE: 0.7584 - val_loss: 0.8535 - val_RMSE: 0.7628\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8499 - RMSE: 0.7567 - val_loss: 0.8533 - val_RMSE: 0.7628\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8452 - RMSE: 0.7521 - val_loss: 0.8531 - val_RMSE: 0.7628\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8470 - RMSE: 0.7541 - val_loss: 0.8529 - val_RMSE: 0.7628\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8485 - RMSE: 0.7558 - val_loss: 0.8527 - val_RMSE: 0.7628\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8387 - RMSE: 0.7461 - val_loss: 0.8525 - val_RMSE: 0.7628\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8351 - RMSE: 0.7427 - val_loss: 0.8523 - val_RMSE: 0.7628\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8411 - RMSE: 0.7489 - val_loss: 0.8522 - val_RMSE: 0.7628\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8557 - RMSE: 0.7637 - val_loss: 0.8520 - val_RMSE: 0.7628\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8558 - RMSE: 0.7640 - val_loss: 0.8518 - val_RMSE: 0.7628\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8396 - RMSE: 0.7480 - val_loss: 0.8516 - val_RMSE: 0.7628\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8469 - RMSE: 0.7555 - val_loss: 0.8514 - val_RMSE: 0.7628\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8436 - RMSE: 0.7524 - val_loss: 0.8512 - val_RMSE: 0.7628\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8442 - RMSE: 0.7532 - val_loss: 0.8511 - val_RMSE: 0.7628\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8471 - RMSE: 0.7563 - val_loss: 0.8509 - val_RMSE: 0.7628\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8446 - RMSE: 0.7539 - val_loss: 0.8507 - val_RMSE: 0.7628\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8324 - RMSE: 0.7419 - val_loss: 0.8505 - val_RMSE: 0.7628\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8501 - RMSE: 0.7598 - val_loss: 0.8503 - val_RMSE: 0.7628\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8418 - RMSE: 0.7517 - val_loss: 0.8501 - val_RMSE: 0.7628\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8408 - RMSE: 0.7509 - val_loss: 0.8500 - val_RMSE: 0.7628\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8323 - RMSE: 0.7425 - val_loss: 0.8498 - val_RMSE: 0.7628\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8470 - RMSE: 0.7574 - val_loss: 0.8496 - val_RMSE: 0.7628\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8381 - RMSE: 0.7487 - val_loss: 0.8494 - val_RMSE: 0.7628\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8408 - RMSE: 0.7516 - val_loss: 0.8493 - val_RMSE: 0.7628\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8473 - RMSE: 0.7583 - val_loss: 0.8491 - val_RMSE: 0.7628\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8383 - RMSE: 0.7494 - val_loss: 0.8489 - val_RMSE: 0.7628\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8456 - RMSE: 0.7569 - val_loss: 0.8487 - val_RMSE: 0.7628\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8496 - RMSE: 0.7611 - val_loss: 0.8485 - val_RMSE: 0.7628\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8344 - RMSE: 0.7461 - val_loss: 0.8484 - val_RMSE: 0.7628\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8374 - RMSE: 0.7492 - val_loss: 0.8482 - val_RMSE: 0.7628\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8417 - RMSE: 0.7537 - val_loss: 0.8480 - val_RMSE: 0.7628\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8323 - RMSE: 0.7445 - val_loss: 0.8478 - val_RMSE: 0.7628\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8309 - RMSE: 0.7433 - val_loss: 0.8477 - val_RMSE: 0.7628\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8423 - RMSE: 0.7548 - val_loss: 0.8475 - val_RMSE: 0.7628\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8379 - RMSE: 0.7506 - val_loss: 0.8473 - val_RMSE: 0.7628\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8479 - RMSE: 0.7608 - val_loss: 0.8471 - val_RMSE: 0.7628\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8432 - RMSE: 0.7563 - val_loss: 0.8470 - val_RMSE: 0.7628\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8390 - RMSE: 0.7523 - val_loss: 0.8468 - val_RMSE: 0.7628\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8292 - RMSE: 0.7426 - val_loss: 0.8466 - val_RMSE: 0.7628\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8412 - RMSE: 0.7548 - val_loss: 0.8465 - val_RMSE: 0.7628\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8483 - RMSE: 0.7620 - val_loss: 0.8463 - val_RMSE: 0.7628\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8374 - RMSE: 0.7514 - val_loss: 0.8461 - val_RMSE: 0.7628\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8358 - RMSE: 0.7499 - val_loss: 0.8459 - val_RMSE: 0.7628\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8279 - RMSE: 0.7422 - val_loss: 0.8458 - val_RMSE: 0.7628\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8517 - RMSE: 0.7661 - val_loss: 0.8456 - val_RMSE: 0.7628\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8271 - RMSE: 0.7417 - val_loss: 0.8454 - val_RMSE: 0.7628\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8372 - RMSE: 0.7520 - val_loss: 0.8453 - val_RMSE: 0.7628\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8391 - RMSE: 0.7540 - val_loss: 0.8451 - val_RMSE: 0.7628\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8350 - RMSE: 0.7501 - val_loss: 0.8449 - val_RMSE: 0.7628\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8348 - RMSE: 0.7501 - val_loss: 0.8448 - val_RMSE: 0.7628\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8312 - RMSE: 0.7467 - val_loss: 0.8446 - val_RMSE: 0.7628\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8424 - RMSE: 0.7581 - val_loss: 0.8444 - val_RMSE: 0.7628\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8453 - RMSE: 0.7611 - val_loss: 0.8443 - val_RMSE: 0.7628\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8439 - RMSE: 0.7599 - val_loss: 0.8441 - val_RMSE: 0.7628\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8426 - RMSE: 0.7587 - val_loss: 0.8439 - val_RMSE: 0.7628\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8439 - RMSE: 0.7602 - val_loss: 0.8438 - val_RMSE: 0.7628\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8308 - RMSE: 0.7473 - val_loss: 0.8436 - val_RMSE: 0.7628\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8426 - RMSE: 0.7592 - val_loss: 0.8434 - val_RMSE: 0.7629\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8356 - RMSE: 0.7524 - val_loss: 0.8433 - val_RMSE: 0.7629\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8231 - RMSE: 0.7401 - val_loss: 0.8431 - val_RMSE: 0.7629\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8381 - RMSE: 0.7552 - val_loss: 0.8430 - val_RMSE: 0.7629\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8329 - RMSE: 0.7502 - val_loss: 0.8428 - val_RMSE: 0.7629\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8390 - RMSE: 0.7564 - val_loss: 0.8426 - val_RMSE: 0.7629\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8385 - RMSE: 0.7561 - val_loss: 0.8425 - val_RMSE: 0.7629\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8374 - RMSE: 0.7551 - val_loss: 0.8423 - val_RMSE: 0.7629\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8333 - RMSE: 0.7512 - val_loss: 0.8421 - val_RMSE: 0.7629\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8264 - RMSE: 0.7445 - val_loss: 0.8420 - val_RMSE: 0.7629\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8297 - RMSE: 0.7479 - val_loss: 0.8418 - val_RMSE: 0.7629\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8276 - RMSE: 0.7460 - val_loss: 0.8417 - val_RMSE: 0.7629\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8323 - RMSE: 0.7509 - val_loss: 0.8415 - val_RMSE: 0.7629\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8412 - RMSE: 0.7599 - val_loss: 0.8413 - val_RMSE: 0.7629\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8328 - RMSE: 0.7517 - val_loss: 0.8412 - val_RMSE: 0.7629\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8349 - RMSE: 0.7540 - val_loss: 0.8410 - val_RMSE: 0.7629\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8353 - RMSE: 0.7545 - val_loss: 0.8409 - val_RMSE: 0.7629\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8338 - RMSE: 0.7532 - val_loss: 0.8407 - val_RMSE: 0.7629\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8328 - RMSE: 0.7523 - val_loss: 0.8406 - val_RMSE: 0.7629\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8390 - RMSE: 0.7587 - val_loss: 0.8404 - val_RMSE: 0.7629\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8289 - RMSE: 0.7488 - val_loss: 0.8403 - val_RMSE: 0.7629\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8305 - RMSE: 0.7505 - val_loss: 0.8401 - val_RMSE: 0.7629\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8331 - RMSE: 0.7532 - val_loss: 0.8399 - val_RMSE: 0.7629\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8243 - RMSE: 0.7446 - val_loss: 0.8398 - val_RMSE: 0.7629\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8387 - RMSE: 0.7592 - val_loss: 0.8396 - val_RMSE: 0.7629\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8292 - RMSE: 0.7498 - val_loss: 0.8395 - val_RMSE: 0.7629\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8383 - RMSE: 0.7591 - val_loss: 0.8393 - val_RMSE: 0.7629\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8348 - RMSE: 0.7557 - val_loss: 0.8392 - val_RMSE: 0.7629\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8340 - RMSE: 0.7551 - val_loss: 0.8390 - val_RMSE: 0.7629\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8382 - RMSE: 0.7594 - val_loss: 0.8389 - val_RMSE: 0.7629\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8333 - RMSE: 0.7547 - val_loss: 0.8387 - val_RMSE: 0.7629\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8238 - RMSE: 0.7454 - val_loss: 0.8386 - val_RMSE: 0.7629\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8318 - RMSE: 0.7535 - val_loss: 0.8384 - val_RMSE: 0.7629\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8391 - RMSE: 0.7610 - val_loss: 0.8383 - val_RMSE: 0.7629\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8439 - RMSE: 0.7659 - val_loss: 0.8381 - val_RMSE: 0.7629\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8276 - RMSE: 0.7497 - val_loss: 0.8380 - val_RMSE: 0.7629\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8436 - RMSE: 0.7659 - val_loss: 0.8378 - val_RMSE: 0.7629\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8352 - RMSE: 0.7577 - val_loss: 0.8377 - val_RMSE: 0.7629\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8302 - RMSE: 0.7528 - val_loss: 0.8375 - val_RMSE: 0.7629\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8284 - RMSE: 0.7512 - val_loss: 0.8374 - val_RMSE: 0.7629\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8257 - RMSE: 0.7486 - val_loss: 0.8372 - val_RMSE: 0.7629\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8343 - RMSE: 0.7574 - val_loss: 0.8371 - val_RMSE: 0.7629\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8351 - RMSE: 0.7583 - val_loss: 0.8369 - val_RMSE: 0.7629\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8316 - RMSE: 0.7549 - val_loss: 0.8368 - val_RMSE: 0.7629\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8272 - RMSE: 0.7507 - val_loss: 0.8366 - val_RMSE: 0.7629\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8244 - RMSE: 0.7481 - val_loss: 0.8365 - val_RMSE: 0.7629\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8298 - RMSE: 0.7536 - val_loss: 0.8363 - val_RMSE: 0.7629\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8344 - RMSE: 0.7583 - val_loss: 0.8362 - val_RMSE: 0.7629\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8231 - RMSE: 0.7471 - val_loss: 0.8360 - val_RMSE: 0.7629\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8288 - RMSE: 0.7530 - val_loss: 0.8359 - val_RMSE: 0.7629\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8319 - RMSE: 0.7562 - val_loss: 0.8358 - val_RMSE: 0.7629\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8218 - RMSE: 0.7463 - val_loss: 0.8356 - val_RMSE: 0.7629\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8226 - RMSE: 0.7473 - val_loss: 0.8355 - val_RMSE: 0.7629\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8216 - RMSE: 0.7464 - val_loss: 0.8353 - val_RMSE: 0.7629\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8250 - RMSE: 0.7500 - val_loss: 0.8352 - val_RMSE: 0.7629\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8303 - RMSE: 0.7554 - val_loss: 0.8350 - val_RMSE: 0.7629\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8200 - RMSE: 0.7452 - val_loss: 0.8349 - val_RMSE: 0.7629\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8306 - RMSE: 0.7560 - val_loss: 0.8348 - val_RMSE: 0.7629\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8338 - RMSE: 0.7594 - val_loss: 0.8346 - val_RMSE: 0.7629\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8319 - RMSE: 0.7575 - val_loss: 0.8345 - val_RMSE: 0.7629\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8298 - RMSE: 0.7556 - val_loss: 0.8343 - val_RMSE: 0.7629\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8267 - RMSE: 0.7527 - val_loss: 0.8342 - val_RMSE: 0.7629\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8251 - RMSE: 0.7512 - val_loss: 0.8341 - val_RMSE: 0.7629\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8266 - RMSE: 0.7528 - val_loss: 0.8339 - val_RMSE: 0.7629\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8259 - RMSE: 0.7523 - val_loss: 0.8338 - val_RMSE: 0.7630\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8312 - RMSE: 0.7577 - val_loss: 0.8336 - val_RMSE: 0.7630\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8325 - RMSE: 0.7592 - val_loss: 0.8335 - val_RMSE: 0.7629\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8297 - RMSE: 0.7565 - val_loss: 0.8334 - val_RMSE: 0.7630\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8329 - RMSE: 0.7598 - val_loss: 0.8332 - val_RMSE: 0.7630\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8257 - RMSE: 0.7527 - val_loss: 0.8331 - val_RMSE: 0.7630\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8335 - RMSE: 0.7607 - val_loss: 0.8329 - val_RMSE: 0.7630\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8156 - RMSE: 0.7429 - val_loss: 0.8328 - val_RMSE: 0.7630\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8224 - RMSE: 0.7498 - val_loss: 0.8327 - val_RMSE: 0.7630\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8248 - RMSE: 0.7525 - val_loss: 0.8325 - val_RMSE: 0.7630\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8286 - RMSE: 0.7564 - val_loss: 0.8324 - val_RMSE: 0.7630\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8339 - RMSE: 0.7618 - val_loss: 0.8323 - val_RMSE: 0.7630\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8327 - RMSE: 0.7607 - val_loss: 0.8321 - val_RMSE: 0.7630\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8237 - RMSE: 0.7519 - val_loss: 0.8320 - val_RMSE: 0.7630\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8291 - RMSE: 0.7574 - val_loss: 0.8319 - val_RMSE: 0.7630\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8294 - RMSE: 0.7578 - val_loss: 0.8317 - val_RMSE: 0.7630\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8343 - RMSE: 0.7628 - val_loss: 0.8316 - val_RMSE: 0.7630\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8172 - RMSE: 0.7459 - val_loss: 0.8315 - val_RMSE: 0.7630\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8188 - RMSE: 0.7476 - val_loss: 0.8313 - val_RMSE: 0.7630\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8242 - RMSE: 0.7532 - val_loss: 0.8312 - val_RMSE: 0.7630\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8194 - RMSE: 0.7486 - val_loss: 0.8311 - val_RMSE: 0.7630\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8242 - RMSE: 0.7534 - val_loss: 0.8309 - val_RMSE: 0.7630\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8194 - RMSE: 0.7488 - val_loss: 0.8308 - val_RMSE: 0.7630\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8149 - RMSE: 0.7444 - val_loss: 0.8307 - val_RMSE: 0.7630\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8244 - RMSE: 0.7540 - val_loss: 0.8305 - val_RMSE: 0.7630\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8261 - RMSE: 0.7559 - val_loss: 0.8304 - val_RMSE: 0.7630\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8247 - RMSE: 0.7546 - val_loss: 0.8303 - val_RMSE: 0.7630\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8293 - RMSE: 0.7593 - val_loss: 0.8301 - val_RMSE: 0.7630\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8236 - RMSE: 0.7538 - val_loss: 0.8300 - val_RMSE: 0.7630\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8241 - RMSE: 0.7544 - val_loss: 0.8299 - val_RMSE: 0.7630\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8199 - RMSE: 0.7503 - val_loss: 0.8298 - val_RMSE: 0.7630\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8236 - RMSE: 0.7542 - val_loss: 0.8296 - val_RMSE: 0.7630\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8161 - RMSE: 0.7468 - val_loss: 0.8295 - val_RMSE: 0.7630\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8250 - RMSE: 0.7558 - val_loss: 0.8294 - val_RMSE: 0.7630\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8183 - RMSE: 0.7492 - val_loss: 0.8292 - val_RMSE: 0.7630\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8209 - RMSE: 0.7519 - val_loss: 0.8291 - val_RMSE: 0.7630\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8258 - RMSE: 0.7570 - val_loss: 0.8290 - val_RMSE: 0.7630\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8205 - RMSE: 0.7519 - val_loss: 0.8289 - val_RMSE: 0.7630\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8285 - RMSE: 0.7599 - val_loss: 0.8287 - val_RMSE: 0.7630\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8290 - RMSE: 0.7606 - val_loss: 0.8286 - val_RMSE: 0.7630\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8163 - RMSE: 0.7480 - val_loss: 0.8285 - val_RMSE: 0.7630\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8209 - RMSE: 0.7527 - val_loss: 0.8284 - val_RMSE: 0.7630\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8180 - RMSE: 0.7499 - val_loss: 0.8282 - val_RMSE: 0.7630\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8145 - RMSE: 0.7466 - val_loss: 0.8281 - val_RMSE: 0.7630\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=13 / Init = GlorotNormal / min_loss = 0.7626311779022217\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotNormal\n",
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_31 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_32 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_60 (Embedding)        (None, 1, 14)        406         input_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_61 (Embedding)        (None, 1, 14)        2268        input_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_15 (Dot)                    (None, 1, 1)         0           embedding_60[0][0]               \n",
            "                                                                 embedding_61[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_62 (Embedding)        (None, 1, 1)         29          input_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_63 (Embedding)        (None, 1, 1)         162         input_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 1, 1)         0           dot_15[0][0]                     \n",
            "                                                                 embedding_62[0][0]               \n",
            "                                                                 embedding_63[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_15 (Flatten)            (None, 1)            0           add_15[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,865\n",
            "Trainable params: 2,865\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.1391 - RMSE: 0.7871 - val_loss: 0.9135 - val_RMSE: 0.7728\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9062 - RMSE: 0.7647 - val_loss: 0.9108 - val_RMSE: 0.7728\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8959 - RMSE: 0.7557 - val_loss: 0.9104 - val_RMSE: 0.7727\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9037 - RMSE: 0.7638 - val_loss: 0.9101 - val_RMSE: 0.7727\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8914 - RMSE: 0.7518 - val_loss: 0.9097 - val_RMSE: 0.7726\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.9024 - RMSE: 0.7631 - val_loss: 0.9094 - val_RMSE: 0.7726\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8943 - RMSE: 0.7553 - val_loss: 0.9091 - val_RMSE: 0.7726\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8934 - RMSE: 0.7547 - val_loss: 0.9087 - val_RMSE: 0.7725\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9039 - RMSE: 0.7656 - val_loss: 0.9084 - val_RMSE: 0.7725\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8920 - RMSE: 0.7539 - val_loss: 0.9080 - val_RMSE: 0.7724\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8963 - RMSE: 0.7585 - val_loss: 0.9077 - val_RMSE: 0.7724\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8906 - RMSE: 0.7532 - val_loss: 0.9074 - val_RMSE: 0.7724\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8895 - RMSE: 0.7523 - val_loss: 0.9070 - val_RMSE: 0.7723\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8961 - RMSE: 0.7592 - val_loss: 0.9067 - val_RMSE: 0.7723\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8973 - RMSE: 0.7608 - val_loss: 0.9063 - val_RMSE: 0.7722\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8876 - RMSE: 0.7513 - val_loss: 0.9060 - val_RMSE: 0.7722\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8896 - RMSE: 0.7536 - val_loss: 0.9057 - val_RMSE: 0.7722\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8845 - RMSE: 0.7488 - val_loss: 0.9053 - val_RMSE: 0.7721\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8793 - RMSE: 0.7439 - val_loss: 0.9050 - val_RMSE: 0.7721\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8935 - RMSE: 0.7584 - val_loss: 0.9047 - val_RMSE: 0.7721\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8973 - RMSE: 0.7624 - val_loss: 0.9043 - val_RMSE: 0.7720\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8874 - RMSE: 0.7529 - val_loss: 0.9040 - val_RMSE: 0.7720\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8891 - RMSE: 0.7548 - val_loss: 0.9037 - val_RMSE: 0.7719\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8842 - RMSE: 0.7503 - val_loss: 0.9034 - val_RMSE: 0.7719\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8992 - RMSE: 0.7655 - val_loss: 0.9030 - val_RMSE: 0.7719\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8821 - RMSE: 0.7488 - val_loss: 0.9027 - val_RMSE: 0.7718\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8823 - RMSE: 0.7493 - val_loss: 0.9024 - val_RMSE: 0.7718\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8973 - RMSE: 0.7645 - val_loss: 0.9021 - val_RMSE: 0.7718\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8941 - RMSE: 0.7616 - val_loss: 0.9017 - val_RMSE: 0.7717\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8861 - RMSE: 0.7539 - val_loss: 0.9014 - val_RMSE: 0.7717\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8984 - RMSE: 0.7665 - val_loss: 0.9011 - val_RMSE: 0.7717\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8856 - RMSE: 0.7539 - val_loss: 0.9008 - val_RMSE: 0.7716\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8931 - RMSE: 0.7617 - val_loss: 0.9005 - val_RMSE: 0.7716\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8907 - RMSE: 0.7596 - val_loss: 0.9001 - val_RMSE: 0.7716\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8923 - RMSE: 0.7615 - val_loss: 0.8998 - val_RMSE: 0.7715\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8836 - RMSE: 0.7530 - val_loss: 0.8995 - val_RMSE: 0.7715\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8834 - RMSE: 0.7531 - val_loss: 0.8992 - val_RMSE: 0.7715\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8900 - RMSE: 0.7600 - val_loss: 0.8989 - val_RMSE: 0.7714\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8879 - RMSE: 0.7582 - val_loss: 0.8986 - val_RMSE: 0.7714\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8868 - RMSE: 0.7574 - val_loss: 0.8982 - val_RMSE: 0.7714\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8747 - RMSE: 0.7455 - val_loss: 0.8979 - val_RMSE: 0.7713\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8845 - RMSE: 0.7556 - val_loss: 0.8976 - val_RMSE: 0.7713\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8781 - RMSE: 0.7495 - val_loss: 0.8973 - val_RMSE: 0.7713\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8829 - RMSE: 0.7546 - val_loss: 0.8970 - val_RMSE: 0.7712\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8872 - RMSE: 0.7592 - val_loss: 0.8967 - val_RMSE: 0.7712\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8862 - RMSE: 0.7585 - val_loss: 0.8964 - val_RMSE: 0.7712\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8858 - RMSE: 0.7583 - val_loss: 0.8961 - val_RMSE: 0.7711\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8927 - RMSE: 0.7655 - val_loss: 0.8958 - val_RMSE: 0.7711\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8787 - RMSE: 0.7518 - val_loss: 0.8955 - val_RMSE: 0.7711\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8639 - RMSE: 0.7372 - val_loss: 0.8952 - val_RMSE: 0.7710\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8879 - RMSE: 0.7615 - val_loss: 0.8949 - val_RMSE: 0.7710\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8824 - RMSE: 0.7562 - val_loss: 0.8946 - val_RMSE: 0.7710\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8789 - RMSE: 0.7530 - val_loss: 0.8943 - val_RMSE: 0.7709\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8812 - RMSE: 0.7556 - val_loss: 0.8940 - val_RMSE: 0.7709\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8781 - RMSE: 0.7527 - val_loss: 0.8937 - val_RMSE: 0.7709\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8919 - RMSE: 0.7668 - val_loss: 0.8934 - val_RMSE: 0.7708\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8813 - RMSE: 0.7565 - val_loss: 0.8931 - val_RMSE: 0.7708\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8839 - RMSE: 0.7593 - val_loss: 0.8928 - val_RMSE: 0.7708\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8830 - RMSE: 0.7587 - val_loss: 0.8925 - val_RMSE: 0.7708\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8867 - RMSE: 0.7627 - val_loss: 0.8922 - val_RMSE: 0.7707\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8772 - RMSE: 0.7534 - val_loss: 0.8919 - val_RMSE: 0.7707\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8757 - RMSE: 0.7522 - val_loss: 0.8916 - val_RMSE: 0.7707\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8816 - RMSE: 0.7584 - val_loss: 0.8913 - val_RMSE: 0.7706\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8673 - RMSE: 0.7444 - val_loss: 0.8910 - val_RMSE: 0.7706\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8742 - RMSE: 0.7515 - val_loss: 0.8907 - val_RMSE: 0.7706\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8787 - RMSE: 0.7562 - val_loss: 0.8904 - val_RMSE: 0.7706\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8824 - RMSE: 0.7602 - val_loss: 0.8902 - val_RMSE: 0.7705\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8776 - RMSE: 0.7557 - val_loss: 0.8899 - val_RMSE: 0.7705\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8660 - RMSE: 0.7443 - val_loss: 0.8896 - val_RMSE: 0.7705\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8695 - RMSE: 0.7481 - val_loss: 0.8893 - val_RMSE: 0.7704\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8674 - RMSE: 0.7462 - val_loss: 0.8890 - val_RMSE: 0.7704\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8684 - RMSE: 0.7475 - val_loss: 0.8887 - val_RMSE: 0.7704\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8735 - RMSE: 0.7529 - val_loss: 0.8884 - val_RMSE: 0.7703\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8718 - RMSE: 0.7514 - val_loss: 0.8881 - val_RMSE: 0.7703\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8839 - RMSE: 0.7637 - val_loss: 0.8879 - val_RMSE: 0.7703\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8785 - RMSE: 0.7586 - val_loss: 0.8876 - val_RMSE: 0.7703\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8807 - RMSE: 0.7610 - val_loss: 0.8873 - val_RMSE: 0.7702\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8677 - RMSE: 0.7483 - val_loss: 0.8870 - val_RMSE: 0.7702\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8677 - RMSE: 0.7486 - val_loss: 0.8867 - val_RMSE: 0.7702\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8718 - RMSE: 0.7529 - val_loss: 0.8865 - val_RMSE: 0.7702\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8749 - RMSE: 0.7563 - val_loss: 0.8862 - val_RMSE: 0.7701\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8740 - RMSE: 0.7556 - val_loss: 0.8859 - val_RMSE: 0.7701\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8708 - RMSE: 0.7527 - val_loss: 0.8856 - val_RMSE: 0.7701\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8676 - RMSE: 0.7497 - val_loss: 0.8854 - val_RMSE: 0.7700\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8751 - RMSE: 0.7575 - val_loss: 0.8851 - val_RMSE: 0.7700\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8714 - RMSE: 0.7539 - val_loss: 0.8848 - val_RMSE: 0.7700\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8723 - RMSE: 0.7551 - val_loss: 0.8845 - val_RMSE: 0.7700\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8779 - RMSE: 0.7610 - val_loss: 0.8843 - val_RMSE: 0.7699\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8677 - RMSE: 0.7510 - val_loss: 0.8840 - val_RMSE: 0.7699\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8631 - RMSE: 0.7467 - val_loss: 0.8837 - val_RMSE: 0.7699\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8651 - RMSE: 0.7490 - val_loss: 0.8835 - val_RMSE: 0.7699\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8699 - RMSE: 0.7539 - val_loss: 0.8832 - val_RMSE: 0.7698\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8632 - RMSE: 0.7475 - val_loss: 0.8829 - val_RMSE: 0.7698\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8700 - RMSE: 0.7546 - val_loss: 0.8826 - val_RMSE: 0.7698\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8688 - RMSE: 0.7536 - val_loss: 0.8824 - val_RMSE: 0.7698\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8745 - RMSE: 0.7595 - val_loss: 0.8821 - val_RMSE: 0.7697\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8705 - RMSE: 0.7558 - val_loss: 0.8818 - val_RMSE: 0.7697\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8720 - RMSE: 0.7575 - val_loss: 0.8816 - val_RMSE: 0.7697\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8668 - RMSE: 0.7526 - val_loss: 0.8813 - val_RMSE: 0.7697\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8662 - RMSE: 0.7522 - val_loss: 0.8811 - val_RMSE: 0.7696\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8648 - RMSE: 0.7511 - val_loss: 0.8808 - val_RMSE: 0.7696\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8590 - RMSE: 0.7454 - val_loss: 0.8805 - val_RMSE: 0.7696\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8732 - RMSE: 0.7599 - val_loss: 0.8803 - val_RMSE: 0.7696\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8745 - RMSE: 0.7614 - val_loss: 0.8800 - val_RMSE: 0.7695\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8610 - RMSE: 0.7482 - val_loss: 0.8797 - val_RMSE: 0.7695\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8742 - RMSE: 0.7616 - val_loss: 0.8795 - val_RMSE: 0.7695\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8688 - RMSE: 0.7564 - val_loss: 0.8792 - val_RMSE: 0.7695\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8678 - RMSE: 0.7556 - val_loss: 0.8790 - val_RMSE: 0.7694\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8635 - RMSE: 0.7516 - val_loss: 0.8787 - val_RMSE: 0.7694\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8551 - RMSE: 0.7434 - val_loss: 0.8785 - val_RMSE: 0.7694\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8726 - RMSE: 0.7612 - val_loss: 0.8782 - val_RMSE: 0.7694\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8654 - RMSE: 0.7542 - val_loss: 0.8779 - val_RMSE: 0.7693\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8652 - RMSE: 0.7542 - val_loss: 0.8777 - val_RMSE: 0.7693\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8629 - RMSE: 0.7521 - val_loss: 0.8774 - val_RMSE: 0.7693\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8572 - RMSE: 0.7467 - val_loss: 0.8772 - val_RMSE: 0.7693\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8600 - RMSE: 0.7497 - val_loss: 0.8769 - val_RMSE: 0.7693\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8647 - RMSE: 0.7546 - val_loss: 0.8767 - val_RMSE: 0.7692\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8644 - RMSE: 0.7546 - val_loss: 0.8764 - val_RMSE: 0.7692\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8631 - RMSE: 0.7536 - val_loss: 0.8762 - val_RMSE: 0.7692\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8594 - RMSE: 0.7500 - val_loss: 0.8759 - val_RMSE: 0.7692\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8574 - RMSE: 0.7483 - val_loss: 0.8757 - val_RMSE: 0.7691\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8586 - RMSE: 0.7497 - val_loss: 0.8754 - val_RMSE: 0.7691\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8599 - RMSE: 0.7512 - val_loss: 0.8752 - val_RMSE: 0.7691\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8605 - RMSE: 0.7521 - val_loss: 0.8749 - val_RMSE: 0.7691\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8595 - RMSE: 0.7513 - val_loss: 0.8747 - val_RMSE: 0.7691\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8668 - RMSE: 0.7588 - val_loss: 0.8744 - val_RMSE: 0.7690\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8560 - RMSE: 0.7482 - val_loss: 0.8742 - val_RMSE: 0.7690\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8626 - RMSE: 0.7550 - val_loss: 0.8740 - val_RMSE: 0.7690\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8588 - RMSE: 0.7514 - val_loss: 0.8737 - val_RMSE: 0.7690\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8585 - RMSE: 0.7513 - val_loss: 0.8735 - val_RMSE: 0.7689\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8497 - RMSE: 0.7428 - val_loss: 0.8732 - val_RMSE: 0.7689\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8559 - RMSE: 0.7492 - val_loss: 0.8730 - val_RMSE: 0.7689\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8609 - RMSE: 0.7544 - val_loss: 0.8727 - val_RMSE: 0.7689\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8532 - RMSE: 0.7469 - val_loss: 0.8725 - val_RMSE: 0.7689\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8589 - RMSE: 0.7529 - val_loss: 0.8723 - val_RMSE: 0.7688\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8508 - RMSE: 0.7450 - val_loss: 0.8720 - val_RMSE: 0.7688\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8570 - RMSE: 0.7514 - val_loss: 0.8718 - val_RMSE: 0.7688\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8622 - RMSE: 0.7568 - val_loss: 0.8715 - val_RMSE: 0.7688\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8538 - RMSE: 0.7486 - val_loss: 0.8713 - val_RMSE: 0.7688\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8551 - RMSE: 0.7502 - val_loss: 0.8711 - val_RMSE: 0.7687\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8597 - RMSE: 0.7549 - val_loss: 0.8708 - val_RMSE: 0.7687\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8554 - RMSE: 0.7509 - val_loss: 0.8706 - val_RMSE: 0.7687\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8489 - RMSE: 0.7446 - val_loss: 0.8704 - val_RMSE: 0.7687\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8532 - RMSE: 0.7491 - val_loss: 0.8701 - val_RMSE: 0.7687\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8541 - RMSE: 0.7502 - val_loss: 0.8699 - val_RMSE: 0.7686\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8490 - RMSE: 0.7453 - val_loss: 0.8697 - val_RMSE: 0.7686\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8549 - RMSE: 0.7514 - val_loss: 0.8694 - val_RMSE: 0.7686\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8683 - RMSE: 0.7651 - val_loss: 0.8692 - val_RMSE: 0.7686\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8516 - RMSE: 0.7485 - val_loss: 0.8690 - val_RMSE: 0.7686\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8606 - RMSE: 0.7578 - val_loss: 0.8687 - val_RMSE: 0.7685\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8580 - RMSE: 0.7554 - val_loss: 0.8685 - val_RMSE: 0.7685\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8633 - RMSE: 0.7609 - val_loss: 0.8683 - val_RMSE: 0.7685\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8464 - RMSE: 0.7442 - val_loss: 0.8681 - val_RMSE: 0.7685\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8528 - RMSE: 0.7508 - val_loss: 0.8678 - val_RMSE: 0.7685\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8676 - RMSE: 0.7658 - val_loss: 0.8676 - val_RMSE: 0.7684\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8511 - RMSE: 0.7495 - val_loss: 0.8674 - val_RMSE: 0.7684\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8419 - RMSE: 0.7405 - val_loss: 0.8672 - val_RMSE: 0.7684\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8562 - RMSE: 0.7550 - val_loss: 0.8669 - val_RMSE: 0.7684\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8473 - RMSE: 0.7463 - val_loss: 0.8667 - val_RMSE: 0.7684\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8478 - RMSE: 0.7471 - val_loss: 0.8665 - val_RMSE: 0.7683\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8568 - RMSE: 0.7563 - val_loss: 0.8663 - val_RMSE: 0.7683\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8494 - RMSE: 0.7491 - val_loss: 0.8660 - val_RMSE: 0.7683\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8479 - RMSE: 0.7477 - val_loss: 0.8658 - val_RMSE: 0.7683\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8523 - RMSE: 0.7523 - val_loss: 0.8656 - val_RMSE: 0.7683\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8506 - RMSE: 0.7508 - val_loss: 0.8654 - val_RMSE: 0.7682\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8625 - RMSE: 0.7629 - val_loss: 0.8651 - val_RMSE: 0.7682\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8484 - RMSE: 0.7490 - val_loss: 0.8649 - val_RMSE: 0.7682\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8536 - RMSE: 0.7545 - val_loss: 0.8647 - val_RMSE: 0.7682\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8555 - RMSE: 0.7565 - val_loss: 0.8645 - val_RMSE: 0.7682\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8551 - RMSE: 0.7564 - val_loss: 0.8643 - val_RMSE: 0.7682\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8432 - RMSE: 0.7446 - val_loss: 0.8640 - val_RMSE: 0.7681\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8457 - RMSE: 0.7473 - val_loss: 0.8638 - val_RMSE: 0.7681\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8551 - RMSE: 0.7569 - val_loss: 0.8636 - val_RMSE: 0.7681\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8460 - RMSE: 0.7480 - val_loss: 0.8634 - val_RMSE: 0.7681\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8525 - RMSE: 0.7547 - val_loss: 0.8632 - val_RMSE: 0.7681\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8505 - RMSE: 0.7529 - val_loss: 0.8630 - val_RMSE: 0.7680\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8514 - RMSE: 0.7540 - val_loss: 0.8628 - val_RMSE: 0.7680\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8586 - RMSE: 0.7615 - val_loss: 0.8625 - val_RMSE: 0.7680\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8553 - RMSE: 0.7583 - val_loss: 0.8623 - val_RMSE: 0.7680\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8479 - RMSE: 0.7511 - val_loss: 0.8621 - val_RMSE: 0.7680\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8485 - RMSE: 0.7519 - val_loss: 0.8619 - val_RMSE: 0.7680\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8461 - RMSE: 0.7497 - val_loss: 0.8617 - val_RMSE: 0.7679\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8480 - RMSE: 0.7518 - val_loss: 0.8615 - val_RMSE: 0.7679\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8476 - RMSE: 0.7516 - val_loss: 0.8613 - val_RMSE: 0.7679\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8554 - RMSE: 0.7595 - val_loss: 0.8611 - val_RMSE: 0.7679\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8439 - RMSE: 0.7483 - val_loss: 0.8609 - val_RMSE: 0.7679\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8464 - RMSE: 0.7509 - val_loss: 0.8606 - val_RMSE: 0.7679\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8443 - RMSE: 0.7490 - val_loss: 0.8604 - val_RMSE: 0.7679\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8541 - RMSE: 0.7591 - val_loss: 0.8602 - val_RMSE: 0.7678\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8455 - RMSE: 0.7506 - val_loss: 0.8600 - val_RMSE: 0.7678\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8504 - RMSE: 0.7557 - val_loss: 0.8598 - val_RMSE: 0.7678\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8482 - RMSE: 0.7537 - val_loss: 0.8596 - val_RMSE: 0.7678\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8520 - RMSE: 0.7577 - val_loss: 0.8594 - val_RMSE: 0.7678\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8449 - RMSE: 0.7508 - val_loss: 0.8592 - val_RMSE: 0.7677\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8404 - RMSE: 0.7465 - val_loss: 0.8590 - val_RMSE: 0.7677\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8487 - RMSE: 0.7550 - val_loss: 0.8588 - val_RMSE: 0.7677\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8513 - RMSE: 0.7578 - val_loss: 0.8586 - val_RMSE: 0.7677\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8528 - RMSE: 0.7594 - val_loss: 0.8584 - val_RMSE: 0.7677\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8544 - RMSE: 0.7612 - val_loss: 0.8582 - val_RMSE: 0.7677\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8450 - RMSE: 0.7520 - val_loss: 0.8580 - val_RMSE: 0.7677\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8539 - RMSE: 0.7610 - val_loss: 0.8578 - val_RMSE: 0.7676\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8494 - RMSE: 0.7568 - val_loss: 0.8576 - val_RMSE: 0.7676\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8442 - RMSE: 0.7518 - val_loss: 0.8574 - val_RMSE: 0.7676\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8512 - RMSE: 0.7589 - val_loss: 0.8572 - val_RMSE: 0.7676\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8554 - RMSE: 0.7634 - val_loss: 0.8570 - val_RMSE: 0.7676\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8482 - RMSE: 0.7563 - val_loss: 0.8568 - val_RMSE: 0.7676\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8391 - RMSE: 0.7474 - val_loss: 0.8566 - val_RMSE: 0.7675\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8431 - RMSE: 0.7516 - val_loss: 0.8564 - val_RMSE: 0.7675\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8456 - RMSE: 0.7542 - val_loss: 0.8562 - val_RMSE: 0.7675\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8381 - RMSE: 0.7469 - val_loss: 0.8560 - val_RMSE: 0.7675\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8429 - RMSE: 0.7519 - val_loss: 0.8558 - val_RMSE: 0.7675\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8538 - RMSE: 0.7631 - val_loss: 0.8556 - val_RMSE: 0.7675\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8429 - RMSE: 0.7523 - val_loss: 0.8554 - val_RMSE: 0.7675\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8437 - RMSE: 0.7532 - val_loss: 0.8552 - val_RMSE: 0.7674\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8414 - RMSE: 0.7511 - val_loss: 0.8550 - val_RMSE: 0.7674\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8482 - RMSE: 0.7582 - val_loss: 0.8548 - val_RMSE: 0.7674\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8418 - RMSE: 0.7519 - val_loss: 0.8546 - val_RMSE: 0.7674\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8483 - RMSE: 0.7585 - val_loss: 0.8544 - val_RMSE: 0.7674\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8368 - RMSE: 0.7472 - val_loss: 0.8543 - val_RMSE: 0.7674\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8328 - RMSE: 0.7435 - val_loss: 0.8541 - val_RMSE: 0.7674\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8530 - RMSE: 0.7638 - val_loss: 0.8539 - val_RMSE: 0.7673\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8418 - RMSE: 0.7527 - val_loss: 0.8537 - val_RMSE: 0.7673\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8465 - RMSE: 0.7576 - val_loss: 0.8535 - val_RMSE: 0.7673\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8370 - RMSE: 0.7484 - val_loss: 0.8533 - val_RMSE: 0.7673\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8413 - RMSE: 0.7528 - val_loss: 0.8531 - val_RMSE: 0.7673\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8436 - RMSE: 0.7552 - val_loss: 0.8529 - val_RMSE: 0.7673\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8275 - RMSE: 0.7393 - val_loss: 0.8527 - val_RMSE: 0.7673\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8502 - RMSE: 0.7622 - val_loss: 0.8526 - val_RMSE: 0.7672\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8431 - RMSE: 0.7553 - val_loss: 0.8524 - val_RMSE: 0.7672\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8416 - RMSE: 0.7540 - val_loss: 0.8522 - val_RMSE: 0.7672\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8373 - RMSE: 0.7499 - val_loss: 0.8520 - val_RMSE: 0.7672\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8499 - RMSE: 0.7626 - val_loss: 0.8518 - val_RMSE: 0.7672\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8470 - RMSE: 0.7599 - val_loss: 0.8516 - val_RMSE: 0.7672\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8322 - RMSE: 0.7453 - val_loss: 0.8514 - val_RMSE: 0.7672\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8335 - RMSE: 0.7467 - val_loss: 0.8513 - val_RMSE: 0.7671\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8286 - RMSE: 0.7420 - val_loss: 0.8511 - val_RMSE: 0.7671\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8318 - RMSE: 0.7454 - val_loss: 0.8509 - val_RMSE: 0.7671\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8405 - RMSE: 0.7543 - val_loss: 0.8507 - val_RMSE: 0.7671\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8377 - RMSE: 0.7516 - val_loss: 0.8505 - val_RMSE: 0.7671\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8371 - RMSE: 0.7511 - val_loss: 0.8504 - val_RMSE: 0.7671\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8329 - RMSE: 0.7471 - val_loss: 0.8502 - val_RMSE: 0.7671\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8348 - RMSE: 0.7492 - val_loss: 0.8500 - val_RMSE: 0.7671\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8469 - RMSE: 0.7614 - val_loss: 0.8498 - val_RMSE: 0.7670\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8400 - RMSE: 0.7547 - val_loss: 0.8496 - val_RMSE: 0.7670\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8312 - RMSE: 0.7461 - val_loss: 0.8494 - val_RMSE: 0.7670\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8312 - RMSE: 0.7463 - val_loss: 0.8493 - val_RMSE: 0.7670\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8271 - RMSE: 0.7423 - val_loss: 0.8491 - val_RMSE: 0.7670\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8430 - RMSE: 0.7584 - val_loss: 0.8489 - val_RMSE: 0.7670\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8439 - RMSE: 0.7594 - val_loss: 0.8487 - val_RMSE: 0.7670\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8387 - RMSE: 0.7544 - val_loss: 0.8486 - val_RMSE: 0.7669\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8436 - RMSE: 0.7594 - val_loss: 0.8484 - val_RMSE: 0.7669\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8348 - RMSE: 0.7508 - val_loss: 0.8482 - val_RMSE: 0.7669\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8409 - RMSE: 0.7571 - val_loss: 0.8480 - val_RMSE: 0.7669\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8449 - RMSE: 0.7612 - val_loss: 0.8479 - val_RMSE: 0.7669\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8372 - RMSE: 0.7537 - val_loss: 0.8477 - val_RMSE: 0.7669\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8387 - RMSE: 0.7554 - val_loss: 0.8475 - val_RMSE: 0.7669\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8345 - RMSE: 0.7514 - val_loss: 0.8473 - val_RMSE: 0.7669\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8372 - RMSE: 0.7542 - val_loss: 0.8472 - val_RMSE: 0.7669\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8174 - RMSE: 0.7346 - val_loss: 0.8470 - val_RMSE: 0.7668\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8387 - RMSE: 0.7561 - val_loss: 0.8468 - val_RMSE: 0.7668\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8344 - RMSE: 0.7519 - val_loss: 0.8466 - val_RMSE: 0.7668\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8366 - RMSE: 0.7543 - val_loss: 0.8465 - val_RMSE: 0.7668\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8392 - RMSE: 0.7570 - val_loss: 0.8463 - val_RMSE: 0.7668\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8332 - RMSE: 0.7511 - val_loss: 0.8461 - val_RMSE: 0.7668\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8235 - RMSE: 0.7416 - val_loss: 0.8460 - val_RMSE: 0.7668\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8375 - RMSE: 0.7558 - val_loss: 0.8458 - val_RMSE: 0.7668\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8351 - RMSE: 0.7535 - val_loss: 0.8456 - val_RMSE: 0.7667\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8245 - RMSE: 0.7431 - val_loss: 0.8455 - val_RMSE: 0.7667\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8356 - RMSE: 0.7543 - val_loss: 0.8453 - val_RMSE: 0.7667\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8415 - RMSE: 0.7604 - val_loss: 0.8451 - val_RMSE: 0.7667\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8315 - RMSE: 0.7506 - val_loss: 0.8450 - val_RMSE: 0.7667\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8279 - RMSE: 0.7471 - val_loss: 0.8448 - val_RMSE: 0.7667\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8281 - RMSE: 0.7474 - val_loss: 0.8446 - val_RMSE: 0.7667\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8377 - RMSE: 0.7573 - val_loss: 0.8444 - val_RMSE: 0.7667\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8376 - RMSE: 0.7572 - val_loss: 0.8443 - val_RMSE: 0.7666\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8339 - RMSE: 0.7537 - val_loss: 0.8441 - val_RMSE: 0.7666\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8397 - RMSE: 0.7596 - val_loss: 0.8439 - val_RMSE: 0.7666\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8363 - RMSE: 0.7565 - val_loss: 0.8438 - val_RMSE: 0.7666\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8301 - RMSE: 0.7504 - val_loss: 0.8436 - val_RMSE: 0.7666\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8305 - RMSE: 0.7509 - val_loss: 0.8435 - val_RMSE: 0.7666\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8405 - RMSE: 0.7611 - val_loss: 0.8433 - val_RMSE: 0.7666\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8329 - RMSE: 0.7537 - val_loss: 0.8431 - val_RMSE: 0.7666\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8250 - RMSE: 0.7459 - val_loss: 0.8430 - val_RMSE: 0.7666\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8268 - RMSE: 0.7478 - val_loss: 0.8428 - val_RMSE: 0.7665\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8402 - RMSE: 0.7615 - val_loss: 0.8426 - val_RMSE: 0.7665\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8273 - RMSE: 0.7487 - val_loss: 0.8425 - val_RMSE: 0.7665\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8260 - RMSE: 0.7475 - val_loss: 0.8423 - val_RMSE: 0.7665\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8342 - RMSE: 0.7558 - val_loss: 0.8422 - val_RMSE: 0.7665\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8346 - RMSE: 0.7564 - val_loss: 0.8420 - val_RMSE: 0.7665\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8274 - RMSE: 0.7494 - val_loss: 0.8418 - val_RMSE: 0.7665\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8212 - RMSE: 0.7433 - val_loss: 0.8417 - val_RMSE: 0.7665\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8355 - RMSE: 0.7578 - val_loss: 0.8415 - val_RMSE: 0.7665\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8316 - RMSE: 0.7540 - val_loss: 0.8414 - val_RMSE: 0.7664\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8230 - RMSE: 0.7455 - val_loss: 0.8412 - val_RMSE: 0.7664\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8326 - RMSE: 0.7553 - val_loss: 0.8410 - val_RMSE: 0.7664\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8151 - RMSE: 0.7379 - val_loss: 0.8409 - val_RMSE: 0.7664\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8241 - RMSE: 0.7471 - val_loss: 0.8407 - val_RMSE: 0.7664\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8255 - RMSE: 0.7486 - val_loss: 0.8406 - val_RMSE: 0.7664\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8314 - RMSE: 0.7547 - val_loss: 0.8404 - val_RMSE: 0.7664\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8231 - RMSE: 0.7465 - val_loss: 0.8403 - val_RMSE: 0.7664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3xV9f348df7Zu9NEkhIwt4ECFBQRASUKqh1otY62lqtq7a4W6VW+9Xaauv41VpX1VawONCiICIIqAhh700gC0JCNtmf3x/n5OZm3TBySYD38/E4j9z7OeeefE6iefP5vD9DjDEopZRSx8rR0RVQSil1etHAoZRS6rho4FBKKXVcNHAopZQ6Lho4lFJKHRfvjq7AqRAdHW2Sk5M7uhpKKXVaWb169WFjTEzT8rMicCQnJ5Oent7R1VBKqdOKiGS0VK5dVUoppY6LBg6llFLHRQOHUkqp43JW5DiUUqen6upqMjMzqaio6OiqnNH8/f1JSEjAx8fnmK7XwKGU6rQyMzMJCQkhOTkZEeno6pyRjDHk5+eTmZlJSkrKMX1Gu6qUUp1WRUUFUVFRGjQ8SESIioo6rladBg6lVKemQcPzjvdnrIHDjY/WZvLv71scxqyUUmctDRxufLIum9mrDnR0NZRSHSA/P5/U1FRSU1OJi4ujW7duzvdVVVVuP5uens4999xzXN8vOTmZwYMHM2TIEMaPH09GRsM/WkWEH//4x873NTU1xMTEMHXqVAAOHjzI1KlTGTp0KAMGDODiiy8GYN++fQQEBDjrnZqayttvv31c9WqJJsfdcIhQpxtdKXVWioqKYt26dQDMnDmT4OBgZsyY4TxfU1ODt3fLf0LT0tJIS0s77u+5ePFioqOjefzxx3nyySf55z//CUBQUBCbNm3i6NGjBAQEsHDhQrp16+b83GOPPcbkyZO59957AdiwYYPzXM+ePZ3P0V60xeGGiFBX19G1UEp1FjfffDO33347o0eP5oEHHmDlypWMGTOGYcOGMXbsWLZv3w7AkiVLnK2BmTNncuutt3L++efTo0cPXnjhhTa/z5gxY8jKympUdvHFFzNv3jwA3nvvPa677jrnuZycHBISEpzvhwwZctLP6o62ONxwCNriUKqT+P2nm9mSXdyu9xzQNZTHpw08rs9kZmby7bff4uXlRXFxMcuWLcPb25svv/ySRx55hA8++KDZZ7Zt28bixYspKSmhb9++3HHHHW7nTMyfP5/LL7+8Udn06dN54oknmDp1Khs2bODWW29l2bJlANx5551ce+21vPTSS0yaNIlbbrmFrl27ArB7925SU1Od93nxxRcZN27ccT1zUxo43BABjRtKKVdXX301Xl5eABQVFXHTTTexc+dORITq6uoWP3PJJZfg5+eHn58fXbp04eDBg41aCPUmTJhAQUEBwcHB/OEPf2h0bsiQIezbt4/33nvPmcOod9FFF7Fnzx7mz5/P559/zrBhw9i0aRPgma4qDRxuOEQwaORQqjM43paBpwQFBTlf/+53v2PChAl89NFH7Nu3j/PPP7/Fz/j5+Tlfe3l5UVNT0+J1ixcvJjw8nBtuuIHHH3+c5557rtH5Sy+9lBkzZrBkyRLy8/MbnYuMjOT666/n+uuvZ+rUqSxdupQRI0ac4FO6pzkON6zkeEfXQinVWRUVFTmT1G+99Va73NPb25u//vWvvP322xQUFDQ6d+utt/L4448zePDgRuVfffUV5eXlAJSUlLB79266d+/eLvVpiQYON0RzHEopNx544AEefvhhhg0b1mor4kTEx8dz3XXX8fLLLzcqT0hIaHGY7+rVq0lLS2PIkCGMGTOGn/3sZ4wcORJoyHHUH8eSnG+LmLPgD2NaWpo5kY2c7n5vLZuyilg84/z2r5RSqk1bt26lf//+HV2Ns0JLP2sRWW2MaTauWFscbjjEWgBMKaVUAw0cbmiOQymlmvNo4BCRKSKyXUR2ichDLZx/XkTW2ccOESm0yye4lK8TkQoRudw+95aI7HU5l9r0vu1Xf81xKKVUUx4bjisiXsDLwGQgE1glIp8YY7bUX2OMuc/l+ruBYXb5YiDVLo8EdgFfuNz+fmPMHE/VvZ5DROdxKKVUE55scYwCdhlj9hhjqoBZwGVurr8OeK+F8quAz40x5R6oo1uCtjiUUqopTwaOboDr0rKZdlkzIpIEpABftXB6Os0DylMissHu6vJr4TOIyG0iki4i6Xl5ecdfe7TFoZRSLeksyfHpwBxjTK1roYjEA4OBBS7FDwP9gJFAJPBgSzc0xrxqjEkzxqTFxMScUKUcDm1xKHW2Opll1cFa6PDbb79t8dxbb71FTEwMqamp9OvXj+eff955bubMmYgIu3btcpb99a9/RUSon1bwxhtvOJdgHzRoEHPnzgWsRRhTUlKc9Rw7duzJ/Aha5cklR7KARJf3CXZZS6YDd7ZQfg3wkTHGuQCMMSbHflkpIm8CM1r4XLsQHVWl1FmrrWXV27JkyRKCg4Nb/eNdvyhhfn4+ffv25aqrriIx0fqTOXjwYGbNmsVvf/tbAP773/8ycKC15EpmZiZPPfUUa9asISwsjNLSUlx7VZ599lmuuuqqE3rmY+XJFscqoLeIpIiIL1Zw+KTpRSLSD4gAvmvhHs3yHnYrBLH2Orwc2NTO9XbSeRxKKVerV69m/PjxjBgxgosuuoicHOvfsS+88AIDBgxgyJAhTJ8+nX379vHKK6/w/PPPk5qa6lzFtiVRUVH06tXLeS+Ayy+/3NmK2L17N2FhYURHRwNw6NAhQkJCCA4OBiA4OJiUlBRPPXKLPNbiMMbUiMhdWN1MXsAbxpjNIvIEkG6MqQ8i04FZpslfaBFJxmqxfN3k1v8WkRis3PU64HZPPYOgGzkp1Wl8/hDkbmzfe8YNhh8+fUyXGmO4++67mTt3LjExMcyePZtHH32UN954g6effpq9e/fi5+dHYWEh4eHh3H777cfUStm/fz8VFRWN9tAIDQ0lMTGRTZs2MXfuXK699lrefPNNAIYOHUpsbCwpKSlMnDiRK664gmnTpjk/e//99/Pkk08CMHDgQP79738f70+lTR5dHdcY8xnwWZOyx5q8n9nKZ/fRQjLdGHNB+9XQPYega+MqpQCorKxk06ZNTJ48GYDa2lri4+MBa8nzG264gcsvv7zZPhqtmT17NkuXLmXbtm289NJL+Pv7Nzo/ffp0Zs2axYIFC1i0aJEzcHh5eTF//nxWrVrFokWLuO+++1i9ejUzZ84ETk1XlS6r7oa1A6CGDqU6hWNsGXiKMYaBAwfy3XfNe9XnzZvH0qVL+fTTT3nqqafYuLHtllF9jiM9PZ0LL7yQSy+9lLi4OOf5qVOncv/995OWlkZoaGijz4oIo0aNYtSoUUyePJlbbrnFGThOhc4yqqpT0uG4Sql6fn5+5OXlOQNHdXU1mzdvpq6ujgMHDjBhwgSeeeYZioqKKC0tJSQkhJKSkjbvm5aWxo033sjf/va3RuWBgYE888wzPProo43Ks7OzWbNmjfP9unXrSEpKaocnPHba4nBDlxxRStVzOBzMmTOHe+65h6KiImpqavjVr35Fnz59+PGPf0xRURHGGO655x7Cw8OZNm0aV111FXPnzm1zu9YHH3yQ4cOH88gjjzQqnz59erNrq6urmTFjBtnZ2fj7+xMTE8Mrr7ziPO+a4wBYuXIlvr6+7fATaKDLqrvx1LwtvLtiP1v/MMUDtVJKtUWXVT91dFn1dqJbxyqlVHMaONzQCYBKKdWcBg43dAKgUh1P/x/0vOP9GWvgcMNKjnd0LZQ6e/n7+5Ofn6/Bw4OMMeTn5zebR+KOjqpywxqOq//BKtVREhISyMzM5ERXuFbHxt/fn4SEhGO+XgOHG33zv2SqZIG52Gp+KKVOKR8fn1O+DpNqmwYONwYe+h9Tfb/FvLIE6TcVel8EXYdZ660rpdRZSv8CuvG/gc/zYPXPwScYlj4Lr10Af+kDH90Bmz6Ao0c6uopKKXXKaYvDHfFidu0EnrzlT/hUFsKuL2HHAtjxOaz/D4gDEkZCr8nQexLEDdXWiFLqjKeBww2Hw8prGAMERsKQa6yjrhayVsPOhbBrISx+0jqCukCvidBrEvS8wPqMUkqdYTRwuFGfD2+2XpXDCxJHWccFj0JpHuxeZAWSHfNh/XtWa6RbGvSebAWS+FRtjSilzggaONxwiEuLw53gGBg63TrqaiFrjdUS2bkQFv8RFj8FQTGQMh56nA89J0DYsQ99U0qpzkQDhxv1A3CPa4VchxckjrSOCY/YrZGvrPzI3q9h0xzruqjeVndWzwmQfC74hbR7/ZVSyhM0cLhR3+I4qaXVg2Ng6LXWYQwc2gK7F8OexbDmbVj5D3B4Q8Io6DHeapV0GwHe7bsMslJKtRcNHG7U5zjabe64CMQOtI6xd0F1BRz43goiuxfDkqdhyf+BTyB0/4HVEkkeZ80d8fJpr1oopdRJ0cDhhjPHUeehb+Djb7UyeoyHSTOteSH7vrG6tPYth0VP2NcFWYEkZZwVSOJTwUt/dUqpjqF/fdxwtDaqylMCIqD/VOsAKz+S8Y0VRPYtgy9nWuW+wZA4GpLGQPexVteWz7EvUKaUUidDA4cb0h45jpMRHAMDL7cOgNJDdhBZDvu/g6/s7SG9fKHr8IZA0n00+Id1TJ2VUmc8DRxuONo7x3GygrvAoCusA6C8APavgP3fQsZ38O2LsPx5QCB2kBVIksZawSQktkOrrpQ6c2jgcKPDWxxtCYyEfhdbB0BVGWSmW62RjG9h7buw8lXrXGQPK4AkjYHEH0BUT13xVyl1QjRwuHHMEwA7C9+ghmQ7QG015Ky3gsj+72D7PFj3rnUuIMJaZythJCSkWXkS7d5SSh0DDRxunPLkeHvz8rGCQkIanHMP1NXB4e1wYCVkrrKOnV/YFwvE9LOXUhltHdoqUUq1QAOHGw1rVXVsPdqNwwFd+lvHiJussqOFkL0GDtiBZMvHsOZf1rmACIgfag3/7ZpqfY1I1mCi1FlOA4cb4uyqOlMiRwsCwu2lTy6w3tfVweEd1sTErNWQsw6+exnqqq3zQV1cWiWjIG4I+AZ2XP2VUqecRwOHiEwB/gZ4Aa8ZY55ucv55YIL9NhDoYowJF5EJwPMul/YDphtjPhaRFGAWEAWsBm40xlR5ov6nXY6jPTgc0KWfddS3SmqqrKVSslZbrZID38O2/1nnxMtqwXQdZgWRuMHWzHj/0I57BqWUR3kscIiIF/AyMBnIBFaJyCfGmC311xhj7nO5/m5gmF2+GEi1yyOBXUB9Z/wzwPPGmFki8grwU+DvnniG0z7H0V68fa2uqq6pMPKnVllpnhVEstdYqwFvmwdr37E/IBDdB7oNt5Lu3YZbw4O9/TrsEZRS7ceTLY5RwC5jzB4AEZkFXAZsaeX664DHWyi/CvjcGFMuVt/RBcD19rl/ATPxWOCoH47ribuf5oJjGg8FNgZKciB3ozWSK2uNtSLw+ves8w4fiBtkTVSMHwKxg62WinZzKXXa8WTg6AYccHmfCYxu6UIRSQJSgK9aOD0deM5+HQUUGmNqXO7ZrZV73gbcBtC9e/fjrbt9D+vrWd/iOBYiENrVOvpcZJUZA0UHrCBS3zLZ8D6kv25/xgGRPa2urbhBVjCJGwSh3TQBr1Qn1lmS49OBOcaYWtdCEYkHBgMLjveGxphXgVcB0tLSTugvv5yNOY72JALh3a2jftmUujoozICDmyB3k/U1e601mqteQITVtRU7yA4og6yhwroel1KdgicDRxaQ6PI+wS5ryXTgzhbKrwE+MsbYQ3rIB8JFxNtudbi750lzLjmikaP9OBwQmWId/ac1lFcUw8HNdkDZaH1d/RbUHLXOi5eVN6kPJPVBJThWWydKnWKeDByrgN72KKgsrOBwfdOLRKQfEAF818I9rgMern9jjDEishgr7zELuAmY2/5Vt2iO4xTyD7XX1hrTUFZXCwV77EBiB5WM72DjfxuuCYxuCCZxg62v0X10IyylPMhjgcMYUyMid2F1M3kBbxhjNovIE0C6MeYT+9LpwCzT5J/1IpKM1WL5usmtHwRmiciTwFrgdU89wwltHavaj8MLontbR/3CjmAt7uhsnWyCgxth5T+httL+nI/VtdWlP8T0sQJJdF9rvS4NKEqdNI/mOIwxnwGfNSl7rMn7ma18dh8tJL7tUVqj2q2SbmiOo5MKjLQ2tUoZ11BWWwP5OxsCSe4ma42uje83XCNeVhdZTD9raHHcUGu+SmiC1YWmlDomnSU53inpPI7TiJd3w3IqXN1QXllqBZTDO60Z8XnbrdZK/QRGsLbqje4DMX2tI9r+GpGiOy0q1QL9v8KNs3Lm+JnGL9ia1d51WOPyo4XWbPi87faxzdoga8Pshmu8fK3hws6A0sdqrUT10hFe6qymgcON+t4LbXGcgQLCrU2uksY2Lq8otlsn2xuCSu4G2PpJw+bz4oDwJKvbKyLFyp1E97HyKWGJVm5GqTOYBg43hE6+kZNqf/6hkDDCOlxVV0D+roaAcngHFOy1JjVWFDZc5+VnBZSoXtay9JE97de9rB0cdeiwOgNo4HCj/v9xDRsKH39r2G/coObnygvs/Mk2K7jk77FaLTu/gFqX9Td9g5sEk55WayWyBwRGaVBRpw0NHG44zoZl1dXJC4yE7j+wDld1tdaSK/m77WMXFOy2ll/Z8nFD1xeAX6g9MdIlmNQf2lJRnYwGDjd0AqA6KQ4va+OriGToNbHxuZoqa+mVgj2Nj5x1sGUuuK6+4xvcOJ/ifJ1ireulORV1imngcMM5HFcjh2pv3r4Nkxubqq2Gwv1WDsUZVHbDoa2w/fOGTbXAGvnVNFEfmWIFq/AkHf2lPEIDhztn2tax6vTg5WPlP6J6Nj9XVwvFWQ1B5che+/Vea8JjVWnj60O62kEk0VpsMiyxYeHJsATdI0WdEA0cbjhzHJoeV52Fw6vhD3+P8Y3PGQNleXBkn3UU7G14nfGttcaXa14FIDjOvp9rYEmy3ocl6n4pqkUaONzQCYDqtCJiJdKD7X3hm6qthuJsK2FfuB8K7a9F+61tgbd80rgbDKxFJOsDS5h9hNcHl+66RfBZSgOHG7rkiDqjePlARJJ1tKSuFkpyXQKLfRQdsJZp2fFFwzL39fzDrQAS2hVC4iAk3krY1web0ARdWPIMpIHDDdEchzqbOLwgrJt1NB1aDFbTuzzfGg1WuB+OZLgElyzITIfyw00+JBAUAyGxVlAJiXcJMl0hNN76GhipQ45PIxo43BCdx6FUAxEIiraObiNavqamykreFx1o6AorybFaMiU51m6PZXnNP+fl2zyYhMTZQSbeLosHnwDPPqM6Jho43NAch1LHydu3YYfH1tRUQelBK5AUZ9tBJRuKc6yynA2wYwFUlzf/rH94QzBxDSj1rZjgOKuFo6sae5T+dN3QHIdSHuDtayfYE1u/xhioLLaDiR1cirPtYGMHmENbrADUdKQYdssoONYKIkEx1oCBoBirLCTWCjAhcdb+9tpFdtw0cLihM8eV6iAi4B9mHV36tX5dbQ2UHWoIKKUH7dZMLpQesrrFCnZDaV7zxD5Yi1I6u8TiIMgOMPWBJzi2YaSaznlx0sBxDLTFoVQn5eVt/dEP7drCfqEujLEmR5YesoPKwYa8S33QydkAZYehsqjle/iH20EktuWvQXaACYw+47vKzuynO0ma41DqDCECfiHW0dKMfFc1lVZLpfSQfRxs+Fpml2Wvtb42nalvfTNrlFhQFwiOaQgozi6z+nK768zLxyOP7EkaONyo38hJR1UpdRbx9rOWYwlLaPvaqjKXAJNrB5y8hgBTlmdNrizLaznIiJcVTPxCrXxLULQVdALt0WuB0daS+0FRDWWdYGSZBg43NMehlHLLN6jtUWT1qsrtgJJnBZKyQ9aQ5dJca+fJo0es9ccyV1nzZepqWr6PT1DjYBIYZQeZqMav/UKtFlZIXLu3ajRwuKGjqpRS7cY3EHyTrUUn22KMtbNkWb41qbI838q/lB+2y+zysjxrA7Hy/JaHLwP88nv3AwxOgAYOt3TrWKVUBxCxuq4CIoBex/aZqvKGgFJeYA1nriyxBg60Mw0cbjh0eLdS6nThG2gd7ubHtBOHx7/Daawhx6EtDqWUqqeBww1n4Gg6MVUppc5iGjjcEE2OK6VUMxo43KgPHBo2lFKqgQYONxy6rLpSSjXj0cAhIlNEZLuI7BKRh1o4/7yIrLOPHSJS6HKuu4h8ISJbRWSLiCTb5W+JyF6Xz6V6qv46AVAppZrz2HBcEfECXgYmA5nAKhH5xBizpf4aY8x9LtffDQxzucXbwFPGmIUiEgy4pqjvN8bM8VTd6+kEQKWUas5ti0NELnB5ndLk3BVt3HsUsMsYs8cYUwXMAi5zc/11wHv2vQcA3saYhQDGmFJjTCvTIj1It45VSqlm2uqq+rPL6w+anPttG5/tBhxweZ9JKwsfi0gSkAJ8ZRf1AQpF5EMRWSsiz9otmHpPicgGu6urxUXyReQ2EUkXkfS8vBa2qjwGDmd2XCOHUkrVaytwSCuvW3p/MqYDc4wxtfZ7b2AcMAMYCfQAbrbPPQz0s8sjgQdbuqEx5lVjTJoxJi0mJuaEKqU5DqWUaq6twGFaed3S+6ayANe57wl2WUumY3dT2TKBdXY3Vw3wMTAcwBiTYyyVwJtYXWIeoTkOpZRqrq3keA8R+QSrdVH/Gvt9W+sIrwJ627mRLKzgcH3Ti0SkHxABfNfks+EiEmOMyQMuANLt6+ONMTkiIsDlwKY26nHCRFscSinVTFuBwzWZ/ecm55q+b8QYUyMidwELAC/gDWPMZhF5Akg3xtQHoenALOMyWcIYUysiM4BFdoBYDfzTPv1vEYnBCl7rgNvbeIYT1pDi0MihlFL13AYOY8zXru9FxAcYBGQZYw61dXNjzGfAZ03KHmvyfmYrn10IDGmh/IIWLvcI3TpWKaWaa2s47isiMtB+HQasx5pfsVZErjsF9etQmuNQSqnm2kqOjzPGbLZf3wLsMMYMBkYAD3i0Zp2AjqpSSqnm2gocVS6vJ2ONbsIYk+uxGnVC2uJQSqkGbQWOQhGZKiLDgHOA+QAi4g0EeLpyHc05AVAppZRTW6OqfgG8AMQBv3JpaUwE5nmyYp2BM8ehfVVKKeXU1qiqHcCUFsoXYA2zPaNpjkMppZpzGzhE5AV3540x97RvdToX3QFQKaWaa6ur6nasmdnvA9m07/pUnZ7oRk5KKdVMW4EjHrgauBaoAWZjLUZY6PZTZxCH6NaxSinlyu2oKmNMvjHmFWPMBKx5HOHAFhG58ZTUrhNwiGhXlVJKuTimHQBFZDjWRkuTgc+x1o46K1iBo6NroZRSnUdbyfEngEuArVg7+D1sL3N+1hDR5LhSSrlqq8XxW2AvMNQ+/mgnjAUwxphmixCeaUR0kUOllHLVVuBoa8+NM55DREdVKaWUi7YmAGa0VC4iDqycR4vnzySa41BKqcbaWlY9VEQeFpGXRORCsdwN7AGuOTVV7Fia41BKqcba6qp6BziCta3rz4BHsPIblxtj1nm4bp2Clczp6FoopVTn0eae4/b+G4jIa0AO0N0YU+HxmnUSDofmOJRSylVby6pX178wxtQCmWdT0ADNcSilVFNttTiGikix/VqAAPt9/XDcUI/WrhNwaI5DKaUaaWtUldepqkhnJdriUEqpRtrqqjrr2U2rjq6GUkp1Gho42mBNAOzoWiilVOehgaMNmuNQSqnGNHC0QXMcSinVmAaONjgcmuNQSilXGjjaIOhGTkop5UoDRxt061illGrMo4FDRKaIyHYR2SUiD7Vw/nkRWWcfO0Sk0OVcdxH5QkS2isgWEUm2y1NE5Hv7nrNFxNeTz6Azx5VSqjGPBQ4R8QJeBn4IDACuE5EBrtcYY+4zxqQaY1KBF4EPXU6/DTxrjOkPjAIO2eXPAM8bY3phLcD4U089g/UcOqpKKaVcebLFMQrYZYzZY4ypwtp69jI3118HvAdgBxhvY8xCAGNMqTGmXKztBy8A5tif+RdwuacewK6LJseVUsqFJwNHN+CAy/tMu6wZEUnC2m3wK7uoD1AoIh+KyFoRedZuwUQBhS77nru7520iki4i6Xl5eSf8EA6BuroT/rhSSp1xOktyfDowx16BF6w1tMYBM4CRQA/g5uO5oTHmVWNMmjEmLSYm5oQr5hDBaHpcKaWcPBk4soBEl/cJdllLpmN3U9kygXV2N1cN8DEwHMgHwkWkfnFGd/dsFzoBUCmlGvNk4FgF9LZHQfliBYdPml4kIv2ACKxdBl0/Gy4i9U2FC4Atxko2LAausstvAuZ6qP6APRxXcxxKKeXkscBhtxTuAhYAW4H3jTGbReQJEbnU5dLpwCzj8tfZ7rKaASwSkY1Yi9T+0z79IPBrEdmFlfN43VPPAPWjqjz5HZRS6vTS1kZOJ8UY8xnwWZOyx5q8n9nKZxcCQ1oo34M1YuuUcOioKqWUaqSzJMc7Lc1xKKVUYxo42qDLqiulVGMaONqgGzkppVRjGjjaIGiLQymlXGngaIO2OJRSqjENHG3QRQ6VUqoxDRxt0BaHUko1poGjDdriUEqpxjRwtMHayEkDh1JK1dPA0QbRrWOVUqoRDRxtqN869sn/beHHr33f0dVRSqkOp4GjDQ6Bkopq/rNyP8t3HWbnwZJG5ytratl7uIzqWt3tSSl1dtDA0QYRYU9eGeVV1h5TH6zJYnN2EWv3HwHgj/O2MuHPS5jw5yXO4FFWWcPqjCMdVmellPIkDRxtOFxaCUD/+FDG9Y7mla93c8kLy7n6le/4bGMO76dnkhgZQOaRo6zYkw/Ai1/t4sq/f8umrKKOrLpSSnmEBo42TBkUx7je0fzrlpE8feUQZk4bwB9/NJieMcH88t9rOFpdy1+vTSXI14vPNuZijOHT9dkAPLdwBxsyC/nFO+lUVFstlvKqGmat3E9tG0vuHq2q5YE568k8Uu7xZ1RKqePh0f04zgS/PL8Xvzy/l/P9zeekAHDRwFjeXbGfOmMYkRTJBf1jmb8ph6lD4skqPMqQhDC+2naIffll7MkrY/TzxV8AACAASURBVP2BQkb3iOKTddk89OFGwgN9mTIortXvO2vVft5PzyTYz4fHpg3w+HMqpdSx0hbHCYoK9uPeSb25b3IfAG4ak8SR8mpuezudIF8vXvtJGtHBvuzJKwNgzf5CALbkFAPw7ooMvtic2+omUel2jiTAV39FSqnORf8qtZO05EimDe1KeXUtz12bSpdQf+6d1IdgP29iQvxYYyfTt9qBY/muw9z2zuoWk+jVtXUs25EHwKHiylP3EEopdQw0cLSjZ68awmf3jOOigVYX1I0/SCL9t5M4r3cMazKOkHmknG05JVw4IJbz+8YAsONgabP7zNuQQ3FFDQC5xRWn7gGUUuoYaOBoR/4+XvSPD21WNrpHJPllVZz7zGJKKmsY3zeGN24aSYCPF7sOlVJVU8e3uw5TV2eorq3juYU76B8fyuQBsWQeOcqjH21k7+GyRvetrTNc/LdlfLgm81Q+olJKaeA4Fa4cnsCc28cQ6OsFWEN7HQ6hZ5cgVu0r4NKXlnP9a9/zwZpM1mQcYX9BOXdO6El8mD97D5fx7+/38/LiXY3uub+gnC05xfz7+/0d8UhKqbOYBo5TwMshpCVH8vX9E/jd1AGkJoQD0DMmmI1ZRWzLLaFrmD9vfrPPmQNJS4okNtTfeY95G3Ioqah2vq+fwb464wgHtTtLKXUKaeA4hWJC/PjpuSk4HAJYgQNgaEIYd0/szZacYmatOkB4oA+xoX7Eh1mBI9DXi6PVtfx5wXbnKKydhxpyI59vzGn0fTZnF5FX0jypXlFdS2VNrUeeTSl19tDA0YFiQ/0AmDIonmlDu+Lr7WBbbgn940IREeLsFseUgXHcck4y//oug99/uoW6OsOOgyV0Cw9gcLcw3lmRQZ09odAYwyUvLGfkU182Cx4/eWMlw55YyP82ZJ/aB1VKnVE0cHSgHw1L4NmrhvDzcSkE+3kzrlc0AP3iQwBIjAwEYGRKJI9NHcDPzk3hrW/3MXjmAuauy6ZXl2B+em4Ku/PK+NoevptT1NBt9Zcvtjtf5xQdZeXeAsqrarn/vxvIKjx6qh5TKXWG0cDRgXy9HVydloi3l/VruMieSd4/zhqZlRgZyH9vH8NVIxIQER69pD8vXjeMYd0jAOgWEcDFg+OJC/XnteV7ANiWa+VIwgJ8nHNEnpm/jWkvLgfgrVtGYjA8O38bADW1ddTVGV5ctJNHPtp4ip5cKXU60yVHOpFLBsezNaeYyQNinWUjkyOdr0WEaUO7ctHAOP6+ZDeXDInH19vBTWOTeWb+NrZkF7M1x0qaX5OWwGvL91JaWcPfl+wGrCXix/eJ4dq0RGatOsCTlTU8+tFG1u4vJKvwKHXGcPcFvYgPC2izrodKKvhm12F+NCyhnX8KSqnOTlscnUiQnzePTxtIRJCv2+t8vR3cO6k3vbpYyfXrR3UnwMeL/6zMYGtOMQkRAYztFY0xsNTuwkqMDODVG9MQEaYO7UplTR0frM7ks4057C8ox8/bgTFw3+x1PDN/G//6dh+3v7O61Tpc+fdvuW/2eh3RpdRZSFscZ4CwQB/O6xPNl1sOEejrRb+4UIbaQ34/XJMFwBOXDWJC3y4AjOgeQVyoP49/shmAl64fRu8uIcz473pW7ClgxZ4CQvy9KamoIfNIOQkRgXy6Ppt/fbuP/3fDcLwcwoECK0ey82ApsaH+GGMoLK9uM+gppU5/Hm1xiMgUEdkuIrtE5KEWzj8vIuvsY4eIFLqcq3U594lL+VsistflXKonn+F0Mal/LLnFFew5XMb4PtFEBvnSPTKQJdsPAdDLHvoL4HAIMy+1VtztEuLHJYPj6RsXwv9dMZinrxiMr5eDEnvJk+U7DwPwwqKdpGcc4cevf8/vP93ivNeOgyVU1tTy8bosRv9xEXsPl5GR33iWO6A7JCp1BvFY4BARL+Bl4IfAAOA6EWm0Prgx5j5jTKoxJhV4EfjQ5fTR+nPGmEub3P5+l3PrPPUMp5ML+nXBIdAzJojpo7oDVjCpqTP4ejvoGt44bzFlUDxL75/AnNvHImLNKxnULYzpo7pz+bCuJEYGEBvqx7KdhzlSVsXuvFJGJUeSXVjBJ+uzuWtCL8IDfXjl692M+MOXvLZsL1W1dfz4te8Z/+wSZ8AB2JRVxMDHFrAlu7jN5zDG8P+W7GJ/vu5DolRn5cmuqlHALmPMHgARmQVcBmxp5frrgMc9WJ8zWlSwH89eNZT+8aH42KO0Lh4cxxvf7KVHdBBe9qRDV92jAlu815OXD6aqto4nPt3M55tyWbA5lzoDj17SnyA/b9bsP8LVIxJYubeAlfsKANhsB4X6Yb7PLthGbOhQSitr+HpHHlW1dSzZcYgBXRvW8qqoruWtb/cxdUg8CRFWXXYcLOVP87dTUFrFwxf35ydvfM8No5O4eHB8+/2wlFInxZNdVd2AAy7vM+2yZkQkCUgBvnIp9heRdBFZISKXN/nIUyKywe7q8mvlnrfZn0/Py8s7icc4fVw5IqHRH+bh3SPoFh7QbOHFtvh6Owj28+ay1G6UVNTw5LytzsmGvboEc01aIiJCr1ir+yvSzmvcO7E3EYE+3DWhF+szi5j8/FJ+9P++5c1v9gGwcm8BmUfKGTxzARsyC3nkw408/fk2rnnlOzKPlLP3cJlz+91V+wrYkl3MN7vyefGrXa3uW6KUOvU6S3J8OjDHGOO6HkaSMSZLRHoAX4nIRmPMbuBhIBfwBV4FHgSeaHpDY8yr9nnS0tLOyr86Docw544xBPh4ndDnx/SIIjEygAMFR/nd1P7OpVLqDe8ewdy1Wcy+7QdszCriR8O6OTe2unBgLLsOlfKXL3aQVXgUEVi97wirM45QUlHDq0v38L8NOVw1IoH5m3L5yRsr2ZNXhq+39W+ZTdnFLNp2ELD2MNmQWcTQRCvhf6ikglB/H/xP8LmUUifHky2OLCDR5X2CXdaS6cB7rgXGmCz76x5gCTDMfp9jLJXAm1hdYqoV8WEBhAee2Egnh0O48/xejOsdzZXDm8/XuHJ4N1Y8MpHesSFcMTzBmSsBGJIQzhXDE/jpudZWu1OHdKWksoZ5G6x1tT7flAvAryf34VeTerMnrwwvh1BVU0dsqB+1dYYXFu2kW3gAPl7C/M25bMgsJKfoKD/86zLueW8tAJ9tzGHt/iP8evY6Xly084SeUyl1fDzZ4lgF9BaRFKyAMR24vulFItIPiAC+cymLAMqNMZUiEg2cA/zJPhdvjMkR66/U5cAmDz7DWW/6qO7OZHtTIkKIv4/bz984Jon4MH8GdQvj0/XZLNpmjfKqrTPEhfrTNTyAm8YmAzCsewS/eCed31zYlz/8bwslFdbeJd/vyWfnwVL+vuQb532/2HKQeRty+PX76+jVJZhtuSUM7mYtFqmU8iyPBQ5jTI2I3AUsALyAN4wxm0XkCSDdGFM/xHY6MMs07sTuD/xDROqwWkVPG2Pqk+r/FpEYQIB1wO2eegZ18ny8HPzQTmx3DfMn22UtreFJ4c5rfjauBwCrHp2EiDA6JZKP1mZxWWo3DhVX8N3uhlFavboEU1Nbx6/fX0dlTZ0zMb+/oJyNmUWIWCPE6tXaC0A2HSCQkV/GgYKjnNs72gNPrtSZy6M5DmPMZ8BnTcoea/J+Zguf+xYY3Mo9L2jHKqpTaGRKJHPXZdMjJog9eWUMt9fcclXf3ZUUFcSvJln5kuSoIL7carVURqVEcsf4nhRXVHPvrHX4eAnVtVZgKCirYtpL1ppcr/x4OFMGxWOM4RfvpFNSUcPsX4xp9L3+tGA7S7fnsWHmhY262ZRS7nWW5Lg6C4xMtgLHDaOT2J5bzLShXY/pc8nRQc7Xb948kiA/b+rqDHNWZzIqOZJXl+6hvLrW2bIAuP3dNdw8NtmaUW8HnZ0HS+gdG8Jry/aw9kAhazKOUFJZQ35ZFdHBDYPz1h8o5Istufxmct9mAwKUUho41Ck0sX8X/vN9KBP7dXEmzY9Fih04EiICCPKz/pN1OIR3fjoagIHdQjlUXMlDH1qr+75+Uxpfbj3EW9/uY+66LLpHBpJVeJQP1mTx4JS+vP1dBvsLGiYYZuSXEeznzcHiCiKCfLnj3dVkF1XQIzqYK0c0HxSwIbOQX81ax+xfjCEmpMXR4Eqd0TRwqFMmPiyAz+4dd9yfq29x9IkNafH8Bf1iKa2scQaOEUkR/KBHFF9tO0hZZS3//Ekaf/liO298s5ewAJ9GQQMgI7+cueuyefu7DCKDfCksryIpKpA/f7Gdy1K7Ope9B2vS4rKdh9lzuIwl2w9xdVoiJ6q2zrQ4MVOpzk5Xx1WdXnyoP9HBfgzvHt7qNcF+3kQF+ZIUFUh4oC9Bft68/4sxzL3rHPrGhfDMlUMYEB/KM/Y+JCnRQfh6OxCB7QdL+GhtFj1jghjePYJ3fzaaeyf2JqeogteX7+Xivy2jpKKaLzbnMuyJhc7lVL7ZdbjV+pRX1bh9ptyiCno+8hlzVmeewE9EqY6lLQ7V6TkcwqLfjCfQ1/2Ev4n9uzTqOkqKasiNRAT58p+fj+Y376+nsqaOeyf2Zl9+GX+av53Xl+2lps7w9xtGOEdY7T1sLdT41y93crS6lo/XZbPhQCFHq2v5zp7d/s3ufKpq6tiaU4y3l5AYGchzX+xgdEok985axwNT+jpHizX11GdbAfhgdSZXtdAd1ppfz15HTKgfD/+w/zF/Rqn2JmfDUg5paWkmPT29o6uhOqFpLy5nY1YRyVGBLPrN+c6uI2MMqU8spOhoNQD940Mpqagm84i1FleXED8OlVTi6+Wgyl7595Ih8c4JjgAi8O5PR/PG8r08fHE/enUJIbeoAi+HMPKpLwHoFxeCj5eDG3+QxDUj3Xd7VVTXMmTmF8SF+bP0gQnNztfY9XDtWlPqZIjIamNMWtNybXGos1rXcH82ZhXx3LWpjfINIsLQxHCW7sgjLSmCdHsbXodAnYF7J/Wmrs6w93A5w5PCefjDjczbkIO3Q6ipMzwwpS/PfbGDx+ZuYndeGamJ4fSLL+fnb6dzxTBrybb+8aFszbHmoDz7xXaOVtey81AJ16Z1Z3BCwzyU7MKjzF51gDpjqKqtY39BOf9NP0BMiB/n23usgLW5Vk2d4ZUfj6CwvJp+8SFU1dQ5BxQo1V70vyh1Vnvy8sHccX4vUhOb50+G2YHjuWtSuf61FWQeOcplqd34aG0Wg7qGOdfOAmvfklmrDnDz2GSuGJ5A//gQFm87xKp9VsBZtusw/1hq7Qv/4Vpr5Z2pQ+KdgSOvpNK5sdbRqjr+cs1QAMoqa7j85W84VFLZqG73z9lAv7gQZ+BYs/8I6zOLAHjif1vYmFnEtKHxzN+cy+LfnH9MrZDlOw/zi3fSWXDfec7VipVqibZp1VktJsSvxaABcOu5Kbx5y0i6RwUy7+5xvHT9MGZOG8jj0wYw2GVmOsA1IxPx9XIwbWhXBnQNRUQY07NhRvrKvQWUVtYQHmgt0RLk6+XckXFUSiSPXtyf129KY0yPKPYcLgVgY2YRj83dzKGSSh692MppxIX6O++542AJm7KKWHegkCf/17BbwdIdeeQWV7B4ex4HCo7y8bpstueWNHu+7MKjvLMiwzn/5aO1WZRV1fKqHeBaU1VTxzsrMjhaVev2OnXm0sChVCvCAnycf9zDAn2YOqQrYYE+3HJOSosrBW/8/YWNWiHn9IwCoHuk9a/32FA/rrfX/erVJZheXYLpEuLHZald+fl5PZjYP5aeXYLYfaiU6to6bnhtBR+ssZLnPz+vBzePTeb28T1IigpE7C6zq175lstf/oY1+wu52V7zq7LGynXsOmQFoBn/Xc+lLy3nUEnj/eFfW7aX3328ifv/ux5jDJU1ViD4b3omR8qqWv25vJ9+gN99vIk5qw+0eo06s2lXlVLtxM+78aivUSmRvHDdMIL9vLj1rXQm9Y9liJ276NklGF9vBysenojraic9Y4Iprqhh4ZaDFFfU8Kcrh3B1mjXqaualAwHoExdCUXk1d/x7DRXVdYxIiiA1MZxfTerNW9/ua1SHnjFBVNcaDhwp5/mFO7h4cDwV1XUYY9hnb/H74dosfjmhJ3sPlxEX6k9ucQVzVmfy8/OajwirrTP8c5nVIvl8Uy43jklujx+dOs1o4FDKQ0SES4d2pbyqhvP7xnD96O5E2Evc97UnMzZtufSw94Z/bdkefLyEHw6Oa7aO1li7Cywu1J+C8irevGUkofYqxfFh/uTYI7dq6wz/d8UQRiZHcOd/1vDeygO8t7KhlRDq782IpAhWZxxh2c7D7D1cxjVpiWzOLuIfS/fw/d4C/nTVEEL8vVmwOZcpA+P4x9I9ZOSXM7BrKN/vLeD/LdnFlcMTiHXpQgMorawhyNerXdcA+9uXOzlcWskfLh/UbvdUJ0YDh1IeFujrzVu3NGwb8+5PR5PaymTGnjHW3JM1+wsZ2zPK7bL114/uTk1tnTNogNUFVlVTR1yYP5uzi+kbG4KI8McfDebqEYkE+nqxNaeYmZ9uobiihon9u3C4tJKP12ZRXlVLj5ggxvaM4vZ3V/PVtoM8t3A7aUmR/Gr2Oq4Ybg0MmDa0K3dO6MnFf1vGn+Zv543l+/jol2OJD/Pn5cW7iQvz4/FPNvO7qQO4YXRSmz+fyppadh0qJTEysNGzuKqrM7yzYh/GwH2T+1BWWUN8mD81dUY39OoAGjiUOsXcLePeNSzA+fqO83u6vc89Lew9MuPCvhwureTLrQcprawhzE7Ghwf6MqGfla8ZmRzJS4t3c7i0kv5xoZzTK5r/fL8fsFYiPq9PDNuf/CFP/m8L76zIYEeulSv5cI217tczVw4m0Neb1b+dzIEj5Vz60jfM35TLwG6hPP/lDmdd5q7NPqbAcfUr37Ehs4grhyc4R5M1tTm7mMOlVt7l1++vY0duCZMGxLJ812EW3jf+mJZuKa+qYdW+I4zvE9PmtWCttnzLW6v4y9VD6NWl5eVuzlYaOJTqRBwO4cNfjrWXTwlq+wNN1CfnR6VEUlbZ8qgnh0M4r3c0H67Nom9cCFHBvs7A0bOL1VXm4+Xgzgt68Z+V+1m5r4BeXYIprajh2auGEOhr/dmICPIlIsiXuFB/NmcXOZPvVw5PoKKmlnkbcnh2wTbyS6u4Oi2REUkR1NTW8emGbFITI0iJDqLoaDUb7GHEW+yhyfe8t5YQf2+e+pG1s8KLi3byl4UNAWnZzsPU1hn+8/1+auoMy3cddgaD3XmlrN1fyA8HxZGRX86ArqHOz81edYDff7qFZQ9MIDGy7eHG6w8Usv5AIQu3HNLA0YQGDqU6mZb2KTleIf4+bru5fjouhfhwf+LDrF0YVz46kZ0HS+kW3tDi6RLiz5RB8Xy6PpubxyZzw+juLeYsBnYNZXN2MXXGMK53NH+5ZijbcouZtyGHlxfvxs/bwbwNOcy5Yyy/m7uJlXsLSIoKZN4949hoB41+cSHsPVxKYXkV8zbmWPuonNeTtQeOOINGbKgfB4srncOHa+oMIvDCop0E+noxMjmSiX/5GrD+6L+zIoM3bx7pbGntOGi1nLbllhxT4MiwBw+sO3CkzWvPNjocV6mz0MCuYdx/UT9nIOgS4s85vZp3of3ivB70iQ1mUv/YVhPdA7qGsvNQKbvzyrjA/iPdNzaE564Zysd3nsNXM87H20u4+c2VrNxbwHWjunOgoJy/fbmD9ZmFAFyW2o2K6jreTz9AbZ2hzsBry/fw7ILtDE0MZ9PvL+Lze89zfk9/HwdBvl7MuLAvqzOOcO0/vuNvXzbsOf/phmwAbn93Nf/3+VaMMey2hyfvONh4TkttnWHdgULq6hovv5Rhr6K8dn8h8zbkNJu3Ur9cU22d4aY3VvLZxhzOFtriUEq1alC3ML64b7zbawa6dAfVL9goIlwxvGHxxtvO68kz87cRHezH7y8dSHFFNXNWZ5KaGE5KdBDD7MECry+3lr6f2K8L767IoM7Aryb1IdjPG/wa1gj7x41p+HgJY3tGc+OYJH782veN8iuF5dWMSokkLtSff3y9hwHxoezOswJH/WTIHQdLeOiDDXSLCOTT9dlck5bAzEsHOrviMvKtwHGopJI7/7OGmdMGUFFTx5XDEwjx9+bcZ77i+lHdObd3DF/vyGNTVhHn9o5uMcG/OqOAqCC/RpuSudqUVURZZQ2je0S5/4V0EtriUEqdlGF219pjUwe02j1245gk4sP8ufXcZHy9HVyTlsiR8moWb89jWPdwetijyQ4WVzKpfyx3nN+TOgO+3g4uHBjrvE9yVBAh/t6c1zvaOSw51N+H2beN4b2f/4AlM87H115eZVyvaP56bSoDu4by5Lyt5NuTGutbHB+syWTN/kI+XZ9Nn9hg3k/P5NxnFpNVaC1kmZFfRtewhmHGn6zP5unPt/HOigz2Hi7jcGkVL3y1iz/8bwveDqGgvIq3m8yjAatl8rN/pfOwvV9MU2WVNdz85iqm/3MFs1ft50hZFdl2HTorbXEopU5KbKg/2/4wxe2w2GA/b7558ALnZMdze0XTNzaE8EAfHpzSjxiXrXvvvqAXydFBTB+ZiL+PV6N/wU8flUhOUUWzbrMAXy/G2DP1e8QEsS23hL5xITgcwu+mDmD6qysAK5eyLbeEn761igNHyukeGcgNo7vzkzHJpGcUcOPrK/l0fTZFR6vZnVfGL87rwYikCJ5buIM1+61uteU78xgQ35As35hVxMjkCMoqrSX377qg8Wi3jPxyjpRX8/3efA6XVjbaphisVtbh0kp6RAfxwqJdfL0jj83ZxSyZcX6z5yyuqGbnwRJSEyM6dBMwDRxKqZN2LHMpXCc7ejmEz+8d16js1nNSiAlp6M55+sohze7h2v3Vmt6xIc7AAfCDHlFcNDCWBZsPcuOYJGZ+splF26x96O+Z2JtfjLeGPZ/bK5ouIX68uGgnZXY+IyEykAsHxrF4+yG22V1c6zOL2JRljQBb8Kvz+N3cTfzy/J4s3naI/67OZOXeAnrEBPH43M2M7xvjbAHVGZi/KZdu4QGEBfowvHsEdfbosPP7xjA6JYpn5m+jpKKa4ooadueVUV5Vw/6CcqYO6QrAYx9v4uN12fSPD2XunefgECg8Wt0sGHmaBg6lVIdoOmv+sWkD2uW+43pHsz23mESXFX6f+tFgLhwQx5UjErh+VHce+mAjs9MPMLZnQ05BREhLjuCzjbmEB/oQ6u/DqORIoGGmf1SQL/llVcxatZ+IQB/6xoXw/i/GAFBcUcO/vsvgmn98x8jkCFbtO0JxRTU9Y4IJ8PGiW0QAr3y9m8wjR3EI7Pm/S1h74Ai5xRU89MN+RAb5Ou8DsHjbIWat2k9W4VEuGhhHQVkV/9uQ4xzF9tnGHI6UV/Gn+dtZ9uAEZ/BYta+ABz/YwEd3nOOcx9PeNMehlDqjXJOWyBf3jW8UmKKD/bjSJXH/yCX9+eOPBjsDQ70RSdb7a0cmsvSBCc5WS794awDA9FGJhPp7c7i0iu5N5tmMTG4YRl2/nP7GrCLWHihkcEIYM6cNdG4EVmeguraOeRty8fV2MLF/l0aDDHy9Hby0eBe788qoqK5jU1YRf/xsK7XG8NL1w+kRE8Try/eSnnHE2qHSXqofrNWR9+SV8d2e1rc2PlkaOJRSZ52wAB+uH929Wavngn5diAv154phjbvEhiaEc+GAWH40rBuT+lvJ+gSXOS8A8WEB/HxcCre5LA5ZWF7N+gOF/KBHFOf2jubxaQOYOiQesEZ3Ld2Z51xaJirYj7hQfxwCv790ICKQGGl9jzveXcPcddn8amIfUqKDuH5UdzZmFTn3vZ+zOtM5PHinPV9lxZ6C9vpxNaOBQymlbCnRQax4ZKKzpVEvwNeLV3+SRq8uIUwaYAWO+m2FXT16yQBmXNiX2FA/zunV0A126VArR3HLOSk8OKUfAF/vyGPXoVLSkhpaKmnJEQxOCOe6Ud1Z89vJfD1jArGhfuQWV3D1iATunWQl3sf1tmbKF5ZX09MeDPDSV7sA2HHIysWs2JPfLj+TlmiOQymljsMF/bowqX8X7pzQq8Xzvt4Ovvz1eBwiDP39F/SLD6GXvZQLQEJEAFFBvry+fC/QeKWAZ64cQk2t1XKobw2NSIpgweaD3O0yWqtPbDDRwb4cLq3igSn9mL8pl78s3EFpZQ0Z+eUE+XqxLbeEob//gk/vOpfuUe27o6MGDqWUOg7+Pl68dtNIt9fUz2d5cEo/+sU3br2ICBcOjHUucT/EZfOvlvaHv/+iflw9IrHRH//6HSY/XZ/NgPhQJvTtgo+XOLcnvmdib/YeLsPX24GfT/t3LGngUEopD2lpMyyw/rDXB47gFoKFq5ToIFJamHF+4w+SCPTxIiEiwLl0/vvpmQCc1yfGOczYEzwaOERkCvA3wAt4zRjzdJPzzwMT7LeBQBdjTLh9rhaon2q53xhzqV2eAswCooDVwI3GmNb3uVRKqU4mPiyAf9w4goCT2EtkVEoko1IaRoV5ezlY9JvxzF51gD6xnl3NV+oz8e1+YxEvYAcwGcgEVgHXGWO2tHL93cAwY8yt9vtSY0xwC9e9D3xojJklIq8A640xf3dXl7S0NJOenn5yD6SUUmcZEVltjElrWu7JUVWjgF3GmD12i2AWcJmb668D3nN3Q7Hm318AzLGL/gVc3g51VUopdYw8GTi6AQdc3mfaZc2ISBKQAnzlUuwvIukiskJE6oNDFFBojKk5hnveZn8+PS8v72SeQymllIvOkhyfDswxxrgueJ9kjMkSkR7AVyKyESg61hsaY14FXgWrq6pda6uUUmcxT7Y4soBEl/cJdllLptOkm8oYk2V/3QMsAYYB+UC4iNQHPHf3VEop5QGeDByrgN4ikiIivljB4ZOmF4lIPyAC+M6lLEJE/OzX0cA5wBZjZfIXA1fZl94EzPXgMyillGrCY4HDzkPcBSwAtgLvG2M2i8gTInKpu8dfMAAABl5JREFUy6XTgVmm8fCu/kC6iKzHChRPu4zGehD4tYjswsp5vO6pZ1BKKdWcx4bjdiY6HFcppY5fRwzHVUopdQY6K1ocIpIHZJzgx6MBzy1sf2rps3RO+iyd05nyLCfzHEnGmJimhWdF4DgZIpLeUlPtdKTP0jnps3ROZ8qzeOI5tKtKKaXUcdHAoZRS6rho4Gjbqx1dgXakz9I56bN0TmfKs7T7c2iOQyml1HHRFodSSqnjooFDKaXUcdHA4YaITBGR7SKyS0Qe6uj6HA8R2SciG0VknYik22WRIrJQRHbaXyM6up6tEZE3ROSQiGxyKWux/mJ5wf49bRCR4R1X88ZaeY6ZIpJl/27WicjFLucetp9ju4hc1DG1bpmIJIrIYhHZIiKbReReu/x0/L209iyn3e9GRPxFZKWIrLef5fd2eYqIfG/Xeba9ZiAi4me/32WfTz7ub2qM0aOFA2u7291AD8AXWA8M6Oh6HUf99wHRTcr+BDxkv34IeKaj6+mm/ucBw4FNbdUfuBj4HBDgB8D3HV3/Np5jJjCjhWsH2P+d+WHtT7Mb8OroZ3CpXzww3H4dgrXD54DT9PfS2rOcdr8b++cbbL/2Ab63f97vA9Pt8leAO+zXvwResV9PB2Yf7/fUFkfrjncHw9PBZVi7JkIn3z3RGLMUKGhS3Fr9LwPeNpYVWEvvx5+amrrXynO05jKsBT8rjTF7gV1Y/x12CsaYHGPMGvt1Cdbipd04PX8vrT1Lazrt78b++Zbab33sw9D6bqmuv685wER7d9VjpoGjdce8g2EnZYAvRGS1iNxml8UaY3Ls17lAbMdU7YS1Vv/T8Xd11/9v735C46qiOI5/f/6h1kZahAriQk0VFKHGPxS1VQRRqCuFiqJWEZfddCdS/4B7dVW0iELVIFJtQNzZCIEupIrGWv8XVwVpNhqpoEh6XNwz6Rj77NwyyZsnvw+EzLz3MnMON5OTd+fNPTl980bflGFn8sjpjRso/912elyW5AIdHBtJ50qaBeaAjyhnRE3dUhdzyf3zlJXGB+bC8f+1JSJuBLYCOyTd0b8zynlqZ6/F7nj8rwAbgAngZ+DFdsOpI2kMeB/YGRG/9e/r2ricJpdOjk1ELETEBKW53SbgmuV8PheOZjUdDEdOnOqgOAdMUX6ZjvemCvL7XHsRnpWm+Ds1VhFxPF/oJ4HXODXlMfJ5SDqf8od2MiL25+ZOjsvpcuny2ABExK+UHka30twtdTGX3L+W0l11YC4czQbqYDiKJK2RdFHvNnAPcIQS/+N5WBe7JzbF/wHwWF7Fcwsw3zd1MnKWzPPfTxkbKHk8lFe9XAlcDRxa6fia5Dz468C3EfFS367OjUtTLl0cG0nrJa3L26uBuynv2TR1S+0fr23Ax3mmOLi2rwgY5S/KVSE/UOYLd7UdT0Xc45QrQL4Evu7FTpnHnAZ+BA4AF7cd63/k8A5lquAvyvzsk03xU64q2Z3j9BVwc9vxnyGPtzLOw/kivrTv+F2Zx/fA1rbjX5LLFso01GFgNr/u7ei4NOXSubEBNgJfZMxHgOdy+ziluB0F9gGrcvsFef9o7h+vfU4vOWJmZlU8VWVmZlVcOMzMrIoLh5mZVXHhMDOzKi4cZmZWxYXDbMRJulPSh23HYdbjwmFmZlVcOMyGRNKj2RdhVtKeXHjuhKSXs0/CtKT1eeyEpE9yMb2pvh4WV0k6kL0VPpe0IR9+TNJ7kr6TNFm7mqnZMLlwmA2BpGuBB4HNURabWwAeAdYAn0XEdcAM8Hz+yJvAUxGxkfJJ5d72SWB3RFwP3Eb51DmU1Vt3UvpCjAOblz0pswbnnfkQMxvAXcBNwKd5MrCastjfSeDdPOZtYL+ktcC6iJjJ7XuBfbm+2GURMQUQEX8A5OMdiohjeX8WuAI4uPxpmf2bC4fZcAjYGxFP/2Oj9OyS4852jZ8/+24v4NeutchTVWbDMQ1sk3QJLPbhvpzyGuutUPowcDAi5oFfJN2e27cDM1E60R2TdF8+xipJF65oFmYD8H8tZkMQEd9IeobSdfEcymq4O4DfgU25b47yPgiUZa1fzcLwE/BEbt8O7JH0Qj7GAyuYhtlAvDqu2TKSdCIixtqOw2yYPFVlZmZVfMZhZmZVfMZhZmZVXDjMzKyKC4eZmVVx4TAzsyouHGZmVuVvSU6cNdWpx5UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=14 / Init = GlorotNormal / min_loss = 0.7663712501525879\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotNormal\n",
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_33 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_34 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_64 (Embedding)        (None, 1, 15)        435         input_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_65 (Embedding)        (None, 1, 15)        2430        input_34[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_16 (Dot)                    (None, 1, 1)         0           embedding_64[0][0]               \n",
            "                                                                 embedding_65[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_66 (Embedding)        (None, 1, 1)         29          input_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_67 (Embedding)        (None, 1, 1)         162         input_34[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 1, 1)         0           dot_16[0][0]                     \n",
            "                                                                 embedding_66[0][0]               \n",
            "                                                                 embedding_67[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_16 (Flatten)            (None, 1)            0           add_16[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 3,056\n",
            "Trainable params: 3,056\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.1538 - RMSE: 0.7788 - val_loss: 0.9202 - val_RMSE: 0.7749\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9061 - RMSE: 0.7575 - val_loss: 0.9172 - val_RMSE: 0.7748\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9068 - RMSE: 0.7596 - val_loss: 0.9168 - val_RMSE: 0.7747\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9010 - RMSE: 0.7540 - val_loss: 0.9164 - val_RMSE: 0.7747\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8951 - RMSE: 0.7485 - val_loss: 0.9161 - val_RMSE: 0.7746\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9017 - RMSE: 0.7554 - val_loss: 0.9157 - val_RMSE: 0.7746\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8994 - RMSE: 0.7535 - val_loss: 0.9154 - val_RMSE: 0.7745\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8943 - RMSE: 0.7486 - val_loss: 0.9150 - val_RMSE: 0.7745\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.9051 - RMSE: 0.7598 - val_loss: 0.9147 - val_RMSE: 0.7744\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9044 - RMSE: 0.7593 - val_loss: 0.9143 - val_RMSE: 0.7744\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.9037 - RMSE: 0.7590 - val_loss: 0.9140 - val_RMSE: 0.7744\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.9048 - RMSE: 0.7604 - val_loss: 0.9136 - val_RMSE: 0.7743\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9092 - RMSE: 0.7652 - val_loss: 0.9133 - val_RMSE: 0.7743\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8970 - RMSE: 0.7533 - val_loss: 0.9129 - val_RMSE: 0.7742\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9030 - RMSE: 0.7595 - val_loss: 0.9126 - val_RMSE: 0.7742\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9076 - RMSE: 0.7644 - val_loss: 0.9122 - val_RMSE: 0.7741\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9046 - RMSE: 0.7617 - val_loss: 0.9119 - val_RMSE: 0.7741\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8918 - RMSE: 0.7493 - val_loss: 0.9116 - val_RMSE: 0.7740\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9046 - RMSE: 0.7624 - val_loss: 0.9112 - val_RMSE: 0.7740\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8953 - RMSE: 0.7533 - val_loss: 0.9109 - val_RMSE: 0.7740\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8947 - RMSE: 0.7531 - val_loss: 0.9105 - val_RMSE: 0.7739\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8994 - RMSE: 0.7581 - val_loss: 0.9102 - val_RMSE: 0.7739\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8979 - RMSE: 0.7569 - val_loss: 0.9099 - val_RMSE: 0.7738\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8926 - RMSE: 0.7519 - val_loss: 0.9095 - val_RMSE: 0.7738\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.9003 - RMSE: 0.7599 - val_loss: 0.9092 - val_RMSE: 0.7737\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.9110 - RMSE: 0.7709 - val_loss: 0.9088 - val_RMSE: 0.7737\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8948 - RMSE: 0.7550 - val_loss: 0.9085 - val_RMSE: 0.7737\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8963 - RMSE: 0.7568 - val_loss: 0.9082 - val_RMSE: 0.7736\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8961 - RMSE: 0.7569 - val_loss: 0.9078 - val_RMSE: 0.7736\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8917 - RMSE: 0.7527 - val_loss: 0.9075 - val_RMSE: 0.7735\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9017 - RMSE: 0.7630 - val_loss: 0.9072 - val_RMSE: 0.7735\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8934 - RMSE: 0.7551 - val_loss: 0.9069 - val_RMSE: 0.7734\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8905 - RMSE: 0.7525 - val_loss: 0.9065 - val_RMSE: 0.7734\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8971 - RMSE: 0.7594 - val_loss: 0.9062 - val_RMSE: 0.7734\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8967 - RMSE: 0.7593 - val_loss: 0.9059 - val_RMSE: 0.7733\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8898 - RMSE: 0.7526 - val_loss: 0.9055 - val_RMSE: 0.7733\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8979 - RMSE: 0.7611 - val_loss: 0.9052 - val_RMSE: 0.7732\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8992 - RMSE: 0.7627 - val_loss: 0.9049 - val_RMSE: 0.7732\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8878 - RMSE: 0.7515 - val_loss: 0.9046 - val_RMSE: 0.7732\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8927 - RMSE: 0.7567 - val_loss: 0.9042 - val_RMSE: 0.7731\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8805 - RMSE: 0.7449 - val_loss: 0.9039 - val_RMSE: 0.7731\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8811 - RMSE: 0.7457 - val_loss: 0.9036 - val_RMSE: 0.7730\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8860 - RMSE: 0.7508 - val_loss: 0.9033 - val_RMSE: 0.7730\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8828 - RMSE: 0.7480 - val_loss: 0.9030 - val_RMSE: 0.7730\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8934 - RMSE: 0.7589 - val_loss: 0.9026 - val_RMSE: 0.7729\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8899 - RMSE: 0.7557 - val_loss: 0.9023 - val_RMSE: 0.7729\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8915 - RMSE: 0.7575 - val_loss: 0.9020 - val_RMSE: 0.7728\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8919 - RMSE: 0.7582 - val_loss: 0.9017 - val_RMSE: 0.7728\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.9054 - RMSE: 0.7720 - val_loss: 0.9014 - val_RMSE: 0.7728\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8853 - RMSE: 0.7522 - val_loss: 0.9011 - val_RMSE: 0.7727\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8915 - RMSE: 0.7587 - val_loss: 0.9007 - val_RMSE: 0.7727\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8852 - RMSE: 0.7527 - val_loss: 0.9004 - val_RMSE: 0.7727\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8931 - RMSE: 0.7608 - val_loss: 0.9001 - val_RMSE: 0.7726\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8826 - RMSE: 0.7506 - val_loss: 0.8998 - val_RMSE: 0.7726\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8806 - RMSE: 0.7488 - val_loss: 0.8995 - val_RMSE: 0.7725\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8872 - RMSE: 0.7557 - val_loss: 0.8992 - val_RMSE: 0.7725\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8892 - RMSE: 0.7581 - val_loss: 0.8989 - val_RMSE: 0.7725\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8845 - RMSE: 0.7537 - val_loss: 0.8986 - val_RMSE: 0.7724\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8884 - RMSE: 0.7578 - val_loss: 0.8983 - val_RMSE: 0.7724\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8885 - RMSE: 0.7582 - val_loss: 0.8980 - val_RMSE: 0.7724\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8881 - RMSE: 0.7580 - val_loss: 0.8977 - val_RMSE: 0.7723\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8833 - RMSE: 0.7535 - val_loss: 0.8974 - val_RMSE: 0.7723\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8889 - RMSE: 0.7594 - val_loss: 0.8970 - val_RMSE: 0.7722\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8895 - RMSE: 0.7603 - val_loss: 0.8967 - val_RMSE: 0.7722\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8961 - RMSE: 0.7671 - val_loss: 0.8964 - val_RMSE: 0.7722\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8823 - RMSE: 0.7536 - val_loss: 0.8961 - val_RMSE: 0.7721\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8873 - RMSE: 0.7589 - val_loss: 0.8958 - val_RMSE: 0.7721\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8800 - RMSE: 0.7518 - val_loss: 0.8955 - val_RMSE: 0.7721\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8818 - RMSE: 0.7539 - val_loss: 0.8952 - val_RMSE: 0.7720\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8895 - RMSE: 0.7619 - val_loss: 0.8949 - val_RMSE: 0.7720\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8807 - RMSE: 0.7533 - val_loss: 0.8946 - val_RMSE: 0.7720\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8821 - RMSE: 0.7551 - val_loss: 0.8944 - val_RMSE: 0.7719\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8804 - RMSE: 0.7536 - val_loss: 0.8941 - val_RMSE: 0.7719\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8869 - RMSE: 0.7603 - val_loss: 0.8938 - val_RMSE: 0.7719\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8731 - RMSE: 0.7468 - val_loss: 0.8935 - val_RMSE: 0.7718\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8847 - RMSE: 0.7587 - val_loss: 0.8932 - val_RMSE: 0.7718\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8800 - RMSE: 0.7542 - val_loss: 0.8929 - val_RMSE: 0.7718\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8820 - RMSE: 0.7565 - val_loss: 0.8926 - val_RMSE: 0.7717\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8842 - RMSE: 0.7590 - val_loss: 0.8923 - val_RMSE: 0.7717\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8754 - RMSE: 0.7505 - val_loss: 0.8920 - val_RMSE: 0.7716\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8833 - RMSE: 0.7586 - val_loss: 0.8917 - val_RMSE: 0.7716\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8778 - RMSE: 0.7534 - val_loss: 0.8914 - val_RMSE: 0.7716\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8763 - RMSE: 0.7522 - val_loss: 0.8911 - val_RMSE: 0.7716\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8831 - RMSE: 0.7593 - val_loss: 0.8909 - val_RMSE: 0.7715\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8899 - RMSE: 0.7663 - val_loss: 0.8906 - val_RMSE: 0.7715\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8736 - RMSE: 0.7502 - val_loss: 0.8903 - val_RMSE: 0.7714\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8639 - RMSE: 0.7408 - val_loss: 0.8900 - val_RMSE: 0.7714\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8712 - RMSE: 0.7484 - val_loss: 0.8897 - val_RMSE: 0.7714\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8755 - RMSE: 0.7529 - val_loss: 0.8894 - val_RMSE: 0.7713\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8781 - RMSE: 0.7558 - val_loss: 0.8891 - val_RMSE: 0.7713\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8767 - RMSE: 0.7546 - val_loss: 0.8889 - val_RMSE: 0.7713\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8722 - RMSE: 0.7504 - val_loss: 0.8886 - val_RMSE: 0.7713\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8817 - RMSE: 0.7602 - val_loss: 0.8883 - val_RMSE: 0.7712\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8785 - RMSE: 0.7572 - val_loss: 0.8880 - val_RMSE: 0.7712\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8800 - RMSE: 0.7589 - val_loss: 0.8877 - val_RMSE: 0.7712\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8722 - RMSE: 0.7514 - val_loss: 0.8875 - val_RMSE: 0.7711\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8781 - RMSE: 0.7576 - val_loss: 0.8872 - val_RMSE: 0.7711\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8713 - RMSE: 0.7510 - val_loss: 0.8869 - val_RMSE: 0.7711\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8823 - RMSE: 0.7622 - val_loss: 0.8866 - val_RMSE: 0.7710\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8842 - RMSE: 0.7644 - val_loss: 0.8864 - val_RMSE: 0.7710\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8758 - RMSE: 0.7563 - val_loss: 0.8861 - val_RMSE: 0.7710\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8616 - RMSE: 0.7423 - val_loss: 0.8858 - val_RMSE: 0.7709\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8783 - RMSE: 0.7592 - val_loss: 0.8855 - val_RMSE: 0.7709\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8783 - RMSE: 0.7595 - val_loss: 0.8853 - val_RMSE: 0.7709\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8736 - RMSE: 0.7551 - val_loss: 0.8850 - val_RMSE: 0.7708\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8625 - RMSE: 0.7442 - val_loss: 0.8847 - val_RMSE: 0.7708\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8769 - RMSE: 0.7588 - val_loss: 0.8844 - val_RMSE: 0.7708\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8750 - RMSE: 0.7572 - val_loss: 0.8842 - val_RMSE: 0.7708\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8709 - RMSE: 0.7533 - val_loss: 0.8839 - val_RMSE: 0.7707\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8682 - RMSE: 0.7508 - val_loss: 0.8836 - val_RMSE: 0.7707\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8802 - RMSE: 0.7632 - val_loss: 0.8834 - val_RMSE: 0.7707\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8668 - RMSE: 0.7499 - val_loss: 0.8831 - val_RMSE: 0.7706\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8646 - RMSE: 0.7480 - val_loss: 0.8828 - val_RMSE: 0.7706\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8713 - RMSE: 0.7549 - val_loss: 0.8826 - val_RMSE: 0.7706\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8770 - RMSE: 0.7609 - val_loss: 0.8823 - val_RMSE: 0.7706\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8665 - RMSE: 0.7506 - val_loss: 0.8820 - val_RMSE: 0.7705\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8663 - RMSE: 0.7507 - val_loss: 0.8818 - val_RMSE: 0.7705\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8689 - RMSE: 0.7535 - val_loss: 0.8815 - val_RMSE: 0.7705\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8724 - RMSE: 0.7573 - val_loss: 0.8813 - val_RMSE: 0.7704\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8720 - RMSE: 0.7571 - val_loss: 0.8810 - val_RMSE: 0.7704\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8675 - RMSE: 0.7528 - val_loss: 0.8807 - val_RMSE: 0.7704\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8639 - RMSE: 0.7495 - val_loss: 0.8805 - val_RMSE: 0.7704\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8648 - RMSE: 0.7506 - val_loss: 0.8802 - val_RMSE: 0.7703\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8649 - RMSE: 0.7509 - val_loss: 0.8800 - val_RMSE: 0.7703\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8702 - RMSE: 0.7564 - val_loss: 0.8797 - val_RMSE: 0.7703\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8674 - RMSE: 0.7539 - val_loss: 0.8795 - val_RMSE: 0.7702\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8615 - RMSE: 0.7483 - val_loss: 0.8792 - val_RMSE: 0.7702\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8723 - RMSE: 0.7592 - val_loss: 0.8789 - val_RMSE: 0.7702\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8721 - RMSE: 0.7593 - val_loss: 0.8787 - val_RMSE: 0.7702\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8675 - RMSE: 0.7549 - val_loss: 0.8784 - val_RMSE: 0.7701\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8622 - RMSE: 0.7498 - val_loss: 0.8782 - val_RMSE: 0.7701\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8711 - RMSE: 0.7590 - val_loss: 0.8779 - val_RMSE: 0.7701\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8707 - RMSE: 0.7588 - val_loss: 0.8777 - val_RMSE: 0.7701\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8606 - RMSE: 0.7490 - val_loss: 0.8774 - val_RMSE: 0.7700\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8537 - RMSE: 0.7423 - val_loss: 0.8772 - val_RMSE: 0.7700\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8707 - RMSE: 0.7595 - val_loss: 0.8769 - val_RMSE: 0.7700\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8616 - RMSE: 0.7507 - val_loss: 0.8767 - val_RMSE: 0.7699\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8653 - RMSE: 0.7546 - val_loss: 0.8764 - val_RMSE: 0.7699\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8648 - RMSE: 0.7543 - val_loss: 0.8762 - val_RMSE: 0.7699\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8723 - RMSE: 0.7620 - val_loss: 0.8759 - val_RMSE: 0.7699\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8645 - RMSE: 0.7545 - val_loss: 0.8757 - val_RMSE: 0.7698\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8651 - RMSE: 0.7553 - val_loss: 0.8754 - val_RMSE: 0.7698\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8677 - RMSE: 0.7580 - val_loss: 0.8752 - val_RMSE: 0.7698\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8704 - RMSE: 0.7610 - val_loss: 0.8749 - val_RMSE: 0.7698\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8568 - RMSE: 0.7476 - val_loss: 0.8747 - val_RMSE: 0.7697\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8620 - RMSE: 0.7531 - val_loss: 0.8744 - val_RMSE: 0.7697\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8578 - RMSE: 0.7490 - val_loss: 0.8742 - val_RMSE: 0.7697\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8650 - RMSE: 0.7565 - val_loss: 0.8740 - val_RMSE: 0.7697\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8596 - RMSE: 0.7513 - val_loss: 0.8737 - val_RMSE: 0.7696\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8643 - RMSE: 0.7563 - val_loss: 0.8735 - val_RMSE: 0.7696\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8665 - RMSE: 0.7587 - val_loss: 0.8732 - val_RMSE: 0.7696\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8602 - RMSE: 0.7526 - val_loss: 0.8730 - val_RMSE: 0.7696\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8614 - RMSE: 0.7540 - val_loss: 0.8728 - val_RMSE: 0.7695\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8565 - RMSE: 0.7493 - val_loss: 0.8725 - val_RMSE: 0.7695\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8581 - RMSE: 0.7511 - val_loss: 0.8723 - val_RMSE: 0.7695\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8588 - RMSE: 0.7520 - val_loss: 0.8720 - val_RMSE: 0.7695\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8610 - RMSE: 0.7545 - val_loss: 0.8718 - val_RMSE: 0.7694\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8625 - RMSE: 0.7562 - val_loss: 0.8716 - val_RMSE: 0.7694\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8537 - RMSE: 0.7476 - val_loss: 0.8713 - val_RMSE: 0.7694\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8593 - RMSE: 0.7534 - val_loss: 0.8711 - val_RMSE: 0.7694\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8524 - RMSE: 0.7467 - val_loss: 0.8709 - val_RMSE: 0.7693\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8661 - RMSE: 0.7606 - val_loss: 0.8706 - val_RMSE: 0.7693\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8597 - RMSE: 0.7545 - val_loss: 0.8704 - val_RMSE: 0.7693\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8498 - RMSE: 0.7448 - val_loss: 0.8702 - val_RMSE: 0.7693\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8531 - RMSE: 0.7483 - val_loss: 0.8699 - val_RMSE: 0.7692\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8604 - RMSE: 0.7558 - val_loss: 0.8697 - val_RMSE: 0.7692\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8503 - RMSE: 0.7459 - val_loss: 0.8695 - val_RMSE: 0.7692\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8548 - RMSE: 0.7507 - val_loss: 0.8692 - val_RMSE: 0.7692\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8646 - RMSE: 0.7606 - val_loss: 0.8690 - val_RMSE: 0.7691\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8629 - RMSE: 0.7591 - val_loss: 0.8688 - val_RMSE: 0.7691\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8577 - RMSE: 0.7542 - val_loss: 0.8686 - val_RMSE: 0.7691\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8568 - RMSE: 0.7535 - val_loss: 0.8683 - val_RMSE: 0.7691\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8560 - RMSE: 0.7529 - val_loss: 0.8681 - val_RMSE: 0.7690\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8615 - RMSE: 0.7586 - val_loss: 0.8679 - val_RMSE: 0.7690\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8548 - RMSE: 0.7521 - val_loss: 0.8676 - val_RMSE: 0.7690\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8559 - RMSE: 0.7534 - val_loss: 0.8674 - val_RMSE: 0.7690\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8577 - RMSE: 0.7554 - val_loss: 0.8672 - val_RMSE: 0.7690\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8514 - RMSE: 0.7493 - val_loss: 0.8670 - val_RMSE: 0.7689\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8589 - RMSE: 0.7570 - val_loss: 0.8667 - val_RMSE: 0.7689\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8504 - RMSE: 0.7487 - val_loss: 0.8665 - val_RMSE: 0.7689\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8459 - RMSE: 0.7444 - val_loss: 0.8663 - val_RMSE: 0.7689\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8563 - RMSE: 0.7551 - val_loss: 0.8661 - val_RMSE: 0.7688\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8592 - RMSE: 0.7581 - val_loss: 0.8659 - val_RMSE: 0.7688\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8541 - RMSE: 0.7532 - val_loss: 0.8656 - val_RMSE: 0.7688\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8498 - RMSE: 0.7492 - val_loss: 0.8654 - val_RMSE: 0.7688\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8491 - RMSE: 0.7486 - val_loss: 0.8652 - val_RMSE: 0.7688\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8546 - RMSE: 0.7543 - val_loss: 0.8650 - val_RMSE: 0.7687\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8511 - RMSE: 0.7510 - val_loss: 0.8648 - val_RMSE: 0.7687\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8564 - RMSE: 0.7565 - val_loss: 0.8645 - val_RMSE: 0.7687\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8471 - RMSE: 0.7474 - val_loss: 0.8643 - val_RMSE: 0.7687\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8656 - RMSE: 0.7662 - val_loss: 0.8641 - val_RMSE: 0.7686\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8556 - RMSE: 0.7564 - val_loss: 0.8639 - val_RMSE: 0.7686\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8521 - RMSE: 0.7530 - val_loss: 0.8637 - val_RMSE: 0.7686\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8538 - RMSE: 0.7550 - val_loss: 0.8635 - val_RMSE: 0.7686\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8543 - RMSE: 0.7557 - val_loss: 0.8632 - val_RMSE: 0.7686\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8488 - RMSE: 0.7503 - val_loss: 0.8630 - val_RMSE: 0.7685\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8540 - RMSE: 0.7558 - val_loss: 0.8628 - val_RMSE: 0.7685\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8540 - RMSE: 0.7559 - val_loss: 0.8626 - val_RMSE: 0.7685\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8567 - RMSE: 0.7588 - val_loss: 0.8624 - val_RMSE: 0.7685\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8522 - RMSE: 0.7545 - val_loss: 0.8622 - val_RMSE: 0.7685\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8459 - RMSE: 0.7484 - val_loss: 0.8620 - val_RMSE: 0.7684\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8623 - RMSE: 0.7650 - val_loss: 0.8618 - val_RMSE: 0.7684\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8531 - RMSE: 0.7560 - val_loss: 0.8616 - val_RMSE: 0.7684\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8511 - RMSE: 0.7542 - val_loss: 0.8613 - val_RMSE: 0.7684\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8509 - RMSE: 0.7542 - val_loss: 0.8611 - val_RMSE: 0.7684\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8429 - RMSE: 0.7464 - val_loss: 0.8609 - val_RMSE: 0.7683\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8528 - RMSE: 0.7565 - val_loss: 0.8607 - val_RMSE: 0.7683\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8470 - RMSE: 0.7509 - val_loss: 0.8605 - val_RMSE: 0.7683\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8485 - RMSE: 0.7526 - val_loss: 0.8603 - val_RMSE: 0.7683\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8470 - RMSE: 0.7513 - val_loss: 0.8601 - val_RMSE: 0.7683\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8548 - RMSE: 0.7593 - val_loss: 0.8599 - val_RMSE: 0.7682\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8544 - RMSE: 0.7590 - val_loss: 0.8597 - val_RMSE: 0.7682\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8455 - RMSE: 0.7503 - val_loss: 0.8595 - val_RMSE: 0.7682\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8443 - RMSE: 0.7493 - val_loss: 0.8593 - val_RMSE: 0.7682\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8327 - RMSE: 0.7379 - val_loss: 0.8591 - val_RMSE: 0.7682\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8513 - RMSE: 0.7567 - val_loss: 0.8589 - val_RMSE: 0.7681\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8499 - RMSE: 0.7555 - val_loss: 0.8587 - val_RMSE: 0.7681\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8489 - RMSE: 0.7547 - val_loss: 0.8585 - val_RMSE: 0.7681\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8450 - RMSE: 0.7510 - val_loss: 0.8583 - val_RMSE: 0.7681\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8562 - RMSE: 0.7623 - val_loss: 0.8581 - val_RMSE: 0.7681\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8463 - RMSE: 0.7527 - val_loss: 0.8579 - val_RMSE: 0.7680\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8474 - RMSE: 0.7539 - val_loss: 0.8576 - val_RMSE: 0.7680\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8416 - RMSE: 0.7483 - val_loss: 0.8575 - val_RMSE: 0.7680\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8360 - RMSE: 0.7429 - val_loss: 0.8573 - val_RMSE: 0.7680\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8526 - RMSE: 0.7597 - val_loss: 0.8571 - val_RMSE: 0.7680\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8425 - RMSE: 0.7497 - val_loss: 0.8569 - val_RMSE: 0.7679\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8444 - RMSE: 0.7518 - val_loss: 0.8567 - val_RMSE: 0.7679\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8415 - RMSE: 0.7491 - val_loss: 0.8565 - val_RMSE: 0.7679\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8407 - RMSE: 0.7485 - val_loss: 0.8563 - val_RMSE: 0.7679\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8482 - RMSE: 0.7561 - val_loss: 0.8561 - val_RMSE: 0.7679\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8353 - RMSE: 0.7434 - val_loss: 0.8559 - val_RMSE: 0.7679\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8447 - RMSE: 0.7531 - val_loss: 0.8557 - val_RMSE: 0.7678\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8387 - RMSE: 0.7472 - val_loss: 0.8555 - val_RMSE: 0.7678\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8361 - RMSE: 0.7448 - val_loss: 0.8553 - val_RMSE: 0.7678\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8507 - RMSE: 0.7596 - val_loss: 0.8551 - val_RMSE: 0.7678\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8484 - RMSE: 0.7575 - val_loss: 0.8549 - val_RMSE: 0.7678\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8397 - RMSE: 0.7489 - val_loss: 0.8547 - val_RMSE: 0.7678\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8438 - RMSE: 0.7532 - val_loss: 0.8545 - val_RMSE: 0.7677\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8467 - RMSE: 0.7563 - val_loss: 0.8543 - val_RMSE: 0.7677\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8449 - RMSE: 0.7547 - val_loss: 0.8541 - val_RMSE: 0.7677\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8404 - RMSE: 0.7504 - val_loss: 0.8539 - val_RMSE: 0.7677\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8424 - RMSE: 0.7525 - val_loss: 0.8538 - val_RMSE: 0.7677\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8337 - RMSE: 0.7440 - val_loss: 0.8536 - val_RMSE: 0.7676\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8439 - RMSE: 0.7544 - val_loss: 0.8534 - val_RMSE: 0.7676\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8445 - RMSE: 0.7552 - val_loss: 0.8532 - val_RMSE: 0.7676\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8414 - RMSE: 0.7522 - val_loss: 0.8530 - val_RMSE: 0.7676\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8375 - RMSE: 0.7486 - val_loss: 0.8528 - val_RMSE: 0.7676\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8376 - RMSE: 0.7488 - val_loss: 0.8526 - val_RMSE: 0.7676\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8360 - RMSE: 0.7473 - val_loss: 0.8524 - val_RMSE: 0.7675\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8478 - RMSE: 0.7593 - val_loss: 0.8523 - val_RMSE: 0.7675\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8433 - RMSE: 0.7550 - val_loss: 0.8521 - val_RMSE: 0.7675\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8313 - RMSE: 0.7432 - val_loss: 0.8519 - val_RMSE: 0.7675\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8495 - RMSE: 0.7615 - val_loss: 0.8517 - val_RMSE: 0.7675\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8424 - RMSE: 0.7546 - val_loss: 0.8515 - val_RMSE: 0.7675\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8346 - RMSE: 0.7470 - val_loss: 0.8513 - val_RMSE: 0.7674\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8408 - RMSE: 0.7534 - val_loss: 0.8511 - val_RMSE: 0.7674\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8421 - RMSE: 0.7548 - val_loss: 0.8510 - val_RMSE: 0.7674\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8319 - RMSE: 0.7448 - val_loss: 0.8508 - val_RMSE: 0.7674\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8289 - RMSE: 0.7419 - val_loss: 0.8506 - val_RMSE: 0.7674\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8294 - RMSE: 0.7427 - val_loss: 0.8504 - val_RMSE: 0.7674\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8422 - RMSE: 0.7556 - val_loss: 0.8502 - val_RMSE: 0.7673\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8226 - RMSE: 0.7361 - val_loss: 0.8501 - val_RMSE: 0.7673\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8354 - RMSE: 0.7491 - val_loss: 0.8499 - val_RMSE: 0.7673\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8522 - RMSE: 0.7661 - val_loss: 0.8497 - val_RMSE: 0.7673\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8378 - RMSE: 0.7519 - val_loss: 0.8495 - val_RMSE: 0.7673\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8473 - RMSE: 0.7615 - val_loss: 0.8493 - val_RMSE: 0.7673\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8355 - RMSE: 0.7499 - val_loss: 0.8492 - val_RMSE: 0.7672\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8281 - RMSE: 0.7427 - val_loss: 0.8490 - val_RMSE: 0.7672\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8281 - RMSE: 0.7428 - val_loss: 0.8488 - val_RMSE: 0.7672\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8374 - RMSE: 0.7523 - val_loss: 0.8486 - val_RMSE: 0.7672\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8413 - RMSE: 0.7564 - val_loss: 0.8484 - val_RMSE: 0.7672\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8333 - RMSE: 0.7486 - val_loss: 0.8483 - val_RMSE: 0.7672\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8325 - RMSE: 0.7479 - val_loss: 0.8481 - val_RMSE: 0.7672\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8470 - RMSE: 0.7625 - val_loss: 0.8479 - val_RMSE: 0.7671\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8375 - RMSE: 0.7533 - val_loss: 0.8477 - val_RMSE: 0.7671\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8251 - RMSE: 0.7410 - val_loss: 0.8476 - val_RMSE: 0.7671\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8360 - RMSE: 0.7521 - val_loss: 0.8474 - val_RMSE: 0.7671\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8287 - RMSE: 0.7450 - val_loss: 0.8472 - val_RMSE: 0.7671\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8313 - RMSE: 0.7476 - val_loss: 0.8471 - val_RMSE: 0.7671\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8399 - RMSE: 0.7564 - val_loss: 0.8469 - val_RMSE: 0.7670\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8406 - RMSE: 0.7573 - val_loss: 0.8467 - val_RMSE: 0.7670\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8223 - RMSE: 0.7391 - val_loss: 0.8465 - val_RMSE: 0.7670\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8273 - RMSE: 0.7444 - val_loss: 0.8464 - val_RMSE: 0.7670\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8353 - RMSE: 0.7524 - val_loss: 0.8462 - val_RMSE: 0.7670\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8321 - RMSE: 0.7494 - val_loss: 0.8460 - val_RMSE: 0.7670\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8276 - RMSE: 0.7451 - val_loss: 0.8458 - val_RMSE: 0.7670\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8329 - RMSE: 0.7505 - val_loss: 0.8457 - val_RMSE: 0.7669\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8376 - RMSE: 0.7554 - val_loss: 0.8455 - val_RMSE: 0.7669\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8373 - RMSE: 0.7552 - val_loss: 0.8453 - val_RMSE: 0.7669\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8366 - RMSE: 0.7547 - val_loss: 0.8452 - val_RMSE: 0.7669\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8348 - RMSE: 0.7531 - val_loss: 0.8450 - val_RMSE: 0.7669\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8440 - RMSE: 0.7624 - val_loss: 0.8448 - val_RMSE: 0.7669\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8392 - RMSE: 0.7578 - val_loss: 0.8447 - val_RMSE: 0.7669\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8284 - RMSE: 0.7471 - val_loss: 0.8445 - val_RMSE: 0.7668\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8282 - RMSE: 0.7471 - val_loss: 0.8443 - val_RMSE: 0.7668\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8254 - RMSE: 0.7444 - val_loss: 0.8442 - val_RMSE: 0.7668\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8380 - RMSE: 0.7572 - val_loss: 0.8440 - val_RMSE: 0.7668\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8332 - RMSE: 0.7525 - val_loss: 0.8438 - val_RMSE: 0.7668\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8319 - RMSE: 0.7514 - val_loss: 0.8437 - val_RMSE: 0.7668\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8351 - RMSE: 0.7547 - val_loss: 0.8435 - val_RMSE: 0.7668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=15 / Init = GlorotNormal / min_loss = 0.7667545676231384\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotUniform\n",
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_35 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_36 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_68 (Embedding)        (None, 1, 10)        290         input_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_69 (Embedding)        (None, 1, 10)        1620        input_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_17 (Dot)                    (None, 1, 1)         0           embedding_68[0][0]               \n",
            "                                                                 embedding_69[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_70 (Embedding)        (None, 1, 1)         29          input_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_71 (Embedding)        (None, 1, 1)         162         input_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 1, 1)         0           dot_17[0][0]                     \n",
            "                                                                 embedding_70[0][0]               \n",
            "                                                                 embedding_71[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_17 (Flatten)            (None, 1)            0           add_17[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,101\n",
            "Trainable params: 2,101\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.0426 - RMSE: 0.7888 - val_loss: 0.8685 - val_RMSE: 0.7674\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8536 - RMSE: 0.7485 - val_loss: 0.8665 - val_RMSE: 0.7672\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8578 - RMSE: 0.7536 - val_loss: 0.8663 - val_RMSE: 0.7672\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8505 - RMSE: 0.7466 - val_loss: 0.8660 - val_RMSE: 0.7672\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8649 - RMSE: 0.7612 - val_loss: 0.8658 - val_RMSE: 0.7672\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8592 - RMSE: 0.7557 - val_loss: 0.8656 - val_RMSE: 0.7672\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8573 - RMSE: 0.7540 - val_loss: 0.8653 - val_RMSE: 0.7671\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8607 - RMSE: 0.7576 - val_loss: 0.8651 - val_RMSE: 0.7671\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8624 - RMSE: 0.7596 - val_loss: 0.8649 - val_RMSE: 0.7671\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8627 - RMSE: 0.7600 - val_loss: 0.8647 - val_RMSE: 0.7671\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8627 - RMSE: 0.7603 - val_loss: 0.8644 - val_RMSE: 0.7671\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8561 - RMSE: 0.7539 - val_loss: 0.8642 - val_RMSE: 0.7671\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8607 - RMSE: 0.7586 - val_loss: 0.8640 - val_RMSE: 0.7670\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8633 - RMSE: 0.7615 - val_loss: 0.8638 - val_RMSE: 0.7670\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8524 - RMSE: 0.7508 - val_loss: 0.8636 - val_RMSE: 0.7670\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8552 - RMSE: 0.7538 - val_loss: 0.8633 - val_RMSE: 0.7670\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8654 - RMSE: 0.7642 - val_loss: 0.8631 - val_RMSE: 0.7670\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8438 - RMSE: 0.7428 - val_loss: 0.8629 - val_RMSE: 0.7670\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8506 - RMSE: 0.7499 - val_loss: 0.8627 - val_RMSE: 0.7669\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8442 - RMSE: 0.7437 - val_loss: 0.8625 - val_RMSE: 0.7669\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8521 - RMSE: 0.7518 - val_loss: 0.8622 - val_RMSE: 0.7669\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8527 - RMSE: 0.7526 - val_loss: 0.8620 - val_RMSE: 0.7669\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8568 - RMSE: 0.7569 - val_loss: 0.8618 - val_RMSE: 0.7669\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8535 - RMSE: 0.7538 - val_loss: 0.8616 - val_RMSE: 0.7668\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8541 - RMSE: 0.7546 - val_loss: 0.8614 - val_RMSE: 0.7668\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8628 - RMSE: 0.7635 - val_loss: 0.8612 - val_RMSE: 0.7668\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8636 - RMSE: 0.7645 - val_loss: 0.8609 - val_RMSE: 0.7668\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8584 - RMSE: 0.7595 - val_loss: 0.8607 - val_RMSE: 0.7668\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8451 - RMSE: 0.7464 - val_loss: 0.8605 - val_RMSE: 0.7668\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8639 - RMSE: 0.7654 - val_loss: 0.8603 - val_RMSE: 0.7668\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8460 - RMSE: 0.7477 - val_loss: 0.8601 - val_RMSE: 0.7667\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8518 - RMSE: 0.7538 - val_loss: 0.8599 - val_RMSE: 0.7667\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8506 - RMSE: 0.7527 - val_loss: 0.8597 - val_RMSE: 0.7667\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8509 - RMSE: 0.7532 - val_loss: 0.8595 - val_RMSE: 0.7667\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8617 - RMSE: 0.7642 - val_loss: 0.8592 - val_RMSE: 0.7667\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8485 - RMSE: 0.7512 - val_loss: 0.8590 - val_RMSE: 0.7667\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8533 - RMSE: 0.7562 - val_loss: 0.8588 - val_RMSE: 0.7666\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8488 - RMSE: 0.7519 - val_loss: 0.8586 - val_RMSE: 0.7666\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8508 - RMSE: 0.7541 - val_loss: 0.8584 - val_RMSE: 0.7666\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8604 - RMSE: 0.7640 - val_loss: 0.8582 - val_RMSE: 0.7666\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8416 - RMSE: 0.7453 - val_loss: 0.8580 - val_RMSE: 0.7666\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8504 - RMSE: 0.7543 - val_loss: 0.8578 - val_RMSE: 0.7666\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8463 - RMSE: 0.7504 - val_loss: 0.8576 - val_RMSE: 0.7665\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8567 - RMSE: 0.7610 - val_loss: 0.8574 - val_RMSE: 0.7665\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8437 - RMSE: 0.7482 - val_loss: 0.8572 - val_RMSE: 0.7665\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8519 - RMSE: 0.7566 - val_loss: 0.8570 - val_RMSE: 0.7665\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8479 - RMSE: 0.7528 - val_loss: 0.8568 - val_RMSE: 0.7665\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8494 - RMSE: 0.7545 - val_loss: 0.8566 - val_RMSE: 0.7665\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8504 - RMSE: 0.7557 - val_loss: 0.8564 - val_RMSE: 0.7665\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8435 - RMSE: 0.7489 - val_loss: 0.8562 - val_RMSE: 0.7664\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8510 - RMSE: 0.7567 - val_loss: 0.8560 - val_RMSE: 0.7664\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8566 - RMSE: 0.7624 - val_loss: 0.8558 - val_RMSE: 0.7664\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8471 - RMSE: 0.7532 - val_loss: 0.8556 - val_RMSE: 0.7664\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8400 - RMSE: 0.7462 - val_loss: 0.8554 - val_RMSE: 0.7664\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8578 - RMSE: 0.7643 - val_loss: 0.8552 - val_RMSE: 0.7664\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8396 - RMSE: 0.7463 - val_loss: 0.8550 - val_RMSE: 0.7663\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8469 - RMSE: 0.7537 - val_loss: 0.8548 - val_RMSE: 0.7663\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8476 - RMSE: 0.7546 - val_loss: 0.8546 - val_RMSE: 0.7663\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8434 - RMSE: 0.7506 - val_loss: 0.8544 - val_RMSE: 0.7663\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8466 - RMSE: 0.7540 - val_loss: 0.8542 - val_RMSE: 0.7663\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8461 - RMSE: 0.7537 - val_loss: 0.8540 - val_RMSE: 0.7663\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8486 - RMSE: 0.7563 - val_loss: 0.8538 - val_RMSE: 0.7663\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8414 - RMSE: 0.7493 - val_loss: 0.8536 - val_RMSE: 0.7663\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8439 - RMSE: 0.7520 - val_loss: 0.8534 - val_RMSE: 0.7662\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8449 - RMSE: 0.7532 - val_loss: 0.8532 - val_RMSE: 0.7662\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8460 - RMSE: 0.7545 - val_loss: 0.8530 - val_RMSE: 0.7662\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8396 - RMSE: 0.7483 - val_loss: 0.8528 - val_RMSE: 0.7662\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8514 - RMSE: 0.7603 - val_loss: 0.8526 - val_RMSE: 0.7662\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8444 - RMSE: 0.7534 - val_loss: 0.8524 - val_RMSE: 0.7662\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8351 - RMSE: 0.7443 - val_loss: 0.8522 - val_RMSE: 0.7662\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8464 - RMSE: 0.7558 - val_loss: 0.8521 - val_RMSE: 0.7662\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8413 - RMSE: 0.7509 - val_loss: 0.8519 - val_RMSE: 0.7661\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8463 - RMSE: 0.7560 - val_loss: 0.8517 - val_RMSE: 0.7661\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8431 - RMSE: 0.7531 - val_loss: 0.8515 - val_RMSE: 0.7661\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8416 - RMSE: 0.7517 - val_loss: 0.8513 - val_RMSE: 0.7661\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8465 - RMSE: 0.7568 - val_loss: 0.8511 - val_RMSE: 0.7661\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8487 - RMSE: 0.7592 - val_loss: 0.8509 - val_RMSE: 0.7661\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8348 - RMSE: 0.7455 - val_loss: 0.8507 - val_RMSE: 0.7661\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8304 - RMSE: 0.7412 - val_loss: 0.8506 - val_RMSE: 0.7661\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8468 - RMSE: 0.7579 - val_loss: 0.8504 - val_RMSE: 0.7660\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8401 - RMSE: 0.7513 - val_loss: 0.8502 - val_RMSE: 0.7660\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8403 - RMSE: 0.7517 - val_loss: 0.8500 - val_RMSE: 0.7660\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8378 - RMSE: 0.7494 - val_loss: 0.8498 - val_RMSE: 0.7660\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8451 - RMSE: 0.7568 - val_loss: 0.8496 - val_RMSE: 0.7660\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8432 - RMSE: 0.7552 - val_loss: 0.8494 - val_RMSE: 0.7660\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8453 - RMSE: 0.7574 - val_loss: 0.8493 - val_RMSE: 0.7660\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8474 - RMSE: 0.7597 - val_loss: 0.8491 - val_RMSE: 0.7659\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8396 - RMSE: 0.7521 - val_loss: 0.8489 - val_RMSE: 0.7659\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8355 - RMSE: 0.7482 - val_loss: 0.8487 - val_RMSE: 0.7659\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8406 - RMSE: 0.7535 - val_loss: 0.8485 - val_RMSE: 0.7659\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8392 - RMSE: 0.7522 - val_loss: 0.8484 - val_RMSE: 0.7659\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8373 - RMSE: 0.7505 - val_loss: 0.8482 - val_RMSE: 0.7659\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8368 - RMSE: 0.7501 - val_loss: 0.8480 - val_RMSE: 0.7659\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8307 - RMSE: 0.7443 - val_loss: 0.8478 - val_RMSE: 0.7659\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8395 - RMSE: 0.7531 - val_loss: 0.8476 - val_RMSE: 0.7659\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8388 - RMSE: 0.7527 - val_loss: 0.8475 - val_RMSE: 0.7658\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8430 - RMSE: 0.7571 - val_loss: 0.8473 - val_RMSE: 0.7658\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8355 - RMSE: 0.7498 - val_loss: 0.8471 - val_RMSE: 0.7658\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8355 - RMSE: 0.7499 - val_loss: 0.8469 - val_RMSE: 0.7658\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8375 - RMSE: 0.7521 - val_loss: 0.8467 - val_RMSE: 0.7658\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8411 - RMSE: 0.7558 - val_loss: 0.8466 - val_RMSE: 0.7658\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8375 - RMSE: 0.7524 - val_loss: 0.8464 - val_RMSE: 0.7658\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8398 - RMSE: 0.7549 - val_loss: 0.8462 - val_RMSE: 0.7658\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8314 - RMSE: 0.7466 - val_loss: 0.8460 - val_RMSE: 0.7657\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8363 - RMSE: 0.7517 - val_loss: 0.8459 - val_RMSE: 0.7657\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8299 - RMSE: 0.7455 - val_loss: 0.8457 - val_RMSE: 0.7657\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8327 - RMSE: 0.7484 - val_loss: 0.8455 - val_RMSE: 0.7657\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8383 - RMSE: 0.7542 - val_loss: 0.8453 - val_RMSE: 0.7657\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8436 - RMSE: 0.7596 - val_loss: 0.8452 - val_RMSE: 0.7657\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8304 - RMSE: 0.7466 - val_loss: 0.8450 - val_RMSE: 0.7657\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8425 - RMSE: 0.7589 - val_loss: 0.8448 - val_RMSE: 0.7657\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8353 - RMSE: 0.7518 - val_loss: 0.8447 - val_RMSE: 0.7657\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8342 - RMSE: 0.7509 - val_loss: 0.8445 - val_RMSE: 0.7656\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8399 - RMSE: 0.7567 - val_loss: 0.8443 - val_RMSE: 0.7656\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8423 - RMSE: 0.7593 - val_loss: 0.8442 - val_RMSE: 0.7656\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8424 - RMSE: 0.7595 - val_loss: 0.8440 - val_RMSE: 0.7656\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8303 - RMSE: 0.7476 - val_loss: 0.8438 - val_RMSE: 0.7656\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8397 - RMSE: 0.7572 - val_loss: 0.8437 - val_RMSE: 0.7656\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8393 - RMSE: 0.7570 - val_loss: 0.8435 - val_RMSE: 0.7656\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8319 - RMSE: 0.7497 - val_loss: 0.8433 - val_RMSE: 0.7656\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8360 - RMSE: 0.7540 - val_loss: 0.8432 - val_RMSE: 0.7656\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8350 - RMSE: 0.7532 - val_loss: 0.8430 - val_RMSE: 0.7656\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8354 - RMSE: 0.7537 - val_loss: 0.8428 - val_RMSE: 0.7655\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8382 - RMSE: 0.7567 - val_loss: 0.8427 - val_RMSE: 0.7655\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8308 - RMSE: 0.7495 - val_loss: 0.8425 - val_RMSE: 0.7655\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8316 - RMSE: 0.7504 - val_loss: 0.8423 - val_RMSE: 0.7655\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8344 - RMSE: 0.7534 - val_loss: 0.8422 - val_RMSE: 0.7655\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8246 - RMSE: 0.7437 - val_loss: 0.8420 - val_RMSE: 0.7655\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8322 - RMSE: 0.7515 - val_loss: 0.8418 - val_RMSE: 0.7655\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8299 - RMSE: 0.7494 - val_loss: 0.8417 - val_RMSE: 0.7655\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8236 - RMSE: 0.7432 - val_loss: 0.8415 - val_RMSE: 0.7655\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8344 - RMSE: 0.7542 - val_loss: 0.8413 - val_RMSE: 0.7655\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8320 - RMSE: 0.7519 - val_loss: 0.8412 - val_RMSE: 0.7654\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8292 - RMSE: 0.7493 - val_loss: 0.8410 - val_RMSE: 0.7654\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8336 - RMSE: 0.7538 - val_loss: 0.8409 - val_RMSE: 0.7654\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8300 - RMSE: 0.7504 - val_loss: 0.8407 - val_RMSE: 0.7654\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8286 - RMSE: 0.7492 - val_loss: 0.8405 - val_RMSE: 0.7654\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8359 - RMSE: 0.7567 - val_loss: 0.8404 - val_RMSE: 0.7654\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8342 - RMSE: 0.7551 - val_loss: 0.8402 - val_RMSE: 0.7654\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8177 - RMSE: 0.7387 - val_loss: 0.8401 - val_RMSE: 0.7654\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8242 - RMSE: 0.7453 - val_loss: 0.8399 - val_RMSE: 0.7654\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8308 - RMSE: 0.7521 - val_loss: 0.8398 - val_RMSE: 0.7654\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8238 - RMSE: 0.7452 - val_loss: 0.8396 - val_RMSE: 0.7654\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8352 - RMSE: 0.7569 - val_loss: 0.8394 - val_RMSE: 0.7653\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8364 - RMSE: 0.7582 - val_loss: 0.8393 - val_RMSE: 0.7653\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8308 - RMSE: 0.7527 - val_loss: 0.8391 - val_RMSE: 0.7653\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8265 - RMSE: 0.7486 - val_loss: 0.8390 - val_RMSE: 0.7653\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8326 - RMSE: 0.7549 - val_loss: 0.8388 - val_RMSE: 0.7653\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8317 - RMSE: 0.7540 - val_loss: 0.8387 - val_RMSE: 0.7653\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8266 - RMSE: 0.7491 - val_loss: 0.8385 - val_RMSE: 0.7653\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8311 - RMSE: 0.7538 - val_loss: 0.8384 - val_RMSE: 0.7653\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8193 - RMSE: 0.7421 - val_loss: 0.8382 - val_RMSE: 0.7653\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8256 - RMSE: 0.7486 - val_loss: 0.8380 - val_RMSE: 0.7653\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8313 - RMSE: 0.7544 - val_loss: 0.8379 - val_RMSE: 0.7652\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8267 - RMSE: 0.7500 - val_loss: 0.8377 - val_RMSE: 0.7652\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8291 - RMSE: 0.7525 - val_loss: 0.8376 - val_RMSE: 0.7652\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8351 - RMSE: 0.7587 - val_loss: 0.8374 - val_RMSE: 0.7652\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8259 - RMSE: 0.7496 - val_loss: 0.8373 - val_RMSE: 0.7652\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8263 - RMSE: 0.7502 - val_loss: 0.8371 - val_RMSE: 0.7652\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8297 - RMSE: 0.7537 - val_loss: 0.8370 - val_RMSE: 0.7652\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8349 - RMSE: 0.7590 - val_loss: 0.8368 - val_RMSE: 0.7652\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8263 - RMSE: 0.7506 - val_loss: 0.8367 - val_RMSE: 0.7652\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8223 - RMSE: 0.7467 - val_loss: 0.8365 - val_RMSE: 0.7652\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8386 - RMSE: 0.7632 - val_loss: 0.8364 - val_RMSE: 0.7652\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8178 - RMSE: 0.7425 - val_loss: 0.8362 - val_RMSE: 0.7652\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8280 - RMSE: 0.7529 - val_loss: 0.8361 - val_RMSE: 0.7651\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8197 - RMSE: 0.7447 - val_loss: 0.8359 - val_RMSE: 0.7651\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8337 - RMSE: 0.7589 - val_loss: 0.8358 - val_RMSE: 0.7651\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8193 - RMSE: 0.7446 - val_loss: 0.8356 - val_RMSE: 0.7651\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8192 - RMSE: 0.7447 - val_loss: 0.8355 - val_RMSE: 0.7651\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8226 - RMSE: 0.7482 - val_loss: 0.8353 - val_RMSE: 0.7651\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8265 - RMSE: 0.7523 - val_loss: 0.8352 - val_RMSE: 0.7651\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8276 - RMSE: 0.7535 - val_loss: 0.8351 - val_RMSE: 0.7651\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8299 - RMSE: 0.7559 - val_loss: 0.8349 - val_RMSE: 0.7651\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8273 - RMSE: 0.7535 - val_loss: 0.8348 - val_RMSE: 0.7651\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8296 - RMSE: 0.7560 - val_loss: 0.8346 - val_RMSE: 0.7651\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8284 - RMSE: 0.7549 - val_loss: 0.8345 - val_RMSE: 0.7651\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8249 - RMSE: 0.7515 - val_loss: 0.8343 - val_RMSE: 0.7650\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8373 - RMSE: 0.7640 - val_loss: 0.8342 - val_RMSE: 0.7650\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8230 - RMSE: 0.7499 - val_loss: 0.8341 - val_RMSE: 0.7650\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8307 - RMSE: 0.7577 - val_loss: 0.8339 - val_RMSE: 0.7650\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8255 - RMSE: 0.7526 - val_loss: 0.8338 - val_RMSE: 0.7650\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8276 - RMSE: 0.7548 - val_loss: 0.8336 - val_RMSE: 0.7650\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8320 - RMSE: 0.7594 - val_loss: 0.8335 - val_RMSE: 0.7650\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8194 - RMSE: 0.7470 - val_loss: 0.8333 - val_RMSE: 0.7650\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8231 - RMSE: 0.7508 - val_loss: 0.8332 - val_RMSE: 0.7650\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8231 - RMSE: 0.7509 - val_loss: 0.8331 - val_RMSE: 0.7650\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8307 - RMSE: 0.7587 - val_loss: 0.8329 - val_RMSE: 0.7650\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8332 - RMSE: 0.7612 - val_loss: 0.8328 - val_RMSE: 0.7650\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8195 - RMSE: 0.7478 - val_loss: 0.8326 - val_RMSE: 0.7650\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8295 - RMSE: 0.7579 - val_loss: 0.8325 - val_RMSE: 0.7649\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8190 - RMSE: 0.7475 - val_loss: 0.8324 - val_RMSE: 0.7649\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8155 - RMSE: 0.7441 - val_loss: 0.8322 - val_RMSE: 0.7649\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8247 - RMSE: 0.7535 - val_loss: 0.8321 - val_RMSE: 0.7649\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8190 - RMSE: 0.7479 - val_loss: 0.8319 - val_RMSE: 0.7649\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8240 - RMSE: 0.7531 - val_loss: 0.8318 - val_RMSE: 0.7649\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8186 - RMSE: 0.7477 - val_loss: 0.8317 - val_RMSE: 0.7649\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8196 - RMSE: 0.7490 - val_loss: 0.8315 - val_RMSE: 0.7649\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8237 - RMSE: 0.7532 - val_loss: 0.8314 - val_RMSE: 0.7649\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8260 - RMSE: 0.7555 - val_loss: 0.8313 - val_RMSE: 0.7649\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8312 - RMSE: 0.7609 - val_loss: 0.8311 - val_RMSE: 0.7649\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8187 - RMSE: 0.7485 - val_loss: 0.8310 - val_RMSE: 0.7649\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8214 - RMSE: 0.7514 - val_loss: 0.8309 - val_RMSE: 0.7649\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8197 - RMSE: 0.7498 - val_loss: 0.8307 - val_RMSE: 0.7648\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8233 - RMSE: 0.7535 - val_loss: 0.8306 - val_RMSE: 0.7648\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8157 - RMSE: 0.7460 - val_loss: 0.8305 - val_RMSE: 0.7648\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8210 - RMSE: 0.7515 - val_loss: 0.8303 - val_RMSE: 0.7648\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8262 - RMSE: 0.7568 - val_loss: 0.8302 - val_RMSE: 0.7648\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8312 - RMSE: 0.7619 - val_loss: 0.8301 - val_RMSE: 0.7648\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.8208 - RMSE: 0.7517 - val_loss: 0.8299 - val_RMSE: 0.7648\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.8207 - RMSE: 0.7517 - val_loss: 0.8298 - val_RMSE: 0.7648\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8167 - RMSE: 0.7478 - val_loss: 0.8297 - val_RMSE: 0.7648\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8164 - RMSE: 0.7477 - val_loss: 0.8295 - val_RMSE: 0.7648\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8184 - RMSE: 0.7498 - val_loss: 0.8294 - val_RMSE: 0.7648\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8188 - RMSE: 0.7503 - val_loss: 0.8293 - val_RMSE: 0.7648\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8215 - RMSE: 0.7531 - val_loss: 0.8291 - val_RMSE: 0.7648\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8211 - RMSE: 0.7528 - val_loss: 0.8290 - val_RMSE: 0.7648\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8211 - RMSE: 0.7530 - val_loss: 0.8289 - val_RMSE: 0.7648\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8097 - RMSE: 0.7417 - val_loss: 0.8288 - val_RMSE: 0.7647\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8118 - RMSE: 0.7439 - val_loss: 0.8286 - val_RMSE: 0.7647\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8141 - RMSE: 0.7464 - val_loss: 0.8285 - val_RMSE: 0.7647\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8130 - RMSE: 0.7454 - val_loss: 0.8284 - val_RMSE: 0.7647\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8203 - RMSE: 0.7528 - val_loss: 0.8282 - val_RMSE: 0.7647\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.8228 - RMSE: 0.7554 - val_loss: 0.8281 - val_RMSE: 0.7647\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.8102 - RMSE: 0.7430 - val_loss: 0.8280 - val_RMSE: 0.7647\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 5ms/step - loss: 0.8156 - RMSE: 0.7485 - val_loss: 0.8279 - val_RMSE: 0.7647\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8127 - RMSE: 0.7457 - val_loss: 0.8277 - val_RMSE: 0.7647\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8100 - RMSE: 0.7432 - val_loss: 0.8276 - val_RMSE: 0.7647\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8152 - RMSE: 0.7485 - val_loss: 0.8275 - val_RMSE: 0.7647\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8162 - RMSE: 0.7496 - val_loss: 0.8274 - val_RMSE: 0.7647\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8076 - RMSE: 0.7412 - val_loss: 0.8272 - val_RMSE: 0.7647\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8151 - RMSE: 0.7487 - val_loss: 0.8271 - val_RMSE: 0.7647\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8153 - RMSE: 0.7490 - val_loss: 0.8270 - val_RMSE: 0.7647\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8082 - RMSE: 0.7420 - val_loss: 0.8269 - val_RMSE: 0.7646\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8183 - RMSE: 0.7523 - val_loss: 0.8267 - val_RMSE: 0.7646\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8191 - RMSE: 0.7533 - val_loss: 0.8266 - val_RMSE: 0.7646\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8234 - RMSE: 0.7576 - val_loss: 0.8265 - val_RMSE: 0.7646\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8083 - RMSE: 0.7427 - val_loss: 0.8264 - val_RMSE: 0.7646\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8236 - RMSE: 0.7580 - val_loss: 0.8262 - val_RMSE: 0.7646\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8177 - RMSE: 0.7523 - val_loss: 0.8261 - val_RMSE: 0.7646\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8104 - RMSE: 0.7451 - val_loss: 0.8260 - val_RMSE: 0.7646\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8151 - RMSE: 0.7499 - val_loss: 0.8259 - val_RMSE: 0.7646\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8161 - RMSE: 0.7510 - val_loss: 0.8257 - val_RMSE: 0.7646\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8274 - RMSE: 0.7624 - val_loss: 0.8256 - val_RMSE: 0.7646\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8184 - RMSE: 0.7536 - val_loss: 0.8255 - val_RMSE: 0.7646\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8161 - RMSE: 0.7514 - val_loss: 0.8254 - val_RMSE: 0.7646\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8227 - RMSE: 0.7581 - val_loss: 0.8253 - val_RMSE: 0.7646\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8091 - RMSE: 0.7447 - val_loss: 0.8251 - val_RMSE: 0.7646\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8154 - RMSE: 0.7511 - val_loss: 0.8250 - val_RMSE: 0.7646\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8165 - RMSE: 0.7523 - val_loss: 0.8249 - val_RMSE: 0.7645\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8161 - RMSE: 0.7520 - val_loss: 0.8248 - val_RMSE: 0.7645\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8149 - RMSE: 0.7509 - val_loss: 0.8247 - val_RMSE: 0.7645\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8283 - RMSE: 0.7644 - val_loss: 0.8246 - val_RMSE: 0.7645\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8062 - RMSE: 0.7424 - val_loss: 0.8244 - val_RMSE: 0.7645\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8183 - RMSE: 0.7547 - val_loss: 0.8243 - val_RMSE: 0.7645\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8199 - RMSE: 0.7564 - val_loss: 0.8242 - val_RMSE: 0.7645\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8207 - RMSE: 0.7573 - val_loss: 0.8241 - val_RMSE: 0.7645\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8106 - RMSE: 0.7474 - val_loss: 0.8240 - val_RMSE: 0.7645\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8113 - RMSE: 0.7481 - val_loss: 0.8238 - val_RMSE: 0.7645\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8153 - RMSE: 0.7522 - val_loss: 0.8237 - val_RMSE: 0.7645\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8092 - RMSE: 0.7462 - val_loss: 0.8236 - val_RMSE: 0.7645\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8071 - RMSE: 0.7443 - val_loss: 0.8235 - val_RMSE: 0.7645\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8096 - RMSE: 0.7468 - val_loss: 0.8234 - val_RMSE: 0.7645\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8110 - RMSE: 0.7484 - val_loss: 0.8233 - val_RMSE: 0.7645\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8103 - RMSE: 0.7478 - val_loss: 0.8232 - val_RMSE: 0.7645\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8117 - RMSE: 0.7494 - val_loss: 0.8230 - val_RMSE: 0.7645\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8206 - RMSE: 0.7583 - val_loss: 0.8229 - val_RMSE: 0.7644\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8056 - RMSE: 0.7434 - val_loss: 0.8228 - val_RMSE: 0.7644\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8078 - RMSE: 0.7458 - val_loss: 0.8227 - val_RMSE: 0.7644\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8044 - RMSE: 0.7425 - val_loss: 0.8226 - val_RMSE: 0.7644\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8158 - RMSE: 0.7540 - val_loss: 0.8225 - val_RMSE: 0.7644\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8065 - RMSE: 0.7448 - val_loss: 0.8224 - val_RMSE: 0.7644\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8145 - RMSE: 0.7529 - val_loss: 0.8222 - val_RMSE: 0.7644\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8053 - RMSE: 0.7438 - val_loss: 0.8221 - val_RMSE: 0.7644\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8178 - RMSE: 0.7564 - val_loss: 0.8220 - val_RMSE: 0.7644\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8129 - RMSE: 0.7516 - val_loss: 0.8219 - val_RMSE: 0.7644\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8143 - RMSE: 0.7531 - val_loss: 0.8218 - val_RMSE: 0.7644\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8183 - RMSE: 0.7572 - val_loss: 0.8217 - val_RMSE: 0.7644\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8077 - RMSE: 0.7468 - val_loss: 0.8216 - val_RMSE: 0.7644\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8089 - RMSE: 0.7481 - val_loss: 0.8215 - val_RMSE: 0.7644\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8138 - RMSE: 0.7531 - val_loss: 0.8214 - val_RMSE: 0.7644\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8127 - RMSE: 0.7521 - val_loss: 0.8212 - val_RMSE: 0.7644\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8076 - RMSE: 0.7471 - val_loss: 0.8211 - val_RMSE: 0.7644\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8010 - RMSE: 0.7406 - val_loss: 0.8210 - val_RMSE: 0.7644\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8039 - RMSE: 0.7436 - val_loss: 0.8209 - val_RMSE: 0.7644\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8118 - RMSE: 0.7516 - val_loss: 0.8208 - val_RMSE: 0.7643\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8111 - RMSE: 0.7510 - val_loss: 0.8207 - val_RMSE: 0.7643\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8164 - RMSE: 0.7564 - val_loss: 0.8206 - val_RMSE: 0.7643\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8111 - RMSE: 0.7513 - val_loss: 0.8205 - val_RMSE: 0.7643\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8114 - RMSE: 0.7516 - val_loss: 0.8204 - val_RMSE: 0.7643\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8135 - RMSE: 0.7538 - val_loss: 0.8203 - val_RMSE: 0.7643\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8099 - RMSE: 0.7504 - val_loss: 0.8202 - val_RMSE: 0.7643\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8073 - RMSE: 0.7479 - val_loss: 0.8201 - val_RMSE: 0.7643\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8153 - RMSE: 0.7560 - val_loss: 0.8199 - val_RMSE: 0.7643\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8035 - RMSE: 0.7443 - val_loss: 0.8198 - val_RMSE: 0.7643\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8121 - RMSE: 0.7530 - val_loss: 0.8197 - val_RMSE: 0.7643\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8103 - RMSE: 0.7513 - val_loss: 0.8196 - val_RMSE: 0.7643\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8039 - RMSE: 0.7450 - val_loss: 0.8195 - val_RMSE: 0.7643\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8107 - RMSE: 0.7519 - val_loss: 0.8194 - val_RMSE: 0.7643\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8146 - RMSE: 0.7559 - val_loss: 0.8193 - val_RMSE: 0.7643\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=10 / Init = GlorotUniform / min_loss = 0.7642785906791687\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotUniform\n",
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_37 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_38 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_72 (Embedding)        (None, 1, 11)        319         input_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_73 (Embedding)        (None, 1, 11)        1782        input_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_18 (Dot)                    (None, 1, 1)         0           embedding_72[0][0]               \n",
            "                                                                 embedding_73[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_74 (Embedding)        (None, 1, 1)         29          input_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_75 (Embedding)        (None, 1, 1)         162         input_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 1, 1)         0           dot_18[0][0]                     \n",
            "                                                                 embedding_74[0][0]               \n",
            "                                                                 embedding_75[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_18 (Flatten)            (None, 1)            0           add_18[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,292\n",
            "Trainable params: 2,292\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.0512 - RMSE: 0.7835 - val_loss: 0.8778 - val_RMSE: 0.7675\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8668 - RMSE: 0.7567 - val_loss: 0.8757 - val_RMSE: 0.7674\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8673 - RMSE: 0.7581 - val_loss: 0.8754 - val_RMSE: 0.7673\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8727 - RMSE: 0.7637 - val_loss: 0.8752 - val_RMSE: 0.7673\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8560 - RMSE: 0.7472 - val_loss: 0.8749 - val_RMSE: 0.7673\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8685 - RMSE: 0.7599 - val_loss: 0.8746 - val_RMSE: 0.7673\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8684 - RMSE: 0.7601 - val_loss: 0.8744 - val_RMSE: 0.7672\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8655 - RMSE: 0.7575 - val_loss: 0.8741 - val_RMSE: 0.7672\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8671 - RMSE: 0.7593 - val_loss: 0.8738 - val_RMSE: 0.7672\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8621 - RMSE: 0.7545 - val_loss: 0.8736 - val_RMSE: 0.7672\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8735 - RMSE: 0.7662 - val_loss: 0.8733 - val_RMSE: 0.7671\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8630 - RMSE: 0.7559 - val_loss: 0.8731 - val_RMSE: 0.7671\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8532 - RMSE: 0.7463 - val_loss: 0.8728 - val_RMSE: 0.7671\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8718 - RMSE: 0.7651 - val_loss: 0.8725 - val_RMSE: 0.7671\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8692 - RMSE: 0.7628 - val_loss: 0.8723 - val_RMSE: 0.7670\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8642 - RMSE: 0.7580 - val_loss: 0.8720 - val_RMSE: 0.7670\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8607 - RMSE: 0.7547 - val_loss: 0.8718 - val_RMSE: 0.7670\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8639 - RMSE: 0.7581 - val_loss: 0.8715 - val_RMSE: 0.7670\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8687 - RMSE: 0.7632 - val_loss: 0.8713 - val_RMSE: 0.7669\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8689 - RMSE: 0.7636 - val_loss: 0.8710 - val_RMSE: 0.7669\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8556 - RMSE: 0.7505 - val_loss: 0.8707 - val_RMSE: 0.7669\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8583 - RMSE: 0.7534 - val_loss: 0.8705 - val_RMSE: 0.7669\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8653 - RMSE: 0.7606 - val_loss: 0.8702 - val_RMSE: 0.7669\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8671 - RMSE: 0.7627 - val_loss: 0.8700 - val_RMSE: 0.7668\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8585 - RMSE: 0.7543 - val_loss: 0.8697 - val_RMSE: 0.7668\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8558 - RMSE: 0.7518 - val_loss: 0.8695 - val_RMSE: 0.7668\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8612 - RMSE: 0.7575 - val_loss: 0.8692 - val_RMSE: 0.7668\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8620 - RMSE: 0.7585 - val_loss: 0.8690 - val_RMSE: 0.7667\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8595 - RMSE: 0.7562 - val_loss: 0.8687 - val_RMSE: 0.7667\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8647 - RMSE: 0.7616 - val_loss: 0.8685 - val_RMSE: 0.7667\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8581 - RMSE: 0.7552 - val_loss: 0.8682 - val_RMSE: 0.7667\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8620 - RMSE: 0.7594 - val_loss: 0.8680 - val_RMSE: 0.7667\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8547 - RMSE: 0.7522 - val_loss: 0.8678 - val_RMSE: 0.7666\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8583 - RMSE: 0.7560 - val_loss: 0.8675 - val_RMSE: 0.7666\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8573 - RMSE: 0.7553 - val_loss: 0.8673 - val_RMSE: 0.7666\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8580 - RMSE: 0.7562 - val_loss: 0.8670 - val_RMSE: 0.7666\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8635 - RMSE: 0.7619 - val_loss: 0.8668 - val_RMSE: 0.7666\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8642 - RMSE: 0.7628 - val_loss: 0.8665 - val_RMSE: 0.7665\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8700 - RMSE: 0.7689 - val_loss: 0.8663 - val_RMSE: 0.7665\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8564 - RMSE: 0.7555 - val_loss: 0.8661 - val_RMSE: 0.7665\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8637 - RMSE: 0.7630 - val_loss: 0.8658 - val_RMSE: 0.7665\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8517 - RMSE: 0.7512 - val_loss: 0.8656 - val_RMSE: 0.7665\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8631 - RMSE: 0.7628 - val_loss: 0.8653 - val_RMSE: 0.7664\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8527 - RMSE: 0.7526 - val_loss: 0.8651 - val_RMSE: 0.7664\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8634 - RMSE: 0.7636 - val_loss: 0.8649 - val_RMSE: 0.7664\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8526 - RMSE: 0.7529 - val_loss: 0.8646 - val_RMSE: 0.7664\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8535 - RMSE: 0.7541 - val_loss: 0.8644 - val_RMSE: 0.7664\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8631 - RMSE: 0.7638 - val_loss: 0.8642 - val_RMSE: 0.7663\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8561 - RMSE: 0.7570 - val_loss: 0.8639 - val_RMSE: 0.7663\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8522 - RMSE: 0.7533 - val_loss: 0.8637 - val_RMSE: 0.7663\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8563 - RMSE: 0.7576 - val_loss: 0.8635 - val_RMSE: 0.7663\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8584 - RMSE: 0.7599 - val_loss: 0.8633 - val_RMSE: 0.7663\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8556 - RMSE: 0.7573 - val_loss: 0.8630 - val_RMSE: 0.7662\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8569 - RMSE: 0.7589 - val_loss: 0.8628 - val_RMSE: 0.7662\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8662 - RMSE: 0.7684 - val_loss: 0.8626 - val_RMSE: 0.7662\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8486 - RMSE: 0.7511 - val_loss: 0.8623 - val_RMSE: 0.7662\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8501 - RMSE: 0.7527 - val_loss: 0.8621 - val_RMSE: 0.7662\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8530 - RMSE: 0.7558 - val_loss: 0.8619 - val_RMSE: 0.7662\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8516 - RMSE: 0.7546 - val_loss: 0.8617 - val_RMSE: 0.7661\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8532 - RMSE: 0.7564 - val_loss: 0.8614 - val_RMSE: 0.7661\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8582 - RMSE: 0.7617 - val_loss: 0.8612 - val_RMSE: 0.7661\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8579 - RMSE: 0.7615 - val_loss: 0.8610 - val_RMSE: 0.7661\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8565 - RMSE: 0.7603 - val_loss: 0.8607 - val_RMSE: 0.7661\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8560 - RMSE: 0.7600 - val_loss: 0.8605 - val_RMSE: 0.7660\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8570 - RMSE: 0.7612 - val_loss: 0.8603 - val_RMSE: 0.7660\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8596 - RMSE: 0.7640 - val_loss: 0.8601 - val_RMSE: 0.7660\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8512 - RMSE: 0.7559 - val_loss: 0.8599 - val_RMSE: 0.7660\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8580 - RMSE: 0.7628 - val_loss: 0.8596 - val_RMSE: 0.7660\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8578 - RMSE: 0.7628 - val_loss: 0.8594 - val_RMSE: 0.7660\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8487 - RMSE: 0.7539 - val_loss: 0.8592 - val_RMSE: 0.7659\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8544 - RMSE: 0.7598 - val_loss: 0.8590 - val_RMSE: 0.7659\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8472 - RMSE: 0.7528 - val_loss: 0.8588 - val_RMSE: 0.7659\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8410 - RMSE: 0.7468 - val_loss: 0.8585 - val_RMSE: 0.7659\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8448 - RMSE: 0.7508 - val_loss: 0.8583 - val_RMSE: 0.7659\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8416 - RMSE: 0.7478 - val_loss: 0.8581 - val_RMSE: 0.7659\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8421 - RMSE: 0.7484 - val_loss: 0.8579 - val_RMSE: 0.7658\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8568 - RMSE: 0.7634 - val_loss: 0.8577 - val_RMSE: 0.7658\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8501 - RMSE: 0.7568 - val_loss: 0.8575 - val_RMSE: 0.7658\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8589 - RMSE: 0.7658 - val_loss: 0.8573 - val_RMSE: 0.7658\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8503 - RMSE: 0.7575 - val_loss: 0.8570 - val_RMSE: 0.7658\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8475 - RMSE: 0.7548 - val_loss: 0.8568 - val_RMSE: 0.7658\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8402 - RMSE: 0.7477 - val_loss: 0.8566 - val_RMSE: 0.7657\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8505 - RMSE: 0.7582 - val_loss: 0.8564 - val_RMSE: 0.7657\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8474 - RMSE: 0.7553 - val_loss: 0.8562 - val_RMSE: 0.7657\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8485 - RMSE: 0.7567 - val_loss: 0.8560 - val_RMSE: 0.7657\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8515 - RMSE: 0.7597 - val_loss: 0.8558 - val_RMSE: 0.7657\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8528 - RMSE: 0.7613 - val_loss: 0.8556 - val_RMSE: 0.7657\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8386 - RMSE: 0.7473 - val_loss: 0.8554 - val_RMSE: 0.7656\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8438 - RMSE: 0.7526 - val_loss: 0.8552 - val_RMSE: 0.7656\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8507 - RMSE: 0.7597 - val_loss: 0.8550 - val_RMSE: 0.7656\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8470 - RMSE: 0.7563 - val_loss: 0.8547 - val_RMSE: 0.7656\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8539 - RMSE: 0.7633 - val_loss: 0.8545 - val_RMSE: 0.7656\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8475 - RMSE: 0.7571 - val_loss: 0.8543 - val_RMSE: 0.7656\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8395 - RMSE: 0.7493 - val_loss: 0.8541 - val_RMSE: 0.7656\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8382 - RMSE: 0.7481 - val_loss: 0.8539 - val_RMSE: 0.7655\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8470 - RMSE: 0.7571 - val_loss: 0.8537 - val_RMSE: 0.7655\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8399 - RMSE: 0.7502 - val_loss: 0.8535 - val_RMSE: 0.7655\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8443 - RMSE: 0.7548 - val_loss: 0.8533 - val_RMSE: 0.7655\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8397 - RMSE: 0.7504 - val_loss: 0.8531 - val_RMSE: 0.7655\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8467 - RMSE: 0.7576 - val_loss: 0.8529 - val_RMSE: 0.7655\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8377 - RMSE: 0.7487 - val_loss: 0.8527 - val_RMSE: 0.7655\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8456 - RMSE: 0.7569 - val_loss: 0.8525 - val_RMSE: 0.7655\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8358 - RMSE: 0.7472 - val_loss: 0.8523 - val_RMSE: 0.7654\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8388 - RMSE: 0.7504 - val_loss: 0.8521 - val_RMSE: 0.7654\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8461 - RMSE: 0.7578 - val_loss: 0.8519 - val_RMSE: 0.7654\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8398 - RMSE: 0.7517 - val_loss: 0.8517 - val_RMSE: 0.7654\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8388 - RMSE: 0.7509 - val_loss: 0.8515 - val_RMSE: 0.7654\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8531 - RMSE: 0.7654 - val_loss: 0.8513 - val_RMSE: 0.7654\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8435 - RMSE: 0.7560 - val_loss: 0.8511 - val_RMSE: 0.7654\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8482 - RMSE: 0.7609 - val_loss: 0.8509 - val_RMSE: 0.7653\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8578 - RMSE: 0.7707 - val_loss: 0.8508 - val_RMSE: 0.7653\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8426 - RMSE: 0.7556 - val_loss: 0.8506 - val_RMSE: 0.7653\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8392 - RMSE: 0.7524 - val_loss: 0.8504 - val_RMSE: 0.7653\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8503 - RMSE: 0.7637 - val_loss: 0.8502 - val_RMSE: 0.7653\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8498 - RMSE: 0.7634 - val_loss: 0.8500 - val_RMSE: 0.7653\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8458 - RMSE: 0.7595 - val_loss: 0.8498 - val_RMSE: 0.7653\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8335 - RMSE: 0.7474 - val_loss: 0.8496 - val_RMSE: 0.7653\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8461 - RMSE: 0.7602 - val_loss: 0.8494 - val_RMSE: 0.7652\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8473 - RMSE: 0.7616 - val_loss: 0.8492 - val_RMSE: 0.7652\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8329 - RMSE: 0.7473 - val_loss: 0.8490 - val_RMSE: 0.7652\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8334 - RMSE: 0.7480 - val_loss: 0.8488 - val_RMSE: 0.7652\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8338 - RMSE: 0.7485 - val_loss: 0.8487 - val_RMSE: 0.7652\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8427 - RMSE: 0.7576 - val_loss: 0.8485 - val_RMSE: 0.7652\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8357 - RMSE: 0.7508 - val_loss: 0.8483 - val_RMSE: 0.7652\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8319 - RMSE: 0.7472 - val_loss: 0.8481 - val_RMSE: 0.7652\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8320 - RMSE: 0.7474 - val_loss: 0.8479 - val_RMSE: 0.7651\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8375 - RMSE: 0.7531 - val_loss: 0.8477 - val_RMSE: 0.7651\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8334 - RMSE: 0.7491 - val_loss: 0.8475 - val_RMSE: 0.7651\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8394 - RMSE: 0.7554 - val_loss: 0.8474 - val_RMSE: 0.7651\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8413 - RMSE: 0.7574 - val_loss: 0.8472 - val_RMSE: 0.7651\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8311 - RMSE: 0.7474 - val_loss: 0.8470 - val_RMSE: 0.7651\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8287 - RMSE: 0.7451 - val_loss: 0.8468 - val_RMSE: 0.7651\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8427 - RMSE: 0.7594 - val_loss: 0.8466 - val_RMSE: 0.7651\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8254 - RMSE: 0.7421 - val_loss: 0.8464 - val_RMSE: 0.7650\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8377 - RMSE: 0.7547 - val_loss: 0.8463 - val_RMSE: 0.7650\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8332 - RMSE: 0.7503 - val_loss: 0.8461 - val_RMSE: 0.7650\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8320 - RMSE: 0.7492 - val_loss: 0.8459 - val_RMSE: 0.7650\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8309 - RMSE: 0.7483 - val_loss: 0.8457 - val_RMSE: 0.7650\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8334 - RMSE: 0.7511 - val_loss: 0.8455 - val_RMSE: 0.7650\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8439 - RMSE: 0.7617 - val_loss: 0.8454 - val_RMSE: 0.7650\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8386 - RMSE: 0.7566 - val_loss: 0.8452 - val_RMSE: 0.7650\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8313 - RMSE: 0.7494 - val_loss: 0.8450 - val_RMSE: 0.7650\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8400 - RMSE: 0.7583 - val_loss: 0.8448 - val_RMSE: 0.7650\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8388 - RMSE: 0.7573 - val_loss: 0.8447 - val_RMSE: 0.7649\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8392 - RMSE: 0.7579 - val_loss: 0.8445 - val_RMSE: 0.7649\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8315 - RMSE: 0.7502 - val_loss: 0.8443 - val_RMSE: 0.7649\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8251 - RMSE: 0.7441 - val_loss: 0.8441 - val_RMSE: 0.7649\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8263 - RMSE: 0.7453 - val_loss: 0.8439 - val_RMSE: 0.7649\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8318 - RMSE: 0.7511 - val_loss: 0.8438 - val_RMSE: 0.7649\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8419 - RMSE: 0.7613 - val_loss: 0.8436 - val_RMSE: 0.7649\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8352 - RMSE: 0.7547 - val_loss: 0.8434 - val_RMSE: 0.7649\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8435 - RMSE: 0.7632 - val_loss: 0.8433 - val_RMSE: 0.7649\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8317 - RMSE: 0.7516 - val_loss: 0.8431 - val_RMSE: 0.7649\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8279 - RMSE: 0.7479 - val_loss: 0.8429 - val_RMSE: 0.7648\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8274 - RMSE: 0.7476 - val_loss: 0.8427 - val_RMSE: 0.7648\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8261 - RMSE: 0.7464 - val_loss: 0.8426 - val_RMSE: 0.7648\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8467 - RMSE: 0.7672 - val_loss: 0.8424 - val_RMSE: 0.7648\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8310 - RMSE: 0.7516 - val_loss: 0.8422 - val_RMSE: 0.7648\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8236 - RMSE: 0.7444 - val_loss: 0.8421 - val_RMSE: 0.7648\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8380 - RMSE: 0.7590 - val_loss: 0.8419 - val_RMSE: 0.7648\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8354 - RMSE: 0.7565 - val_loss: 0.8417 - val_RMSE: 0.7648\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8303 - RMSE: 0.7516 - val_loss: 0.8416 - val_RMSE: 0.7648\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8309 - RMSE: 0.7523 - val_loss: 0.8414 - val_RMSE: 0.7647\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8212 - RMSE: 0.7428 - val_loss: 0.8412 - val_RMSE: 0.7647\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8315 - RMSE: 0.7532 - val_loss: 0.8411 - val_RMSE: 0.7647\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8407 - RMSE: 0.7626 - val_loss: 0.8409 - val_RMSE: 0.7647\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8254 - RMSE: 0.7474 - val_loss: 0.8407 - val_RMSE: 0.7647\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8285 - RMSE: 0.7507 - val_loss: 0.8406 - val_RMSE: 0.7647\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8361 - RMSE: 0.7585 - val_loss: 0.8404 - val_RMSE: 0.7647\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8257 - RMSE: 0.7482 - val_loss: 0.8402 - val_RMSE: 0.7647\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8224 - RMSE: 0.7450 - val_loss: 0.8401 - val_RMSE: 0.7647\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8393 - RMSE: 0.7621 - val_loss: 0.8399 - val_RMSE: 0.7647\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8399 - RMSE: 0.7629 - val_loss: 0.8397 - val_RMSE: 0.7647\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8297 - RMSE: 0.7529 - val_loss: 0.8396 - val_RMSE: 0.7646\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8219 - RMSE: 0.7451 - val_loss: 0.8394 - val_RMSE: 0.7646\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8360 - RMSE: 0.7595 - val_loss: 0.8392 - val_RMSE: 0.7646\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8257 - RMSE: 0.7492 - val_loss: 0.8391 - val_RMSE: 0.7646\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8385 - RMSE: 0.7622 - val_loss: 0.8389 - val_RMSE: 0.7646\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8373 - RMSE: 0.7612 - val_loss: 0.8388 - val_RMSE: 0.7646\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8375 - RMSE: 0.7615 - val_loss: 0.8386 - val_RMSE: 0.7646\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8199 - RMSE: 0.7440 - val_loss: 0.8385 - val_RMSE: 0.7646\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8370 - RMSE: 0.7613 - val_loss: 0.8383 - val_RMSE: 0.7646\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8304 - RMSE: 0.7548 - val_loss: 0.8381 - val_RMSE: 0.7646\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8313 - RMSE: 0.7559 - val_loss: 0.8380 - val_RMSE: 0.7646\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8234 - RMSE: 0.7481 - val_loss: 0.8378 - val_RMSE: 0.7646\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8410 - RMSE: 0.7659 - val_loss: 0.8377 - val_RMSE: 0.7645\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8322 - RMSE: 0.7572 - val_loss: 0.8375 - val_RMSE: 0.7645\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8215 - RMSE: 0.7467 - val_loss: 0.8373 - val_RMSE: 0.7645\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8341 - RMSE: 0.7594 - val_loss: 0.8372 - val_RMSE: 0.7645\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8161 - RMSE: 0.7416 - val_loss: 0.8370 - val_RMSE: 0.7645\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8229 - RMSE: 0.7485 - val_loss: 0.8369 - val_RMSE: 0.7645\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8299 - RMSE: 0.7557 - val_loss: 0.8367 - val_RMSE: 0.7645\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8139 - RMSE: 0.7398 - val_loss: 0.8366 - val_RMSE: 0.7645\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8340 - RMSE: 0.7600 - val_loss: 0.8364 - val_RMSE: 0.7645\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8283 - RMSE: 0.7545 - val_loss: 0.8363 - val_RMSE: 0.7645\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8232 - RMSE: 0.7495 - val_loss: 0.8361 - val_RMSE: 0.7645\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8257 - RMSE: 0.7522 - val_loss: 0.8360 - val_RMSE: 0.7644\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8194 - RMSE: 0.7460 - val_loss: 0.8358 - val_RMSE: 0.7644\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8288 - RMSE: 0.7555 - val_loss: 0.8357 - val_RMSE: 0.7644\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8285 - RMSE: 0.7554 - val_loss: 0.8355 - val_RMSE: 0.7644\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8256 - RMSE: 0.7527 - val_loss: 0.8354 - val_RMSE: 0.7644\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8283 - RMSE: 0.7554 - val_loss: 0.8352 - val_RMSE: 0.7644\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8342 - RMSE: 0.7616 - val_loss: 0.8350 - val_RMSE: 0.7644\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8200 - RMSE: 0.7474 - val_loss: 0.8349 - val_RMSE: 0.7644\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8270 - RMSE: 0.7546 - val_loss: 0.8347 - val_RMSE: 0.7644\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8378 - RMSE: 0.7655 - val_loss: 0.8346 - val_RMSE: 0.7644\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8261 - RMSE: 0.7540 - val_loss: 0.8345 - val_RMSE: 0.7644\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8186 - RMSE: 0.7466 - val_loss: 0.8343 - val_RMSE: 0.7644\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8258 - RMSE: 0.7539 - val_loss: 0.8342 - val_RMSE: 0.7644\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8227 - RMSE: 0.7510 - val_loss: 0.8340 - val_RMSE: 0.7643\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8187 - RMSE: 0.7471 - val_loss: 0.8339 - val_RMSE: 0.7643\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8226 - RMSE: 0.7511 - val_loss: 0.8337 - val_RMSE: 0.7643\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8314 - RMSE: 0.7601 - val_loss: 0.8336 - val_RMSE: 0.7643\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8146 - RMSE: 0.7435 - val_loss: 0.8334 - val_RMSE: 0.7643\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8142 - RMSE: 0.7431 - val_loss: 0.8333 - val_RMSE: 0.7643\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8271 - RMSE: 0.7561 - val_loss: 0.8331 - val_RMSE: 0.7643\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8260 - RMSE: 0.7552 - val_loss: 0.8330 - val_RMSE: 0.7643\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8236 - RMSE: 0.7529 - val_loss: 0.8329 - val_RMSE: 0.7643\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8304 - RMSE: 0.7598 - val_loss: 0.8327 - val_RMSE: 0.7643\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8186 - RMSE: 0.7482 - val_loss: 0.8326 - val_RMSE: 0.7643\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8181 - RMSE: 0.7478 - val_loss: 0.8324 - val_RMSE: 0.7643\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8327 - RMSE: 0.7626 - val_loss: 0.8323 - val_RMSE: 0.7643\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8220 - RMSE: 0.7520 - val_loss: 0.8321 - val_RMSE: 0.7643\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8341 - RMSE: 0.7642 - val_loss: 0.8320 - val_RMSE: 0.7643\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8269 - RMSE: 0.7572 - val_loss: 0.8319 - val_RMSE: 0.7642\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8202 - RMSE: 0.7506 - val_loss: 0.8317 - val_RMSE: 0.7642\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8312 - RMSE: 0.7617 - val_loss: 0.8316 - val_RMSE: 0.7642\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8231 - RMSE: 0.7538 - val_loss: 0.8314 - val_RMSE: 0.7642\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8231 - RMSE: 0.7539 - val_loss: 0.8313 - val_RMSE: 0.7642\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8356 - RMSE: 0.7666 - val_loss: 0.8312 - val_RMSE: 0.7642\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8190 - RMSE: 0.7501 - val_loss: 0.8310 - val_RMSE: 0.7642\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8257 - RMSE: 0.7569 - val_loss: 0.8309 - val_RMSE: 0.7642\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8218 - RMSE: 0.7532 - val_loss: 0.8307 - val_RMSE: 0.7642\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8260 - RMSE: 0.7575 - val_loss: 0.8306 - val_RMSE: 0.7642\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8181 - RMSE: 0.7497 - val_loss: 0.8305 - val_RMSE: 0.7642\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8173 - RMSE: 0.7490 - val_loss: 0.8303 - val_RMSE: 0.7642\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8146 - RMSE: 0.7464 - val_loss: 0.8302 - val_RMSE: 0.7642\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8288 - RMSE: 0.7607 - val_loss: 0.8301 - val_RMSE: 0.7642\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8242 - RMSE: 0.7563 - val_loss: 0.8299 - val_RMSE: 0.7642\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8174 - RMSE: 0.7496 - val_loss: 0.8298 - val_RMSE: 0.7642\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8209 - RMSE: 0.7532 - val_loss: 0.8297 - val_RMSE: 0.7641\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8258 - RMSE: 0.7583 - val_loss: 0.8295 - val_RMSE: 0.7641\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8240 - RMSE: 0.7566 - val_loss: 0.8294 - val_RMSE: 0.7641\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8219 - RMSE: 0.7546 - val_loss: 0.8292 - val_RMSE: 0.7641\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8196 - RMSE: 0.7524 - val_loss: 0.8291 - val_RMSE: 0.7641\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8220 - RMSE: 0.7549 - val_loss: 0.8290 - val_RMSE: 0.7641\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8167 - RMSE: 0.7498 - val_loss: 0.8288 - val_RMSE: 0.7641\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8055 - RMSE: 0.7387 - val_loss: 0.8287 - val_RMSE: 0.7641\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8242 - RMSE: 0.7576 - val_loss: 0.8286 - val_RMSE: 0.7641\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8229 - RMSE: 0.7563 - val_loss: 0.8284 - val_RMSE: 0.7641\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8172 - RMSE: 0.7508 - val_loss: 0.8283 - val_RMSE: 0.7641\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8256 - RMSE: 0.7594 - val_loss: 0.8282 - val_RMSE: 0.7641\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8221 - RMSE: 0.7560 - val_loss: 0.8281 - val_RMSE: 0.7641\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8189 - RMSE: 0.7529 - val_loss: 0.8279 - val_RMSE: 0.7641\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8237 - RMSE: 0.7577 - val_loss: 0.8278 - val_RMSE: 0.7641\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8234 - RMSE: 0.7576 - val_loss: 0.8277 - val_RMSE: 0.7640\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8091 - RMSE: 0.7434 - val_loss: 0.8275 - val_RMSE: 0.7640\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8140 - RMSE: 0.7484 - val_loss: 0.8274 - val_RMSE: 0.7640\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8191 - RMSE: 0.7537 - val_loss: 0.8273 - val_RMSE: 0.7640\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8142 - RMSE: 0.7489 - val_loss: 0.8272 - val_RMSE: 0.7640\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8201 - RMSE: 0.7549 - val_loss: 0.8270 - val_RMSE: 0.7640\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8192 - RMSE: 0.7541 - val_loss: 0.8269 - val_RMSE: 0.7640\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8118 - RMSE: 0.7468 - val_loss: 0.8268 - val_RMSE: 0.7640\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8213 - RMSE: 0.7565 - val_loss: 0.8266 - val_RMSE: 0.7640\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8139 - RMSE: 0.7492 - val_loss: 0.8265 - val_RMSE: 0.7640\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8167 - RMSE: 0.7521 - val_loss: 0.8264 - val_RMSE: 0.7640\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8145 - RMSE: 0.7501 - val_loss: 0.8263 - val_RMSE: 0.7640\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8222 - RMSE: 0.7578 - val_loss: 0.8261 - val_RMSE: 0.7640\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8225 - RMSE: 0.7583 - val_loss: 0.8260 - val_RMSE: 0.7640\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8178 - RMSE: 0.7537 - val_loss: 0.8259 - val_RMSE: 0.7640\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8081 - RMSE: 0.7441 - val_loss: 0.8258 - val_RMSE: 0.7640\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8069 - RMSE: 0.7430 - val_loss: 0.8256 - val_RMSE: 0.7640\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8000 - RMSE: 0.7362 - val_loss: 0.8255 - val_RMSE: 0.7640\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8120 - RMSE: 0.7484 - val_loss: 0.8254 - val_RMSE: 0.7640\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8112 - RMSE: 0.7476 - val_loss: 0.8253 - val_RMSE: 0.7639\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8106 - RMSE: 0.7472 - val_loss: 0.8251 - val_RMSE: 0.7639\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8131 - RMSE: 0.7498 - val_loss: 0.8250 - val_RMSE: 0.7639\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8147 - RMSE: 0.7516 - val_loss: 0.8249 - val_RMSE: 0.7639\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8156 - RMSE: 0.7525 - val_loss: 0.8248 - val_RMSE: 0.7639\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8083 - RMSE: 0.7453 - val_loss: 0.8246 - val_RMSE: 0.7639\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8120 - RMSE: 0.7492 - val_loss: 0.8245 - val_RMSE: 0.7639\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8018 - RMSE: 0.7391 - val_loss: 0.8244 - val_RMSE: 0.7639\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8167 - RMSE: 0.7541 - val_loss: 0.8243 - val_RMSE: 0.7639\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8242 - RMSE: 0.7617 - val_loss: 0.8242 - val_RMSE: 0.7639\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8098 - RMSE: 0.7475 - val_loss: 0.8241 - val_RMSE: 0.7639\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8190 - RMSE: 0.7567 - val_loss: 0.8239 - val_RMSE: 0.7639\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8164 - RMSE: 0.7543 - val_loss: 0.8238 - val_RMSE: 0.7639\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8197 - RMSE: 0.7576 - val_loss: 0.8237 - val_RMSE: 0.7639\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8180 - RMSE: 0.7561 - val_loss: 0.8236 - val_RMSE: 0.7639\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8143 - RMSE: 0.7525 - val_loss: 0.8235 - val_RMSE: 0.7639\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8119 - RMSE: 0.7502 - val_loss: 0.8233 - val_RMSE: 0.7639\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8097 - RMSE: 0.7481 - val_loss: 0.8232 - val_RMSE: 0.7639\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8110 - RMSE: 0.7495 - val_loss: 0.8231 - val_RMSE: 0.7639\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8140 - RMSE: 0.7526 - val_loss: 0.8230 - val_RMSE: 0.7639\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8183 - RMSE: 0.7571 - val_loss: 0.8229 - val_RMSE: 0.7639\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8190 - RMSE: 0.7579 - val_loss: 0.8227 - val_RMSE: 0.7638\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8155 - RMSE: 0.7544 - val_loss: 0.8226 - val_RMSE: 0.7638\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8163 - RMSE: 0.7554 - val_loss: 0.8225 - val_RMSE: 0.7638\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8138 - RMSE: 0.7530 - val_loss: 0.8224 - val_RMSE: 0.7638\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7995 - RMSE: 0.7388 - val_loss: 0.8223 - val_RMSE: 0.7638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=11 / Init = GlorotUniform / min_loss = 0.7638276219367981\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotUniform\n",
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_39 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_40 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_76 (Embedding)        (None, 1, 12)        348         input_39[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_77 (Embedding)        (None, 1, 12)        1944        input_40[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_19 (Dot)                    (None, 1, 1)         0           embedding_76[0][0]               \n",
            "                                                                 embedding_77[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_78 (Embedding)        (None, 1, 1)         29          input_39[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_79 (Embedding)        (None, 1, 1)         162         input_40[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 1, 1)         0           dot_19[0][0]                     \n",
            "                                                                 embedding_78[0][0]               \n",
            "                                                                 embedding_79[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_19 (Flatten)            (None, 1)            0           add_19[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,483\n",
            "Trainable params: 2,483\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.0666 - RMSE: 0.7741 - val_loss: 0.8856 - val_RMSE: 0.7685\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8815 - RMSE: 0.7637 - val_loss: 0.8831 - val_RMSE: 0.7682\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8728 - RMSE: 0.7560 - val_loss: 0.8828 - val_RMSE: 0.7682\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8814 - RMSE: 0.7649 - val_loss: 0.8826 - val_RMSE: 0.7682\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8826 - RMSE: 0.7662 - val_loss: 0.8823 - val_RMSE: 0.7682\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8711 - RMSE: 0.7550 - val_loss: 0.8820 - val_RMSE: 0.7681\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8815 - RMSE: 0.7657 - val_loss: 0.8817 - val_RMSE: 0.7681\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8742 - RMSE: 0.7587 - val_loss: 0.8815 - val_RMSE: 0.7681\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8662 - RMSE: 0.7509 - val_loss: 0.8812 - val_RMSE: 0.7681\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8769 - RMSE: 0.7618 - val_loss: 0.8809 - val_RMSE: 0.7680\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8728 - RMSE: 0.7579 - val_loss: 0.8807 - val_RMSE: 0.7680\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8720 - RMSE: 0.7574 - val_loss: 0.8804 - val_RMSE: 0.7680\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8708 - RMSE: 0.7565 - val_loss: 0.8801 - val_RMSE: 0.7680\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8697 - RMSE: 0.7557 - val_loss: 0.8798 - val_RMSE: 0.7680\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8783 - RMSE: 0.7644 - val_loss: 0.8796 - val_RMSE: 0.7679\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8737 - RMSE: 0.7601 - val_loss: 0.8793 - val_RMSE: 0.7679\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8804 - RMSE: 0.7670 - val_loss: 0.8790 - val_RMSE: 0.7679\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8729 - RMSE: 0.7598 - val_loss: 0.8788 - val_RMSE: 0.7679\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8701 - RMSE: 0.7573 - val_loss: 0.8785 - val_RMSE: 0.7679\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8717 - RMSE: 0.7591 - val_loss: 0.8783 - val_RMSE: 0.7678\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8718 - RMSE: 0.7594 - val_loss: 0.8780 - val_RMSE: 0.7678\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8709 - RMSE: 0.7587 - val_loss: 0.8777 - val_RMSE: 0.7678\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8858 - RMSE: 0.7739 - val_loss: 0.8775 - val_RMSE: 0.7678\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8735 - RMSE: 0.7618 - val_loss: 0.8772 - val_RMSE: 0.7677\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8613 - RMSE: 0.7499 - val_loss: 0.8770 - val_RMSE: 0.7677\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8747 - RMSE: 0.7635 - val_loss: 0.8767 - val_RMSE: 0.7677\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8760 - RMSE: 0.7650 - val_loss: 0.8764 - val_RMSE: 0.7677\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8792 - RMSE: 0.7685 - val_loss: 0.8762 - val_RMSE: 0.7677\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8705 - RMSE: 0.7600 - val_loss: 0.8759 - val_RMSE: 0.7676\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8650 - RMSE: 0.7548 - val_loss: 0.8757 - val_RMSE: 0.7676\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8769 - RMSE: 0.7669 - val_loss: 0.8754 - val_RMSE: 0.7676\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8590 - RMSE: 0.7492 - val_loss: 0.8752 - val_RMSE: 0.7676\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8761 - RMSE: 0.7665 - val_loss: 0.8749 - val_RMSE: 0.7676\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8664 - RMSE: 0.7571 - val_loss: 0.8747 - val_RMSE: 0.7675\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8677 - RMSE: 0.7586 - val_loss: 0.8744 - val_RMSE: 0.7675\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8612 - RMSE: 0.7523 - val_loss: 0.8741 - val_RMSE: 0.7675\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8562 - RMSE: 0.7476 - val_loss: 0.8739 - val_RMSE: 0.7675\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8770 - RMSE: 0.7686 - val_loss: 0.8736 - val_RMSE: 0.7675\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8637 - RMSE: 0.7555 - val_loss: 0.8734 - val_RMSE: 0.7674\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8610 - RMSE: 0.7530 - val_loss: 0.8731 - val_RMSE: 0.7674\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8635 - RMSE: 0.7557 - val_loss: 0.8729 - val_RMSE: 0.7674\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8680 - RMSE: 0.7604 - val_loss: 0.8726 - val_RMSE: 0.7674\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8648 - RMSE: 0.7575 - val_loss: 0.8724 - val_RMSE: 0.7674\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8761 - RMSE: 0.7690 - val_loss: 0.8722 - val_RMSE: 0.7673\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8639 - RMSE: 0.7571 - val_loss: 0.8719 - val_RMSE: 0.7673\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8689 - RMSE: 0.7623 - val_loss: 0.8717 - val_RMSE: 0.7673\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8547 - RMSE: 0.7483 - val_loss: 0.8714 - val_RMSE: 0.7673\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8650 - RMSE: 0.7588 - val_loss: 0.8712 - val_RMSE: 0.7673\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8695 - RMSE: 0.7635 - val_loss: 0.8709 - val_RMSE: 0.7673\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8676 - RMSE: 0.7618 - val_loss: 0.8707 - val_RMSE: 0.7672\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8581 - RMSE: 0.7526 - val_loss: 0.8705 - val_RMSE: 0.7672\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8597 - RMSE: 0.7544 - val_loss: 0.8702 - val_RMSE: 0.7672\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8677 - RMSE: 0.7626 - val_loss: 0.8700 - val_RMSE: 0.7672\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8607 - RMSE: 0.7559 - val_loss: 0.8697 - val_RMSE: 0.7672\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8633 - RMSE: 0.7587 - val_loss: 0.8695 - val_RMSE: 0.7672\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8668 - RMSE: 0.7624 - val_loss: 0.8693 - val_RMSE: 0.7671\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8660 - RMSE: 0.7618 - val_loss: 0.8690 - val_RMSE: 0.7671\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8665 - RMSE: 0.7625 - val_loss: 0.8688 - val_RMSE: 0.7671\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8629 - RMSE: 0.7592 - val_loss: 0.8686 - val_RMSE: 0.7671\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8586 - RMSE: 0.7550 - val_loss: 0.8683 - val_RMSE: 0.7671\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8580 - RMSE: 0.7547 - val_loss: 0.8681 - val_RMSE: 0.7671\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8586 - RMSE: 0.7555 - val_loss: 0.8679 - val_RMSE: 0.7670\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8545 - RMSE: 0.7515 - val_loss: 0.8676 - val_RMSE: 0.7670\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8615 - RMSE: 0.7588 - val_loss: 0.8674 - val_RMSE: 0.7670\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8616 - RMSE: 0.7592 - val_loss: 0.8672 - val_RMSE: 0.7670\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8595 - RMSE: 0.7572 - val_loss: 0.8669 - val_RMSE: 0.7670\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8533 - RMSE: 0.7512 - val_loss: 0.8667 - val_RMSE: 0.7670\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8552 - RMSE: 0.7534 - val_loss: 0.8665 - val_RMSE: 0.7669\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8622 - RMSE: 0.7606 - val_loss: 0.8662 - val_RMSE: 0.7669\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8537 - RMSE: 0.7522 - val_loss: 0.8660 - val_RMSE: 0.7669\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8663 - RMSE: 0.7651 - val_loss: 0.8658 - val_RMSE: 0.7669\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8489 - RMSE: 0.7479 - val_loss: 0.8656 - val_RMSE: 0.7669\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8543 - RMSE: 0.7536 - val_loss: 0.8653 - val_RMSE: 0.7669\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8565 - RMSE: 0.7559 - val_loss: 0.8651 - val_RMSE: 0.7668\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8469 - RMSE: 0.7465 - val_loss: 0.8649 - val_RMSE: 0.7668\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8618 - RMSE: 0.7617 - val_loss: 0.8647 - val_RMSE: 0.7668\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8600 - RMSE: 0.7601 - val_loss: 0.8644 - val_RMSE: 0.7668\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8593 - RMSE: 0.7596 - val_loss: 0.8642 - val_RMSE: 0.7668\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8541 - RMSE: 0.7546 - val_loss: 0.8640 - val_RMSE: 0.7668\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8580 - RMSE: 0.7586 - val_loss: 0.8638 - val_RMSE: 0.7668\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8661 - RMSE: 0.7670 - val_loss: 0.8635 - val_RMSE: 0.7667\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8518 - RMSE: 0.7529 - val_loss: 0.8633 - val_RMSE: 0.7667\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8576 - RMSE: 0.7589 - val_loss: 0.8631 - val_RMSE: 0.7667\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8472 - RMSE: 0.7487 - val_loss: 0.8629 - val_RMSE: 0.7667\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8526 - RMSE: 0.7542 - val_loss: 0.8627 - val_RMSE: 0.7667\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8504 - RMSE: 0.7523 - val_loss: 0.8624 - val_RMSE: 0.7667\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8612 - RMSE: 0.7633 - val_loss: 0.8622 - val_RMSE: 0.7667\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8530 - RMSE: 0.7553 - val_loss: 0.8620 - val_RMSE: 0.7666\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8482 - RMSE: 0.7507 - val_loss: 0.8618 - val_RMSE: 0.7666\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8531 - RMSE: 0.7558 - val_loss: 0.8616 - val_RMSE: 0.7666\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8487 - RMSE: 0.7516 - val_loss: 0.8614 - val_RMSE: 0.7666\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8470 - RMSE: 0.7501 - val_loss: 0.8612 - val_RMSE: 0.7666\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8504 - RMSE: 0.7537 - val_loss: 0.8609 - val_RMSE: 0.7666\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8535 - RMSE: 0.7570 - val_loss: 0.8607 - val_RMSE: 0.7666\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8526 - RMSE: 0.7563 - val_loss: 0.8605 - val_RMSE: 0.7665\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8575 - RMSE: 0.7614 - val_loss: 0.8603 - val_RMSE: 0.7665\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8551 - RMSE: 0.7592 - val_loss: 0.8601 - val_RMSE: 0.7665\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8498 - RMSE: 0.7541 - val_loss: 0.8599 - val_RMSE: 0.7665\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8446 - RMSE: 0.7491 - val_loss: 0.8597 - val_RMSE: 0.7665\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8400 - RMSE: 0.7447 - val_loss: 0.8595 - val_RMSE: 0.7665\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8514 - RMSE: 0.7563 - val_loss: 0.8592 - val_RMSE: 0.7665\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8525 - RMSE: 0.7575 - val_loss: 0.8590 - val_RMSE: 0.7665\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8622 - RMSE: 0.7674 - val_loss: 0.8588 - val_RMSE: 0.7664\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8570 - RMSE: 0.7625 - val_loss: 0.8586 - val_RMSE: 0.7664\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8442 - RMSE: 0.7498 - val_loss: 0.8584 - val_RMSE: 0.7664\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8549 - RMSE: 0.7608 - val_loss: 0.8582 - val_RMSE: 0.7664\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8494 - RMSE: 0.7555 - val_loss: 0.8580 - val_RMSE: 0.7664\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8504 - RMSE: 0.7566 - val_loss: 0.8578 - val_RMSE: 0.7664\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8511 - RMSE: 0.7575 - val_loss: 0.8576 - val_RMSE: 0.7664\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8521 - RMSE: 0.7587 - val_loss: 0.8574 - val_RMSE: 0.7663\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8376 - RMSE: 0.7444 - val_loss: 0.8572 - val_RMSE: 0.7663\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8453 - RMSE: 0.7523 - val_loss: 0.8570 - val_RMSE: 0.7663\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8406 - RMSE: 0.7478 - val_loss: 0.8568 - val_RMSE: 0.7663\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8548 - RMSE: 0.7621 - val_loss: 0.8566 - val_RMSE: 0.7663\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8460 - RMSE: 0.7535 - val_loss: 0.8564 - val_RMSE: 0.7663\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8458 - RMSE: 0.7536 - val_loss: 0.8562 - val_RMSE: 0.7663\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8496 - RMSE: 0.7576 - val_loss: 0.8560 - val_RMSE: 0.7663\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8499 - RMSE: 0.7579 - val_loss: 0.8558 - val_RMSE: 0.7662\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8495 - RMSE: 0.7578 - val_loss: 0.8556 - val_RMSE: 0.7662\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8496 - RMSE: 0.7581 - val_loss: 0.8554 - val_RMSE: 0.7662\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8428 - RMSE: 0.7514 - val_loss: 0.8552 - val_RMSE: 0.7662\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8463 - RMSE: 0.7551 - val_loss: 0.8550 - val_RMSE: 0.7662\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8424 - RMSE: 0.7514 - val_loss: 0.8548 - val_RMSE: 0.7662\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8535 - RMSE: 0.7627 - val_loss: 0.8546 - val_RMSE: 0.7662\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8484 - RMSE: 0.7578 - val_loss: 0.8544 - val_RMSE: 0.7662\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8370 - RMSE: 0.7466 - val_loss: 0.8542 - val_RMSE: 0.7661\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8515 - RMSE: 0.7612 - val_loss: 0.8540 - val_RMSE: 0.7661\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8292 - RMSE: 0.7391 - val_loss: 0.8538 - val_RMSE: 0.7661\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8471 - RMSE: 0.7572 - val_loss: 0.8536 - val_RMSE: 0.7661\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8388 - RMSE: 0.7491 - val_loss: 0.8534 - val_RMSE: 0.7661\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8504 - RMSE: 0.7609 - val_loss: 0.8532 - val_RMSE: 0.7661\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8382 - RMSE: 0.7488 - val_loss: 0.8530 - val_RMSE: 0.7661\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8445 - RMSE: 0.7554 - val_loss: 0.8528 - val_RMSE: 0.7661\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8389 - RMSE: 0.7499 - val_loss: 0.8526 - val_RMSE: 0.7660\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8493 - RMSE: 0.7605 - val_loss: 0.8524 - val_RMSE: 0.7660\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8442 - RMSE: 0.7556 - val_loss: 0.8522 - val_RMSE: 0.7660\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8314 - RMSE: 0.7430 - val_loss: 0.8520 - val_RMSE: 0.7660\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8229 - RMSE: 0.7346 - val_loss: 0.8519 - val_RMSE: 0.7660\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8379 - RMSE: 0.7498 - val_loss: 0.8517 - val_RMSE: 0.7660\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8405 - RMSE: 0.7526 - val_loss: 0.8515 - val_RMSE: 0.7660\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8474 - RMSE: 0.7596 - val_loss: 0.8513 - val_RMSE: 0.7660\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8424 - RMSE: 0.7549 - val_loss: 0.8511 - val_RMSE: 0.7660\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8340 - RMSE: 0.7466 - val_loss: 0.8509 - val_RMSE: 0.7660\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8334 - RMSE: 0.7462 - val_loss: 0.8507 - val_RMSE: 0.7659\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8342 - RMSE: 0.7472 - val_loss: 0.8505 - val_RMSE: 0.7659\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8432 - RMSE: 0.7563 - val_loss: 0.8504 - val_RMSE: 0.7659\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8414 - RMSE: 0.7547 - val_loss: 0.8502 - val_RMSE: 0.7659\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8437 - RMSE: 0.7572 - val_loss: 0.8500 - val_RMSE: 0.7659\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8478 - RMSE: 0.7615 - val_loss: 0.8498 - val_RMSE: 0.7659\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8388 - RMSE: 0.7527 - val_loss: 0.8496 - val_RMSE: 0.7659\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8416 - RMSE: 0.7556 - val_loss: 0.8494 - val_RMSE: 0.7659\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8283 - RMSE: 0.7424 - val_loss: 0.8493 - val_RMSE: 0.7659\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8411 - RMSE: 0.7554 - val_loss: 0.8491 - val_RMSE: 0.7658\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8352 - RMSE: 0.7497 - val_loss: 0.8489 - val_RMSE: 0.7658\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8416 - RMSE: 0.7563 - val_loss: 0.8487 - val_RMSE: 0.7658\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8354 - RMSE: 0.7502 - val_loss: 0.8485 - val_RMSE: 0.7658\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8378 - RMSE: 0.7529 - val_loss: 0.8484 - val_RMSE: 0.7658\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8404 - RMSE: 0.7556 - val_loss: 0.8482 - val_RMSE: 0.7658\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8434 - RMSE: 0.7587 - val_loss: 0.8480 - val_RMSE: 0.7658\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8372 - RMSE: 0.7527 - val_loss: 0.8478 - val_RMSE: 0.7658\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8525 - RMSE: 0.7682 - val_loss: 0.8476 - val_RMSE: 0.7658\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8298 - RMSE: 0.7457 - val_loss: 0.8475 - val_RMSE: 0.7658\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8407 - RMSE: 0.7568 - val_loss: 0.8473 - val_RMSE: 0.7657\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8434 - RMSE: 0.7596 - val_loss: 0.8471 - val_RMSE: 0.7657\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8482 - RMSE: 0.7645 - val_loss: 0.8469 - val_RMSE: 0.7657\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8370 - RMSE: 0.7535 - val_loss: 0.8468 - val_RMSE: 0.7657\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8348 - RMSE: 0.7515 - val_loss: 0.8466 - val_RMSE: 0.7657\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8288 - RMSE: 0.7457 - val_loss: 0.8464 - val_RMSE: 0.7657\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8249 - RMSE: 0.7419 - val_loss: 0.8462 - val_RMSE: 0.7657\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8401 - RMSE: 0.7573 - val_loss: 0.8461 - val_RMSE: 0.7657\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8369 - RMSE: 0.7543 - val_loss: 0.8459 - val_RMSE: 0.7657\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8307 - RMSE: 0.7482 - val_loss: 0.8457 - val_RMSE: 0.7657\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8487 - RMSE: 0.7664 - val_loss: 0.8455 - val_RMSE: 0.7656\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8388 - RMSE: 0.7566 - val_loss: 0.8454 - val_RMSE: 0.7656\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8374 - RMSE: 0.7554 - val_loss: 0.8452 - val_RMSE: 0.7656\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8383 - RMSE: 0.7565 - val_loss: 0.8450 - val_RMSE: 0.7656\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8448 - RMSE: 0.7631 - val_loss: 0.8449 - val_RMSE: 0.7656\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8297 - RMSE: 0.7482 - val_loss: 0.8447 - val_RMSE: 0.7656\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8327 - RMSE: 0.7513 - val_loss: 0.8445 - val_RMSE: 0.7656\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8339 - RMSE: 0.7527 - val_loss: 0.8443 - val_RMSE: 0.7656\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8381 - RMSE: 0.7571 - val_loss: 0.8442 - val_RMSE: 0.7656\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8348 - RMSE: 0.7540 - val_loss: 0.8440 - val_RMSE: 0.7656\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8375 - RMSE: 0.7568 - val_loss: 0.8438 - val_RMSE: 0.7656\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8389 - RMSE: 0.7583 - val_loss: 0.8437 - val_RMSE: 0.7656\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8327 - RMSE: 0.7523 - val_loss: 0.8435 - val_RMSE: 0.7655\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8346 - RMSE: 0.7543 - val_loss: 0.8433 - val_RMSE: 0.7655\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8289 - RMSE: 0.7488 - val_loss: 0.8432 - val_RMSE: 0.7655\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8444 - RMSE: 0.7645 - val_loss: 0.8430 - val_RMSE: 0.7655\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8337 - RMSE: 0.7539 - val_loss: 0.8428 - val_RMSE: 0.7655\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8276 - RMSE: 0.7480 - val_loss: 0.8427 - val_RMSE: 0.7655\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8183 - RMSE: 0.7388 - val_loss: 0.8425 - val_RMSE: 0.7655\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8326 - RMSE: 0.7533 - val_loss: 0.8423 - val_RMSE: 0.7655\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8323 - RMSE: 0.7531 - val_loss: 0.8422 - val_RMSE: 0.7655\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8416 - RMSE: 0.7626 - val_loss: 0.8420 - val_RMSE: 0.7655\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8398 - RMSE: 0.7610 - val_loss: 0.8419 - val_RMSE: 0.7655\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8341 - RMSE: 0.7554 - val_loss: 0.8417 - val_RMSE: 0.7654\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8384 - RMSE: 0.7599 - val_loss: 0.8415 - val_RMSE: 0.7654\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8343 - RMSE: 0.7559 - val_loss: 0.8414 - val_RMSE: 0.7654\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8267 - RMSE: 0.7484 - val_loss: 0.8412 - val_RMSE: 0.7654\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8282 - RMSE: 0.7501 - val_loss: 0.8410 - val_RMSE: 0.7654\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8310 - RMSE: 0.7531 - val_loss: 0.8409 - val_RMSE: 0.7654\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8261 - RMSE: 0.7483 - val_loss: 0.8407 - val_RMSE: 0.7654\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8304 - RMSE: 0.7528 - val_loss: 0.8406 - val_RMSE: 0.7654\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8306 - RMSE: 0.7531 - val_loss: 0.8404 - val_RMSE: 0.7654\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8337 - RMSE: 0.7564 - val_loss: 0.8403 - val_RMSE: 0.7654\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8334 - RMSE: 0.7562 - val_loss: 0.8401 - val_RMSE: 0.7654\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8281 - RMSE: 0.7511 - val_loss: 0.8399 - val_RMSE: 0.7654\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8305 - RMSE: 0.7536 - val_loss: 0.8398 - val_RMSE: 0.7654\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8369 - RMSE: 0.7601 - val_loss: 0.8396 - val_RMSE: 0.7653\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8351 - RMSE: 0.7585 - val_loss: 0.8395 - val_RMSE: 0.7653\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8263 - RMSE: 0.7499 - val_loss: 0.8393 - val_RMSE: 0.7653\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8331 - RMSE: 0.7568 - val_loss: 0.8392 - val_RMSE: 0.7653\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8248 - RMSE: 0.7486 - val_loss: 0.8390 - val_RMSE: 0.7653\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8298 - RMSE: 0.7538 - val_loss: 0.8388 - val_RMSE: 0.7653\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8342 - RMSE: 0.7584 - val_loss: 0.8387 - val_RMSE: 0.7653\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8319 - RMSE: 0.7562 - val_loss: 0.8385 - val_RMSE: 0.7653\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8260 - RMSE: 0.7504 - val_loss: 0.8384 - val_RMSE: 0.7653\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8310 - RMSE: 0.7555 - val_loss: 0.8382 - val_RMSE: 0.7653\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8334 - RMSE: 0.7581 - val_loss: 0.8381 - val_RMSE: 0.7653\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8237 - RMSE: 0.7485 - val_loss: 0.8379 - val_RMSE: 0.7653\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8272 - RMSE: 0.7522 - val_loss: 0.8378 - val_RMSE: 0.7653\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8345 - RMSE: 0.7597 - val_loss: 0.8376 - val_RMSE: 0.7653\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8279 - RMSE: 0.7532 - val_loss: 0.8375 - val_RMSE: 0.7652\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8280 - RMSE: 0.7535 - val_loss: 0.8373 - val_RMSE: 0.7652\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8310 - RMSE: 0.7566 - val_loss: 0.8372 - val_RMSE: 0.7652\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8239 - RMSE: 0.7496 - val_loss: 0.8370 - val_RMSE: 0.7652\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8305 - RMSE: 0.7564 - val_loss: 0.8369 - val_RMSE: 0.7652\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8187 - RMSE: 0.7447 - val_loss: 0.8367 - val_RMSE: 0.7652\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8270 - RMSE: 0.7531 - val_loss: 0.8366 - val_RMSE: 0.7652\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8312 - RMSE: 0.7575 - val_loss: 0.8364 - val_RMSE: 0.7652\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8225 - RMSE: 0.7489 - val_loss: 0.8363 - val_RMSE: 0.7652\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8293 - RMSE: 0.7558 - val_loss: 0.8361 - val_RMSE: 0.7652\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8278 - RMSE: 0.7545 - val_loss: 0.8360 - val_RMSE: 0.7652\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8200 - RMSE: 0.7468 - val_loss: 0.8358 - val_RMSE: 0.7652\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8217 - RMSE: 0.7486 - val_loss: 0.8357 - val_RMSE: 0.7652\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8177 - RMSE: 0.7448 - val_loss: 0.8355 - val_RMSE: 0.7651\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8332 - RMSE: 0.7604 - val_loss: 0.8354 - val_RMSE: 0.7651\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8305 - RMSE: 0.7579 - val_loss: 0.8352 - val_RMSE: 0.7651\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8250 - RMSE: 0.7525 - val_loss: 0.8351 - val_RMSE: 0.7651\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8214 - RMSE: 0.7491 - val_loss: 0.8349 - val_RMSE: 0.7651\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8188 - RMSE: 0.7466 - val_loss: 0.8348 - val_RMSE: 0.7651\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8288 - RMSE: 0.7567 - val_loss: 0.8347 - val_RMSE: 0.7651\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8200 - RMSE: 0.7481 - val_loss: 0.8345 - val_RMSE: 0.7651\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8132 - RMSE: 0.7414 - val_loss: 0.8344 - val_RMSE: 0.7651\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8229 - RMSE: 0.7512 - val_loss: 0.8342 - val_RMSE: 0.7651\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8260 - RMSE: 0.7545 - val_loss: 0.8341 - val_RMSE: 0.7651\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8245 - RMSE: 0.7531 - val_loss: 0.8339 - val_RMSE: 0.7651\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8279 - RMSE: 0.7567 - val_loss: 0.8338 - val_RMSE: 0.7651\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8244 - RMSE: 0.7533 - val_loss: 0.8337 - val_RMSE: 0.7651\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8169 - RMSE: 0.7459 - val_loss: 0.8335 - val_RMSE: 0.7650\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8279 - RMSE: 0.7570 - val_loss: 0.8334 - val_RMSE: 0.7650\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8159 - RMSE: 0.7452 - val_loss: 0.8332 - val_RMSE: 0.7650\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8280 - RMSE: 0.7574 - val_loss: 0.8331 - val_RMSE: 0.7650\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8241 - RMSE: 0.7536 - val_loss: 0.8330 - val_RMSE: 0.7650\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8181 - RMSE: 0.7478 - val_loss: 0.8328 - val_RMSE: 0.7650\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8162 - RMSE: 0.7461 - val_loss: 0.8327 - val_RMSE: 0.7650\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8254 - RMSE: 0.7554 - val_loss: 0.8325 - val_RMSE: 0.7650\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8167 - RMSE: 0.7467 - val_loss: 0.8324 - val_RMSE: 0.7650\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8198 - RMSE: 0.7500 - val_loss: 0.8323 - val_RMSE: 0.7650\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8142 - RMSE: 0.7446 - val_loss: 0.8321 - val_RMSE: 0.7650\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8224 - RMSE: 0.7529 - val_loss: 0.8320 - val_RMSE: 0.7650\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8266 - RMSE: 0.7572 - val_loss: 0.8319 - val_RMSE: 0.7650\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8222 - RMSE: 0.7529 - val_loss: 0.8317 - val_RMSE: 0.7650\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8181 - RMSE: 0.7490 - val_loss: 0.8316 - val_RMSE: 0.7650\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8237 - RMSE: 0.7547 - val_loss: 0.8314 - val_RMSE: 0.7649\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8337 - RMSE: 0.7648 - val_loss: 0.8313 - val_RMSE: 0.7649\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8246 - RMSE: 0.7558 - val_loss: 0.8312 - val_RMSE: 0.7649\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8171 - RMSE: 0.7485 - val_loss: 0.8310 - val_RMSE: 0.7649\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8297 - RMSE: 0.7612 - val_loss: 0.8309 - val_RMSE: 0.7649\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8223 - RMSE: 0.7539 - val_loss: 0.8308 - val_RMSE: 0.7649\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8264 - RMSE: 0.7581 - val_loss: 0.8306 - val_RMSE: 0.7649\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8321 - RMSE: 0.7640 - val_loss: 0.8305 - val_RMSE: 0.7649\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8198 - RMSE: 0.7518 - val_loss: 0.8304 - val_RMSE: 0.7649\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8153 - RMSE: 0.7475 - val_loss: 0.8302 - val_RMSE: 0.7649\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8281 - RMSE: 0.7604 - val_loss: 0.8301 - val_RMSE: 0.7649\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8270 - RMSE: 0.7594 - val_loss: 0.8300 - val_RMSE: 0.7649\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8219 - RMSE: 0.7544 - val_loss: 0.8298 - val_RMSE: 0.7649\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8231 - RMSE: 0.7558 - val_loss: 0.8297 - val_RMSE: 0.7649\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8206 - RMSE: 0.7534 - val_loss: 0.8296 - val_RMSE: 0.7649\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8269 - RMSE: 0.7598 - val_loss: 0.8295 - val_RMSE: 0.7649\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8189 - RMSE: 0.7519 - val_loss: 0.8293 - val_RMSE: 0.7649\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8157 - RMSE: 0.7488 - val_loss: 0.8292 - val_RMSE: 0.7648\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8210 - RMSE: 0.7542 - val_loss: 0.8291 - val_RMSE: 0.7648\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8107 - RMSE: 0.7441 - val_loss: 0.8289 - val_RMSE: 0.7648\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8224 - RMSE: 0.7559 - val_loss: 0.8288 - val_RMSE: 0.7648\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8153 - RMSE: 0.7490 - val_loss: 0.8287 - val_RMSE: 0.7648\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8083 - RMSE: 0.7421 - val_loss: 0.8285 - val_RMSE: 0.7648\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8257 - RMSE: 0.7596 - val_loss: 0.8284 - val_RMSE: 0.7648\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8186 - RMSE: 0.7526 - val_loss: 0.8283 - val_RMSE: 0.7648\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8298 - RMSE: 0.7639 - val_loss: 0.8282 - val_RMSE: 0.7648\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8120 - RMSE: 0.7462 - val_loss: 0.8280 - val_RMSE: 0.7648\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8208 - RMSE: 0.7552 - val_loss: 0.8279 - val_RMSE: 0.7648\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8171 - RMSE: 0.7516 - val_loss: 0.8278 - val_RMSE: 0.7648\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8284 - RMSE: 0.7630 - val_loss: 0.8276 - val_RMSE: 0.7648\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8102 - RMSE: 0.7449 - val_loss: 0.8275 - val_RMSE: 0.7648\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8134 - RMSE: 0.7483 - val_loss: 0.8274 - val_RMSE: 0.7648\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8096 - RMSE: 0.7446 - val_loss: 0.8273 - val_RMSE: 0.7648\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8283 - RMSE: 0.7634 - val_loss: 0.8271 - val_RMSE: 0.7648\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8217 - RMSE: 0.7569 - val_loss: 0.8270 - val_RMSE: 0.7647\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8059 - RMSE: 0.7412 - val_loss: 0.8269 - val_RMSE: 0.7647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=12 / Init = GlorotUniform / min_loss = 0.7647413015365601\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotUniform\n",
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_41 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_42 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_80 (Embedding)        (None, 1, 13)        377         input_41[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_81 (Embedding)        (None, 1, 13)        2106        input_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_20 (Dot)                    (None, 1, 1)         0           embedding_80[0][0]               \n",
            "                                                                 embedding_81[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_82 (Embedding)        (None, 1, 1)         29          input_41[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_83 (Embedding)        (None, 1, 1)         162         input_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 1, 1)         0           dot_20[0][0]                     \n",
            "                                                                 embedding_82[0][0]               \n",
            "                                                                 embedding_83[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_20 (Flatten)            (None, 1)            0           add_20[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,674\n",
            "Trainable params: 2,674\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.1025 - RMSE: 0.7849 - val_loss: 0.8930 - val_RMSE: 0.7689\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8834 - RMSE: 0.7558 - val_loss: 0.8903 - val_RMSE: 0.7687\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8819 - RMSE: 0.7555 - val_loss: 0.8901 - val_RMSE: 0.7687\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8880 - RMSE: 0.7619 - val_loss: 0.8898 - val_RMSE: 0.7687\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8815 - RMSE: 0.7556 - val_loss: 0.8895 - val_RMSE: 0.7687\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8913 - RMSE: 0.7657 - val_loss: 0.8892 - val_RMSE: 0.7686\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8834 - RMSE: 0.7580 - val_loss: 0.8889 - val_RMSE: 0.7686\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8811 - RMSE: 0.7560 - val_loss: 0.8886 - val_RMSE: 0.7686\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8783 - RMSE: 0.7534 - val_loss: 0.8883 - val_RMSE: 0.7686\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8808 - RMSE: 0.7563 - val_loss: 0.8881 - val_RMSE: 0.7685\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8786 - RMSE: 0.7543 - val_loss: 0.8878 - val_RMSE: 0.7685\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8850 - RMSE: 0.7610 - val_loss: 0.8875 - val_RMSE: 0.7685\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8778 - RMSE: 0.7541 - val_loss: 0.8872 - val_RMSE: 0.7685\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8848 - RMSE: 0.7613 - val_loss: 0.8870 - val_RMSE: 0.7685\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8833 - RMSE: 0.7601 - val_loss: 0.8867 - val_RMSE: 0.7684\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8779 - RMSE: 0.7550 - val_loss: 0.8864 - val_RMSE: 0.7684\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8806 - RMSE: 0.7579 - val_loss: 0.8861 - val_RMSE: 0.7684\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8841 - RMSE: 0.7617 - val_loss: 0.8858 - val_RMSE: 0.7684\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8797 - RMSE: 0.7576 - val_loss: 0.8856 - val_RMSE: 0.7684\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8768 - RMSE: 0.7549 - val_loss: 0.8853 - val_RMSE: 0.7683\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8886 - RMSE: 0.7669 - val_loss: 0.8850 - val_RMSE: 0.7683\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8769 - RMSE: 0.7555 - val_loss: 0.8848 - val_RMSE: 0.7683\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8738 - RMSE: 0.7527 - val_loss: 0.8845 - val_RMSE: 0.7683\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8874 - RMSE: 0.7665 - val_loss: 0.8842 - val_RMSE: 0.7683\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8789 - RMSE: 0.7583 - val_loss: 0.8839 - val_RMSE: 0.7682\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8883 - RMSE: 0.7679 - val_loss: 0.8837 - val_RMSE: 0.7682\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8728 - RMSE: 0.7527 - val_loss: 0.8834 - val_RMSE: 0.7682\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8686 - RMSE: 0.7488 - val_loss: 0.8831 - val_RMSE: 0.7682\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8734 - RMSE: 0.7538 - val_loss: 0.8829 - val_RMSE: 0.7682\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8736 - RMSE: 0.7543 - val_loss: 0.8826 - val_RMSE: 0.7682\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8829 - RMSE: 0.7638 - val_loss: 0.8823 - val_RMSE: 0.7681\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8737 - RMSE: 0.7548 - val_loss: 0.8821 - val_RMSE: 0.7681\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8803 - RMSE: 0.7617 - val_loss: 0.8818 - val_RMSE: 0.7681\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8687 - RMSE: 0.7504 - val_loss: 0.8815 - val_RMSE: 0.7681\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8744 - RMSE: 0.7563 - val_loss: 0.8813 - val_RMSE: 0.7681\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8681 - RMSE: 0.7503 - val_loss: 0.8810 - val_RMSE: 0.7680\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8698 - RMSE: 0.7522 - val_loss: 0.8808 - val_RMSE: 0.7680\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8811 - RMSE: 0.7638 - val_loss: 0.8805 - val_RMSE: 0.7680\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8785 - RMSE: 0.7614 - val_loss: 0.8802 - val_RMSE: 0.7680\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8667 - RMSE: 0.7498 - val_loss: 0.8800 - val_RMSE: 0.7680\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8778 - RMSE: 0.7612 - val_loss: 0.8797 - val_RMSE: 0.7680\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8623 - RMSE: 0.7460 - val_loss: 0.8795 - val_RMSE: 0.7679\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8774 - RMSE: 0.7613 - val_loss: 0.8792 - val_RMSE: 0.7679\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8674 - RMSE: 0.7515 - val_loss: 0.8789 - val_RMSE: 0.7679\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8773 - RMSE: 0.7617 - val_loss: 0.8787 - val_RMSE: 0.7679\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8748 - RMSE: 0.7594 - val_loss: 0.8784 - val_RMSE: 0.7679\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8712 - RMSE: 0.7561 - val_loss: 0.8782 - val_RMSE: 0.7678\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8795 - RMSE: 0.7645 - val_loss: 0.8779 - val_RMSE: 0.7678\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8665 - RMSE: 0.7519 - val_loss: 0.8777 - val_RMSE: 0.7678\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8627 - RMSE: 0.7483 - val_loss: 0.8774 - val_RMSE: 0.7678\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8785 - RMSE: 0.7643 - val_loss: 0.8772 - val_RMSE: 0.7678\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8636 - RMSE: 0.7496 - val_loss: 0.8769 - val_RMSE: 0.7678\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8714 - RMSE: 0.7577 - val_loss: 0.8767 - val_RMSE: 0.7677\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8602 - RMSE: 0.7467 - val_loss: 0.8764 - val_RMSE: 0.7677\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8726 - RMSE: 0.7593 - val_loss: 0.8762 - val_RMSE: 0.7677\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8591 - RMSE: 0.7461 - val_loss: 0.8759 - val_RMSE: 0.7677\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 8ms/step - loss: 0.8650 - RMSE: 0.7522 - val_loss: 0.8757 - val_RMSE: 0.7677\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8678 - RMSE: 0.7552 - val_loss: 0.8754 - val_RMSE: 0.7677\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8643 - RMSE: 0.7520 - val_loss: 0.8752 - val_RMSE: 0.7676\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8578 - RMSE: 0.7458 - val_loss: 0.8749 - val_RMSE: 0.7676\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8809 - RMSE: 0.7691 - val_loss: 0.8747 - val_RMSE: 0.7676\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8668 - RMSE: 0.7552 - val_loss: 0.8744 - val_RMSE: 0.7676\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8761 - RMSE: 0.7647 - val_loss: 0.8742 - val_RMSE: 0.7676\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8575 - RMSE: 0.7464 - val_loss: 0.8739 - val_RMSE: 0.7676\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8604 - RMSE: 0.7495 - val_loss: 0.8737 - val_RMSE: 0.7675\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8637 - RMSE: 0.7531 - val_loss: 0.8735 - val_RMSE: 0.7675\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8581 - RMSE: 0.7477 - val_loss: 0.8732 - val_RMSE: 0.7675\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8641 - RMSE: 0.7539 - val_loss: 0.8730 - val_RMSE: 0.7675\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8646 - RMSE: 0.7546 - val_loss: 0.8727 - val_RMSE: 0.7675\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8626 - RMSE: 0.7529 - val_loss: 0.8725 - val_RMSE: 0.7675\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8517 - RMSE: 0.7422 - val_loss: 0.8723 - val_RMSE: 0.7675\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8663 - RMSE: 0.7571 - val_loss: 0.8720 - val_RMSE: 0.7674\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8546 - RMSE: 0.7455 - val_loss: 0.8718 - val_RMSE: 0.7674\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8626 - RMSE: 0.7537 - val_loss: 0.8715 - val_RMSE: 0.7674\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8616 - RMSE: 0.7530 - val_loss: 0.8713 - val_RMSE: 0.7674\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8670 - RMSE: 0.7586 - val_loss: 0.8711 - val_RMSE: 0.7674\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8612 - RMSE: 0.7531 - val_loss: 0.8708 - val_RMSE: 0.7673\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8610 - RMSE: 0.7531 - val_loss: 0.8706 - val_RMSE: 0.7673\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8586 - RMSE: 0.7509 - val_loss: 0.8704 - val_RMSE: 0.7673\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8625 - RMSE: 0.7550 - val_loss: 0.8701 - val_RMSE: 0.7673\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8562 - RMSE: 0.7489 - val_loss: 0.8699 - val_RMSE: 0.7673\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8634 - RMSE: 0.7563 - val_loss: 0.8697 - val_RMSE: 0.7673\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8594 - RMSE: 0.7526 - val_loss: 0.8694 - val_RMSE: 0.7673\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8588 - RMSE: 0.7522 - val_loss: 0.8692 - val_RMSE: 0.7673\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8530 - RMSE: 0.7466 - val_loss: 0.8690 - val_RMSE: 0.7672\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8576 - RMSE: 0.7515 - val_loss: 0.8688 - val_RMSE: 0.7672\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8680 - RMSE: 0.7621 - val_loss: 0.8685 - val_RMSE: 0.7672\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8528 - RMSE: 0.7471 - val_loss: 0.8683 - val_RMSE: 0.7672\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8608 - RMSE: 0.7553 - val_loss: 0.8681 - val_RMSE: 0.7672\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8595 - RMSE: 0.7542 - val_loss: 0.8679 - val_RMSE: 0.7672\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8544 - RMSE: 0.7493 - val_loss: 0.8676 - val_RMSE: 0.7671\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8583 - RMSE: 0.7534 - val_loss: 0.8674 - val_RMSE: 0.7671\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8547 - RMSE: 0.7501 - val_loss: 0.8672 - val_RMSE: 0.7671\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8594 - RMSE: 0.7550 - val_loss: 0.8669 - val_RMSE: 0.7671\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8656 - RMSE: 0.7614 - val_loss: 0.8667 - val_RMSE: 0.7671\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8509 - RMSE: 0.7469 - val_loss: 0.8665 - val_RMSE: 0.7671\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8638 - RMSE: 0.7600 - val_loss: 0.8663 - val_RMSE: 0.7671\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8492 - RMSE: 0.7456 - val_loss: 0.8661 - val_RMSE: 0.7670\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8470 - RMSE: 0.7436 - val_loss: 0.8658 - val_RMSE: 0.7670\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8579 - RMSE: 0.7548 - val_loss: 0.8656 - val_RMSE: 0.7670\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8523 - RMSE: 0.7493 - val_loss: 0.8654 - val_RMSE: 0.7670\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8549 - RMSE: 0.7521 - val_loss: 0.8652 - val_RMSE: 0.7670\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8484 - RMSE: 0.7459 - val_loss: 0.8649 - val_RMSE: 0.7670\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8526 - RMSE: 0.7502 - val_loss: 0.8647 - val_RMSE: 0.7670\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8551 - RMSE: 0.7530 - val_loss: 0.8645 - val_RMSE: 0.7669\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8533 - RMSE: 0.7514 - val_loss: 0.8643 - val_RMSE: 0.7669\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8523 - RMSE: 0.7506 - val_loss: 0.8641 - val_RMSE: 0.7669\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8557 - RMSE: 0.7543 - val_loss: 0.8639 - val_RMSE: 0.7669\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8495 - RMSE: 0.7483 - val_loss: 0.8636 - val_RMSE: 0.7669\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8475 - RMSE: 0.7465 - val_loss: 0.8634 - val_RMSE: 0.7669\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8521 - RMSE: 0.7512 - val_loss: 0.8632 - val_RMSE: 0.7669\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8545 - RMSE: 0.7538 - val_loss: 0.8630 - val_RMSE: 0.7669\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8510 - RMSE: 0.7505 - val_loss: 0.8628 - val_RMSE: 0.7668\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8599 - RMSE: 0.7597 - val_loss: 0.8626 - val_RMSE: 0.7668\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8519 - RMSE: 0.7519 - val_loss: 0.8624 - val_RMSE: 0.7668\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8513 - RMSE: 0.7515 - val_loss: 0.8621 - val_RMSE: 0.7668\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8544 - RMSE: 0.7548 - val_loss: 0.8619 - val_RMSE: 0.7668\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8617 - RMSE: 0.7623 - val_loss: 0.8617 - val_RMSE: 0.7668\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8434 - RMSE: 0.7442 - val_loss: 0.8615 - val_RMSE: 0.7668\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8545 - RMSE: 0.7555 - val_loss: 0.8613 - val_RMSE: 0.7667\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8641 - RMSE: 0.7652 - val_loss: 0.8611 - val_RMSE: 0.7667\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8487 - RMSE: 0.7501 - val_loss: 0.8609 - val_RMSE: 0.7667\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8531 - RMSE: 0.7547 - val_loss: 0.8607 - val_RMSE: 0.7667\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8589 - RMSE: 0.7607 - val_loss: 0.8605 - val_RMSE: 0.7667\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8457 - RMSE: 0.7477 - val_loss: 0.8603 - val_RMSE: 0.7667\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8538 - RMSE: 0.7560 - val_loss: 0.8601 - val_RMSE: 0.7667\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8477 - RMSE: 0.7500 - val_loss: 0.8598 - val_RMSE: 0.7667\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8436 - RMSE: 0.7462 - val_loss: 0.8596 - val_RMSE: 0.7667\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8479 - RMSE: 0.7507 - val_loss: 0.8594 - val_RMSE: 0.7666\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8552 - RMSE: 0.7582 - val_loss: 0.8592 - val_RMSE: 0.7666\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8360 - RMSE: 0.7392 - val_loss: 0.8590 - val_RMSE: 0.7666\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8632 - RMSE: 0.7665 - val_loss: 0.8588 - val_RMSE: 0.7666\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8463 - RMSE: 0.7499 - val_loss: 0.8586 - val_RMSE: 0.7666\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8509 - RMSE: 0.7546 - val_loss: 0.8584 - val_RMSE: 0.7666\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8520 - RMSE: 0.7560 - val_loss: 0.8582 - val_RMSE: 0.7666\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8549 - RMSE: 0.7591 - val_loss: 0.8580 - val_RMSE: 0.7665\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8541 - RMSE: 0.7584 - val_loss: 0.8578 - val_RMSE: 0.7665\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8437 - RMSE: 0.7483 - val_loss: 0.8576 - val_RMSE: 0.7665\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8460 - RMSE: 0.7507 - val_loss: 0.8574 - val_RMSE: 0.7665\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8571 - RMSE: 0.7620 - val_loss: 0.8572 - val_RMSE: 0.7665\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8426 - RMSE: 0.7478 - val_loss: 0.8570 - val_RMSE: 0.7665\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8533 - RMSE: 0.7585 - val_loss: 0.8568 - val_RMSE: 0.7665\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8486 - RMSE: 0.7541 - val_loss: 0.8566 - val_RMSE: 0.7665\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8436 - RMSE: 0.7492 - val_loss: 0.8564 - val_RMSE: 0.7665\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8471 - RMSE: 0.7529 - val_loss: 0.8562 - val_RMSE: 0.7664\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8611 - RMSE: 0.7671 - val_loss: 0.8560 - val_RMSE: 0.7664\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8522 - RMSE: 0.7585 - val_loss: 0.8558 - val_RMSE: 0.7664\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8442 - RMSE: 0.7506 - val_loss: 0.8556 - val_RMSE: 0.7664\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8403 - RMSE: 0.7469 - val_loss: 0.8554 - val_RMSE: 0.7664\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8499 - RMSE: 0.7567 - val_loss: 0.8553 - val_RMSE: 0.7664\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8527 - RMSE: 0.7597 - val_loss: 0.8551 - val_RMSE: 0.7664\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8492 - RMSE: 0.7564 - val_loss: 0.8549 - val_RMSE: 0.7664\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8445 - RMSE: 0.7518 - val_loss: 0.8547 - val_RMSE: 0.7664\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8438 - RMSE: 0.7513 - val_loss: 0.8545 - val_RMSE: 0.7663\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8494 - RMSE: 0.7572 - val_loss: 0.8543 - val_RMSE: 0.7663\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8417 - RMSE: 0.7496 - val_loss: 0.8541 - val_RMSE: 0.7663\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8531 - RMSE: 0.7612 - val_loss: 0.8539 - val_RMSE: 0.7663\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8442 - RMSE: 0.7524 - val_loss: 0.8537 - val_RMSE: 0.7663\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8426 - RMSE: 0.7511 - val_loss: 0.8535 - val_RMSE: 0.7663\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8441 - RMSE: 0.7527 - val_loss: 0.8533 - val_RMSE: 0.7663\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8429 - RMSE: 0.7517 - val_loss: 0.8532 - val_RMSE: 0.7663\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8436 - RMSE: 0.7526 - val_loss: 0.8530 - val_RMSE: 0.7663\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8520 - RMSE: 0.7612 - val_loss: 0.8528 - val_RMSE: 0.7663\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8482 - RMSE: 0.7576 - val_loss: 0.8526 - val_RMSE: 0.7662\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8555 - RMSE: 0.7650 - val_loss: 0.8524 - val_RMSE: 0.7662\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8441 - RMSE: 0.7538 - val_loss: 0.8522 - val_RMSE: 0.7662\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8513 - RMSE: 0.7612 - val_loss: 0.8520 - val_RMSE: 0.7662\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8369 - RMSE: 0.7470 - val_loss: 0.8518 - val_RMSE: 0.7662\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8432 - RMSE: 0.7535 - val_loss: 0.8517 - val_RMSE: 0.7662\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8370 - RMSE: 0.7474 - val_loss: 0.8515 - val_RMSE: 0.7662\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8422 - RMSE: 0.7529 - val_loss: 0.8513 - val_RMSE: 0.7662\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8437 - RMSE: 0.7545 - val_loss: 0.8511 - val_RMSE: 0.7662\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8444 - RMSE: 0.7554 - val_loss: 0.8509 - val_RMSE: 0.7662\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8363 - RMSE: 0.7475 - val_loss: 0.8507 - val_RMSE: 0.7661\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8379 - RMSE: 0.7492 - val_loss: 0.8506 - val_RMSE: 0.7661\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8485 - RMSE: 0.7600 - val_loss: 0.8504 - val_RMSE: 0.7661\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8440 - RMSE: 0.7557 - val_loss: 0.8502 - val_RMSE: 0.7661\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8307 - RMSE: 0.7425 - val_loss: 0.8500 - val_RMSE: 0.7661\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8412 - RMSE: 0.7532 - val_loss: 0.8498 - val_RMSE: 0.7661\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8396 - RMSE: 0.7518 - val_loss: 0.8497 - val_RMSE: 0.7661\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8425 - RMSE: 0.7549 - val_loss: 0.8495 - val_RMSE: 0.7661\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8437 - RMSE: 0.7563 - val_loss: 0.8493 - val_RMSE: 0.7661\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8339 - RMSE: 0.7467 - val_loss: 0.8491 - val_RMSE: 0.7661\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8408 - RMSE: 0.7536 - val_loss: 0.8490 - val_RMSE: 0.7661\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8361 - RMSE: 0.7491 - val_loss: 0.8488 - val_RMSE: 0.7660\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8431 - RMSE: 0.7563 - val_loss: 0.8486 - val_RMSE: 0.7660\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8291 - RMSE: 0.7425 - val_loss: 0.8484 - val_RMSE: 0.7660\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8377 - RMSE: 0.7512 - val_loss: 0.8482 - val_RMSE: 0.7660\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8356 - RMSE: 0.7494 - val_loss: 0.8481 - val_RMSE: 0.7660\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8440 - RMSE: 0.7579 - val_loss: 0.8479 - val_RMSE: 0.7660\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8348 - RMSE: 0.7489 - val_loss: 0.8477 - val_RMSE: 0.7660\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8318 - RMSE: 0.7461 - val_loss: 0.8475 - val_RMSE: 0.7660\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8354 - RMSE: 0.7498 - val_loss: 0.8474 - val_RMSE: 0.7660\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8336 - RMSE: 0.7482 - val_loss: 0.8472 - val_RMSE: 0.7660\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8406 - RMSE: 0.7554 - val_loss: 0.8470 - val_RMSE: 0.7659\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8433 - RMSE: 0.7582 - val_loss: 0.8468 - val_RMSE: 0.7659\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8467 - RMSE: 0.7618 - val_loss: 0.8467 - val_RMSE: 0.7659\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8449 - RMSE: 0.7602 - val_loss: 0.8465 - val_RMSE: 0.7659\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8313 - RMSE: 0.7467 - val_loss: 0.8463 - val_RMSE: 0.7659\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8301 - RMSE: 0.7457 - val_loss: 0.8462 - val_RMSE: 0.7659\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8402 - RMSE: 0.7560 - val_loss: 0.8460 - val_RMSE: 0.7659\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8353 - RMSE: 0.7512 - val_loss: 0.8458 - val_RMSE: 0.7659\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8301 - RMSE: 0.7462 - val_loss: 0.8456 - val_RMSE: 0.7659\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8351 - RMSE: 0.7513 - val_loss: 0.8455 - val_RMSE: 0.7659\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8370 - RMSE: 0.7534 - val_loss: 0.8453 - val_RMSE: 0.7658\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8393 - RMSE: 0.7559 - val_loss: 0.8452 - val_RMSE: 0.7658\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8380 - RMSE: 0.7547 - val_loss: 0.8450 - val_RMSE: 0.7658\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8343 - RMSE: 0.7512 - val_loss: 0.8448 - val_RMSE: 0.7658\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8256 - RMSE: 0.7426 - val_loss: 0.8446 - val_RMSE: 0.7658\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8321 - RMSE: 0.7493 - val_loss: 0.8445 - val_RMSE: 0.7658\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8365 - RMSE: 0.7539 - val_loss: 0.8443 - val_RMSE: 0.7658\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8478 - RMSE: 0.7654 - val_loss: 0.8442 - val_RMSE: 0.7658\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8375 - RMSE: 0.7552 - val_loss: 0.8440 - val_RMSE: 0.7658\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8308 - RMSE: 0.7487 - val_loss: 0.8438 - val_RMSE: 0.7658\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8278 - RMSE: 0.7458 - val_loss: 0.8437 - val_RMSE: 0.7658\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8459 - RMSE: 0.7641 - val_loss: 0.8435 - val_RMSE: 0.7658\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8368 - RMSE: 0.7552 - val_loss: 0.8433 - val_RMSE: 0.7658\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8318 - RMSE: 0.7503 - val_loss: 0.8432 - val_RMSE: 0.7657\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8327 - RMSE: 0.7513 - val_loss: 0.8430 - val_RMSE: 0.7657\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8375 - RMSE: 0.7564 - val_loss: 0.8428 - val_RMSE: 0.7657\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8361 - RMSE: 0.7551 - val_loss: 0.8427 - val_RMSE: 0.7657\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8308 - RMSE: 0.7499 - val_loss: 0.8425 - val_RMSE: 0.7657\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8278 - RMSE: 0.7471 - val_loss: 0.8424 - val_RMSE: 0.7657\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8304 - RMSE: 0.7498 - val_loss: 0.8422 - val_RMSE: 0.7657\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8348 - RMSE: 0.7544 - val_loss: 0.8420 - val_RMSE: 0.7657\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8408 - RMSE: 0.7605 - val_loss: 0.8419 - val_RMSE: 0.7657\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8227 - RMSE: 0.7426 - val_loss: 0.8417 - val_RMSE: 0.7657\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8348 - RMSE: 0.7548 - val_loss: 0.8416 - val_RMSE: 0.7657\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8251 - RMSE: 0.7453 - val_loss: 0.8414 - val_RMSE: 0.7656\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8219 - RMSE: 0.7422 - val_loss: 0.8412 - val_RMSE: 0.7656\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8262 - RMSE: 0.7467 - val_loss: 0.8411 - val_RMSE: 0.7656\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8412 - RMSE: 0.7619 - val_loss: 0.8409 - val_RMSE: 0.7656\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8266 - RMSE: 0.7474 - val_loss: 0.8408 - val_RMSE: 0.7656\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8402 - RMSE: 0.7612 - val_loss: 0.8406 - val_RMSE: 0.7656\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8271 - RMSE: 0.7482 - val_loss: 0.8405 - val_RMSE: 0.7656\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8321 - RMSE: 0.7533 - val_loss: 0.8403 - val_RMSE: 0.7656\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8299 - RMSE: 0.7513 - val_loss: 0.8401 - val_RMSE: 0.7656\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8375 - RMSE: 0.7590 - val_loss: 0.8400 - val_RMSE: 0.7656\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8262 - RMSE: 0.7479 - val_loss: 0.8398 - val_RMSE: 0.7656\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8248 - RMSE: 0.7467 - val_loss: 0.8397 - val_RMSE: 0.7656\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8281 - RMSE: 0.7501 - val_loss: 0.8395 - val_RMSE: 0.7656\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8356 - RMSE: 0.7577 - val_loss: 0.8394 - val_RMSE: 0.7655\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8229 - RMSE: 0.7452 - val_loss: 0.8392 - val_RMSE: 0.7655\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8310 - RMSE: 0.7535 - val_loss: 0.8391 - val_RMSE: 0.7655\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8316 - RMSE: 0.7542 - val_loss: 0.8389 - val_RMSE: 0.7655\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8246 - RMSE: 0.7474 - val_loss: 0.8388 - val_RMSE: 0.7655\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8227 - RMSE: 0.7456 - val_loss: 0.8386 - val_RMSE: 0.7655\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8356 - RMSE: 0.7587 - val_loss: 0.8385 - val_RMSE: 0.7655\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8281 - RMSE: 0.7513 - val_loss: 0.8383 - val_RMSE: 0.7655\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8192 - RMSE: 0.7426 - val_loss: 0.8382 - val_RMSE: 0.7655\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8295 - RMSE: 0.7530 - val_loss: 0.8380 - val_RMSE: 0.7655\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8243 - RMSE: 0.7480 - val_loss: 0.8379 - val_RMSE: 0.7655\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8363 - RMSE: 0.7601 - val_loss: 0.8377 - val_RMSE: 0.7655\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8322 - RMSE: 0.7562 - val_loss: 0.8376 - val_RMSE: 0.7654\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8284 - RMSE: 0.7525 - val_loss: 0.8374 - val_RMSE: 0.7654\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8266 - RMSE: 0.7508 - val_loss: 0.8373 - val_RMSE: 0.7654\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8345 - RMSE: 0.7588 - val_loss: 0.8371 - val_RMSE: 0.7654\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8219 - RMSE: 0.7464 - val_loss: 0.8370 - val_RMSE: 0.7654\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8301 - RMSE: 0.7548 - val_loss: 0.8368 - val_RMSE: 0.7654\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8278 - RMSE: 0.7526 - val_loss: 0.8367 - val_RMSE: 0.7654\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8266 - RMSE: 0.7515 - val_loss: 0.8365 - val_RMSE: 0.7654\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8237 - RMSE: 0.7488 - val_loss: 0.8364 - val_RMSE: 0.7654\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8359 - RMSE: 0.7611 - val_loss: 0.8362 - val_RMSE: 0.7654\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8254 - RMSE: 0.7508 - val_loss: 0.8361 - val_RMSE: 0.7654\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8270 - RMSE: 0.7525 - val_loss: 0.8359 - val_RMSE: 0.7654\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8206 - RMSE: 0.7462 - val_loss: 0.8358 - val_RMSE: 0.7654\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8263 - RMSE: 0.7521 - val_loss: 0.8357 - val_RMSE: 0.7653\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8187 - RMSE: 0.7446 - val_loss: 0.8355 - val_RMSE: 0.7653\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8210 - RMSE: 0.7471 - val_loss: 0.8354 - val_RMSE: 0.7653\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8246 - RMSE: 0.7508 - val_loss: 0.8352 - val_RMSE: 0.7653\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8328 - RMSE: 0.7592 - val_loss: 0.8351 - val_RMSE: 0.7653\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8161 - RMSE: 0.7426 - val_loss: 0.8349 - val_RMSE: 0.7653\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8328 - RMSE: 0.7594 - val_loss: 0.8348 - val_RMSE: 0.7653\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8270 - RMSE: 0.7537 - val_loss: 0.8347 - val_RMSE: 0.7653\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8217 - RMSE: 0.7486 - val_loss: 0.8345 - val_RMSE: 0.7653\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8281 - RMSE: 0.7551 - val_loss: 0.8344 - val_RMSE: 0.7653\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8285 - RMSE: 0.7556 - val_loss: 0.8342 - val_RMSE: 0.7653\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8215 - RMSE: 0.7488 - val_loss: 0.8341 - val_RMSE: 0.7653\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8157 - RMSE: 0.7431 - val_loss: 0.8340 - val_RMSE: 0.7653\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8188 - RMSE: 0.7464 - val_loss: 0.8338 - val_RMSE: 0.7653\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8269 - RMSE: 0.7546 - val_loss: 0.8337 - val_RMSE: 0.7653\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8338 - RMSE: 0.7616 - val_loss: 0.8335 - val_RMSE: 0.7652\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8225 - RMSE: 0.7505 - val_loss: 0.8334 - val_RMSE: 0.7652\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8139 - RMSE: 0.7420 - val_loss: 0.8333 - val_RMSE: 0.7652\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8215 - RMSE: 0.7498 - val_loss: 0.8331 - val_RMSE: 0.7652\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8207 - RMSE: 0.7490 - val_loss: 0.8330 - val_RMSE: 0.7652\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8222 - RMSE: 0.7507 - val_loss: 0.8328 - val_RMSE: 0.7652\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8321 - RMSE: 0.7608 - val_loss: 0.8327 - val_RMSE: 0.7652\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8214 - RMSE: 0.7502 - val_loss: 0.8326 - val_RMSE: 0.7652\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8277 - RMSE: 0.7566 - val_loss: 0.8324 - val_RMSE: 0.7652\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8267 - RMSE: 0.7557 - val_loss: 0.8323 - val_RMSE: 0.7652\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8196 - RMSE: 0.7488 - val_loss: 0.8322 - val_RMSE: 0.7652\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8109 - RMSE: 0.7402 - val_loss: 0.8320 - val_RMSE: 0.7652\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8102 - RMSE: 0.7396 - val_loss: 0.8319 - val_RMSE: 0.7652\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8313 - RMSE: 0.7608 - val_loss: 0.8318 - val_RMSE: 0.7652\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8254 - RMSE: 0.7551 - val_loss: 0.8316 - val_RMSE: 0.7651\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8212 - RMSE: 0.7510 - val_loss: 0.8315 - val_RMSE: 0.7651\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8145 - RMSE: 0.7445 - val_loss: 0.8314 - val_RMSE: 0.7651\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8158 - RMSE: 0.7459 - val_loss: 0.8312 - val_RMSE: 0.7651\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8254 - RMSE: 0.7556 - val_loss: 0.8311 - val_RMSE: 0.7651\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=13 / Init = GlorotUniform / min_loss = 0.765123188495636\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotUniform\n",
            "Model: \"model_21\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_43 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_44 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_84 (Embedding)        (None, 1, 14)        406         input_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_85 (Embedding)        (None, 1, 14)        2268        input_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_21 (Dot)                    (None, 1, 1)         0           embedding_84[0][0]               \n",
            "                                                                 embedding_85[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_86 (Embedding)        (None, 1, 1)         29          input_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_87 (Embedding)        (None, 1, 1)         162         input_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 1, 1)         0           dot_21[0][0]                     \n",
            "                                                                 embedding_86[0][0]               \n",
            "                                                                 embedding_87[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_21 (Flatten)            (None, 1)            0           add_21[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,865\n",
            "Trainable params: 2,865\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.1146 - RMSE: 0.7848 - val_loss: 0.9037 - val_RMSE: 0.7752\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8982 - RMSE: 0.7673 - val_loss: 0.9009 - val_RMSE: 0.7750\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8997 - RMSE: 0.7700 - val_loss: 0.9005 - val_RMSE: 0.7749\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8969 - RMSE: 0.7674 - val_loss: 0.9002 - val_RMSE: 0.7749\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8878 - RMSE: 0.7586 - val_loss: 0.8999 - val_RMSE: 0.7749\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8873 - RMSE: 0.7583 - val_loss: 0.8996 - val_RMSE: 0.7748\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8876 - RMSE: 0.7589 - val_loss: 0.8993 - val_RMSE: 0.7748\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8869 - RMSE: 0.7585 - val_loss: 0.8989 - val_RMSE: 0.7747\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8845 - RMSE: 0.7564 - val_loss: 0.8986 - val_RMSE: 0.7747\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8820 - RMSE: 0.7541 - val_loss: 0.8983 - val_RMSE: 0.7747\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8817 - RMSE: 0.7541 - val_loss: 0.8980 - val_RMSE: 0.7746\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8860 - RMSE: 0.7587 - val_loss: 0.8977 - val_RMSE: 0.7746\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8911 - RMSE: 0.7641 - val_loss: 0.8974 - val_RMSE: 0.7745\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8895 - RMSE: 0.7628 - val_loss: 0.8971 - val_RMSE: 0.7745\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8863 - RMSE: 0.7598 - val_loss: 0.8968 - val_RMSE: 0.7745\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8934 - RMSE: 0.7671 - val_loss: 0.8965 - val_RMSE: 0.7744\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8809 - RMSE: 0.7549 - val_loss: 0.8962 - val_RMSE: 0.7744\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8897 - RMSE: 0.7640 - val_loss: 0.8959 - val_RMSE: 0.7744\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8824 - RMSE: 0.7570 - val_loss: 0.8956 - val_RMSE: 0.7743\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8793 - RMSE: 0.7542 - val_loss: 0.8953 - val_RMSE: 0.7743\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8837 - RMSE: 0.7589 - val_loss: 0.8950 - val_RMSE: 0.7742\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8756 - RMSE: 0.7510 - val_loss: 0.8947 - val_RMSE: 0.7742\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8851 - RMSE: 0.7608 - val_loss: 0.8944 - val_RMSE: 0.7742\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8787 - RMSE: 0.7547 - val_loss: 0.8941 - val_RMSE: 0.7741\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8827 - RMSE: 0.7589 - val_loss: 0.8938 - val_RMSE: 0.7741\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8835 - RMSE: 0.7599 - val_loss: 0.8935 - val_RMSE: 0.7740\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8862 - RMSE: 0.7629 - val_loss: 0.8932 - val_RMSE: 0.7740\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8739 - RMSE: 0.7509 - val_loss: 0.8929 - val_RMSE: 0.7740\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8830 - RMSE: 0.7603 - val_loss: 0.8926 - val_RMSE: 0.7739\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8803 - RMSE: 0.7579 - val_loss: 0.8923 - val_RMSE: 0.7739\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8761 - RMSE: 0.7539 - val_loss: 0.8920 - val_RMSE: 0.7739\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8865 - RMSE: 0.7646 - val_loss: 0.8917 - val_RMSE: 0.7738\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8786 - RMSE: 0.7569 - val_loss: 0.8914 - val_RMSE: 0.7738\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8748 - RMSE: 0.7534 - val_loss: 0.8911 - val_RMSE: 0.7737\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8768 - RMSE: 0.7557 - val_loss: 0.8908 - val_RMSE: 0.7737\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8845 - RMSE: 0.7636 - val_loss: 0.8905 - val_RMSE: 0.7737\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8738 - RMSE: 0.7531 - val_loss: 0.8902 - val_RMSE: 0.7736\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8767 - RMSE: 0.7563 - val_loss: 0.8899 - val_RMSE: 0.7736\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8779 - RMSE: 0.7577 - val_loss: 0.8897 - val_RMSE: 0.7736\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8809 - RMSE: 0.7610 - val_loss: 0.8894 - val_RMSE: 0.7735\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8835 - RMSE: 0.7638 - val_loss: 0.8891 - val_RMSE: 0.7735\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8773 - RMSE: 0.7579 - val_loss: 0.8888 - val_RMSE: 0.7735\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8721 - RMSE: 0.7530 - val_loss: 0.8885 - val_RMSE: 0.7734\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8820 - RMSE: 0.7631 - val_loss: 0.8882 - val_RMSE: 0.7734\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8800 - RMSE: 0.7614 - val_loss: 0.8880 - val_RMSE: 0.7734\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8690 - RMSE: 0.7506 - val_loss: 0.8877 - val_RMSE: 0.7733\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8689 - RMSE: 0.7507 - val_loss: 0.8874 - val_RMSE: 0.7733\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8769 - RMSE: 0.7590 - val_loss: 0.8871 - val_RMSE: 0.7733\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8713 - RMSE: 0.7536 - val_loss: 0.8868 - val_RMSE: 0.7732\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8787 - RMSE: 0.7613 - val_loss: 0.8865 - val_RMSE: 0.7732\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8712 - RMSE: 0.7541 - val_loss: 0.8863 - val_RMSE: 0.7732\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8782 - RMSE: 0.7613 - val_loss: 0.8860 - val_RMSE: 0.7731\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8758 - RMSE: 0.7592 - val_loss: 0.8857 - val_RMSE: 0.7731\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8721 - RMSE: 0.7557 - val_loss: 0.8854 - val_RMSE: 0.7731\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8759 - RMSE: 0.7598 - val_loss: 0.8852 - val_RMSE: 0.7730\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8763 - RMSE: 0.7604 - val_loss: 0.8849 - val_RMSE: 0.7730\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8794 - RMSE: 0.7637 - val_loss: 0.8846 - val_RMSE: 0.7730\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8760 - RMSE: 0.7606 - val_loss: 0.8843 - val_RMSE: 0.7729\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8757 - RMSE: 0.7606 - val_loss: 0.8841 - val_RMSE: 0.7729\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8745 - RMSE: 0.7596 - val_loss: 0.8838 - val_RMSE: 0.7729\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8674 - RMSE: 0.7527 - val_loss: 0.8835 - val_RMSE: 0.7728\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8670 - RMSE: 0.7526 - val_loss: 0.8833 - val_RMSE: 0.7728\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8718 - RMSE: 0.7576 - val_loss: 0.8830 - val_RMSE: 0.7728\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8683 - RMSE: 0.7544 - val_loss: 0.8827 - val_RMSE: 0.7727\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8625 - RMSE: 0.7488 - val_loss: 0.8825 - val_RMSE: 0.7727\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8672 - RMSE: 0.7537 - val_loss: 0.8822 - val_RMSE: 0.7727\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8622 - RMSE: 0.7490 - val_loss: 0.8819 - val_RMSE: 0.7726\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8692 - RMSE: 0.7562 - val_loss: 0.8817 - val_RMSE: 0.7726\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8629 - RMSE: 0.7502 - val_loss: 0.8814 - val_RMSE: 0.7726\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8663 - RMSE: 0.7538 - val_loss: 0.8811 - val_RMSE: 0.7725\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8648 - RMSE: 0.7525 - val_loss: 0.8809 - val_RMSE: 0.7725\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8557 - RMSE: 0.7437 - val_loss: 0.8806 - val_RMSE: 0.7725\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8686 - RMSE: 0.7568 - val_loss: 0.8803 - val_RMSE: 0.7725\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8616 - RMSE: 0.7500 - val_loss: 0.8801 - val_RMSE: 0.7724\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8694 - RMSE: 0.7580 - val_loss: 0.8798 - val_RMSE: 0.7724\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8691 - RMSE: 0.7579 - val_loss: 0.8796 - val_RMSE: 0.7724\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8739 - RMSE: 0.7630 - val_loss: 0.8793 - val_RMSE: 0.7723\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8705 - RMSE: 0.7599 - val_loss: 0.8790 - val_RMSE: 0.7723\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8661 - RMSE: 0.7557 - val_loss: 0.8788 - val_RMSE: 0.7723\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8682 - RMSE: 0.7580 - val_loss: 0.8785 - val_RMSE: 0.7722\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8638 - RMSE: 0.7538 - val_loss: 0.8783 - val_RMSE: 0.7722\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8668 - RMSE: 0.7571 - val_loss: 0.8780 - val_RMSE: 0.7722\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8636 - RMSE: 0.7541 - val_loss: 0.8778 - val_RMSE: 0.7722\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8681 - RMSE: 0.7588 - val_loss: 0.8775 - val_RMSE: 0.7721\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8560 - RMSE: 0.7469 - val_loss: 0.8773 - val_RMSE: 0.7721\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8608 - RMSE: 0.7520 - val_loss: 0.8770 - val_RMSE: 0.7721\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8673 - RMSE: 0.7588 - val_loss: 0.8767 - val_RMSE: 0.7720\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8596 - RMSE: 0.7512 - val_loss: 0.8765 - val_RMSE: 0.7720\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8607 - RMSE: 0.7526 - val_loss: 0.8762 - val_RMSE: 0.7720\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8455 - RMSE: 0.7376 - val_loss: 0.8760 - val_RMSE: 0.7720\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8559 - RMSE: 0.7482 - val_loss: 0.8757 - val_RMSE: 0.7719\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8589 - RMSE: 0.7514 - val_loss: 0.8755 - val_RMSE: 0.7719\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8648 - RMSE: 0.7576 - val_loss: 0.8752 - val_RMSE: 0.7719\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8698 - RMSE: 0.7628 - val_loss: 0.8750 - val_RMSE: 0.7718\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8548 - RMSE: 0.7480 - val_loss: 0.8748 - val_RMSE: 0.7718\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8621 - RMSE: 0.7555 - val_loss: 0.8745 - val_RMSE: 0.7718\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8594 - RMSE: 0.7530 - val_loss: 0.8743 - val_RMSE: 0.7718\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8570 - RMSE: 0.7509 - val_loss: 0.8740 - val_RMSE: 0.7717\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8638 - RMSE: 0.7579 - val_loss: 0.8738 - val_RMSE: 0.7717\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8635 - RMSE: 0.7578 - val_loss: 0.8735 - val_RMSE: 0.7717\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8614 - RMSE: 0.7559 - val_loss: 0.8733 - val_RMSE: 0.7717\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8656 - RMSE: 0.7604 - val_loss: 0.8731 - val_RMSE: 0.7716\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8578 - RMSE: 0.7527 - val_loss: 0.8728 - val_RMSE: 0.7716\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8572 - RMSE: 0.7524 - val_loss: 0.8726 - val_RMSE: 0.7716\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8622 - RMSE: 0.7576 - val_loss: 0.8723 - val_RMSE: 0.7715\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8629 - RMSE: 0.7585 - val_loss: 0.8721 - val_RMSE: 0.7715\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8676 - RMSE: 0.7635 - val_loss: 0.8718 - val_RMSE: 0.7715\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8639 - RMSE: 0.7599 - val_loss: 0.8716 - val_RMSE: 0.7715\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8542 - RMSE: 0.7504 - val_loss: 0.8714 - val_RMSE: 0.7714\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8559 - RMSE: 0.7523 - val_loss: 0.8711 - val_RMSE: 0.7714\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8616 - RMSE: 0.7582 - val_loss: 0.8709 - val_RMSE: 0.7714\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8590 - RMSE: 0.7559 - val_loss: 0.8707 - val_RMSE: 0.7714\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8608 - RMSE: 0.7579 - val_loss: 0.8704 - val_RMSE: 0.7713\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8533 - RMSE: 0.7506 - val_loss: 0.8702 - val_RMSE: 0.7713\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8625 - RMSE: 0.7601 - val_loss: 0.8700 - val_RMSE: 0.7713\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8562 - RMSE: 0.7540 - val_loss: 0.8697 - val_RMSE: 0.7713\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8652 - RMSE: 0.7632 - val_loss: 0.8695 - val_RMSE: 0.7712\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8590 - RMSE: 0.7571 - val_loss: 0.8693 - val_RMSE: 0.7712\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8628 - RMSE: 0.7611 - val_loss: 0.8690 - val_RMSE: 0.7712\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8592 - RMSE: 0.7577 - val_loss: 0.8688 - val_RMSE: 0.7712\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8558 - RMSE: 0.7546 - val_loss: 0.8686 - val_RMSE: 0.7711\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8501 - RMSE: 0.7491 - val_loss: 0.8683 - val_RMSE: 0.7711\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8499 - RMSE: 0.7491 - val_loss: 0.8681 - val_RMSE: 0.7711\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8608 - RMSE: 0.7602 - val_loss: 0.8679 - val_RMSE: 0.7711\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8643 - RMSE: 0.7639 - val_loss: 0.8677 - val_RMSE: 0.7710\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8604 - RMSE: 0.7602 - val_loss: 0.8674 - val_RMSE: 0.7710\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8498 - RMSE: 0.7499 - val_loss: 0.8672 - val_RMSE: 0.7710\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8552 - RMSE: 0.7554 - val_loss: 0.8670 - val_RMSE: 0.7710\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8624 - RMSE: 0.7629 - val_loss: 0.8668 - val_RMSE: 0.7709\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8536 - RMSE: 0.7543 - val_loss: 0.8665 - val_RMSE: 0.7709\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8574 - RMSE: 0.7582 - val_loss: 0.8663 - val_RMSE: 0.7709\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8528 - RMSE: 0.7539 - val_loss: 0.8661 - val_RMSE: 0.7709\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8563 - RMSE: 0.7575 - val_loss: 0.8659 - val_RMSE: 0.7708\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8515 - RMSE: 0.7529 - val_loss: 0.8656 - val_RMSE: 0.7708\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8571 - RMSE: 0.7588 - val_loss: 0.8654 - val_RMSE: 0.7708\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8549 - RMSE: 0.7567 - val_loss: 0.8652 - val_RMSE: 0.7708\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8526 - RMSE: 0.7546 - val_loss: 0.8650 - val_RMSE: 0.7707\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8425 - RMSE: 0.7447 - val_loss: 0.8648 - val_RMSE: 0.7707\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8383 - RMSE: 0.7407 - val_loss: 0.8645 - val_RMSE: 0.7707\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8512 - RMSE: 0.7538 - val_loss: 0.8643 - val_RMSE: 0.7707\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8538 - RMSE: 0.7566 - val_loss: 0.8641 - val_RMSE: 0.7707\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8567 - RMSE: 0.7597 - val_loss: 0.8639 - val_RMSE: 0.7706\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8495 - RMSE: 0.7528 - val_loss: 0.8637 - val_RMSE: 0.7706\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8499 - RMSE: 0.7533 - val_loss: 0.8635 - val_RMSE: 0.7706\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8558 - RMSE: 0.7594 - val_loss: 0.8632 - val_RMSE: 0.7706\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8507 - RMSE: 0.7545 - val_loss: 0.8630 - val_RMSE: 0.7705\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8454 - RMSE: 0.7494 - val_loss: 0.8628 - val_RMSE: 0.7705\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8531 - RMSE: 0.7573 - val_loss: 0.8626 - val_RMSE: 0.7705\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8467 - RMSE: 0.7511 - val_loss: 0.8624 - val_RMSE: 0.7705\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8499 - RMSE: 0.7545 - val_loss: 0.8622 - val_RMSE: 0.7704\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8428 - RMSE: 0.7476 - val_loss: 0.8619 - val_RMSE: 0.7704\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8500 - RMSE: 0.7550 - val_loss: 0.8617 - val_RMSE: 0.7704\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8483 - RMSE: 0.7534 - val_loss: 0.8615 - val_RMSE: 0.7704\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8443 - RMSE: 0.7497 - val_loss: 0.8613 - val_RMSE: 0.7704\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8472 - RMSE: 0.7527 - val_loss: 0.8611 - val_RMSE: 0.7703\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8395 - RMSE: 0.7452 - val_loss: 0.8609 - val_RMSE: 0.7703\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8455 - RMSE: 0.7514 - val_loss: 0.8607 - val_RMSE: 0.7703\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8529 - RMSE: 0.7591 - val_loss: 0.8605 - val_RMSE: 0.7703\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8482 - RMSE: 0.7545 - val_loss: 0.8603 - val_RMSE: 0.7702\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8435 - RMSE: 0.7500 - val_loss: 0.8601 - val_RMSE: 0.7702\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8351 - RMSE: 0.7417 - val_loss: 0.8599 - val_RMSE: 0.7702\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8443 - RMSE: 0.7511 - val_loss: 0.8596 - val_RMSE: 0.7702\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8353 - RMSE: 0.7424 - val_loss: 0.8594 - val_RMSE: 0.7702\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8395 - RMSE: 0.7467 - val_loss: 0.8592 - val_RMSE: 0.7701\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8480 - RMSE: 0.7555 - val_loss: 0.8590 - val_RMSE: 0.7701\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8412 - RMSE: 0.7488 - val_loss: 0.8588 - val_RMSE: 0.7701\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8471 - RMSE: 0.7549 - val_loss: 0.8586 - val_RMSE: 0.7701\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8459 - RMSE: 0.7539 - val_loss: 0.8584 - val_RMSE: 0.7701\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8355 - RMSE: 0.7436 - val_loss: 0.8582 - val_RMSE: 0.7700\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8475 - RMSE: 0.7559 - val_loss: 0.8580 - val_RMSE: 0.7700\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8431 - RMSE: 0.7517 - val_loss: 0.8578 - val_RMSE: 0.7700\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8403 - RMSE: 0.7490 - val_loss: 0.8576 - val_RMSE: 0.7700\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8463 - RMSE: 0.7552 - val_loss: 0.8574 - val_RMSE: 0.7699\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8519 - RMSE: 0.7610 - val_loss: 0.8572 - val_RMSE: 0.7699\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8529 - RMSE: 0.7621 - val_loss: 0.8570 - val_RMSE: 0.7699\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8353 - RMSE: 0.7448 - val_loss: 0.8568 - val_RMSE: 0.7699\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8412 - RMSE: 0.7508 - val_loss: 0.8566 - val_RMSE: 0.7699\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8484 - RMSE: 0.7582 - val_loss: 0.8564 - val_RMSE: 0.7698\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8457 - RMSE: 0.7557 - val_loss: 0.8562 - val_RMSE: 0.7698\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8457 - RMSE: 0.7559 - val_loss: 0.8560 - val_RMSE: 0.7698\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8525 - RMSE: 0.7628 - val_loss: 0.8558 - val_RMSE: 0.7698\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8404 - RMSE: 0.7509 - val_loss: 0.8556 - val_RMSE: 0.7698\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8436 - RMSE: 0.7543 - val_loss: 0.8554 - val_RMSE: 0.7697\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8402 - RMSE: 0.7511 - val_loss: 0.8552 - val_RMSE: 0.7697\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8543 - RMSE: 0.7653 - val_loss: 0.8550 - val_RMSE: 0.7697\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8491 - RMSE: 0.7603 - val_loss: 0.8548 - val_RMSE: 0.7697\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8446 - RMSE: 0.7560 - val_loss: 0.8547 - val_RMSE: 0.7697\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8479 - RMSE: 0.7595 - val_loss: 0.8545 - val_RMSE: 0.7696\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8348 - RMSE: 0.7466 - val_loss: 0.8543 - val_RMSE: 0.7696\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8412 - RMSE: 0.7532 - val_loss: 0.8541 - val_RMSE: 0.7696\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8401 - RMSE: 0.7522 - val_loss: 0.8539 - val_RMSE: 0.7696\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8447 - RMSE: 0.7570 - val_loss: 0.8537 - val_RMSE: 0.7696\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8392 - RMSE: 0.7517 - val_loss: 0.8535 - val_RMSE: 0.7696\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8371 - RMSE: 0.7498 - val_loss: 0.8533 - val_RMSE: 0.7695\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8346 - RMSE: 0.7474 - val_loss: 0.8531 - val_RMSE: 0.7695\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8293 - RMSE: 0.7422 - val_loss: 0.8529 - val_RMSE: 0.7695\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8387 - RMSE: 0.7518 - val_loss: 0.8528 - val_RMSE: 0.7695\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8440 - RMSE: 0.7574 - val_loss: 0.8526 - val_RMSE: 0.7695\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8329 - RMSE: 0.7464 - val_loss: 0.8524 - val_RMSE: 0.7694\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8392 - RMSE: 0.7529 - val_loss: 0.8522 - val_RMSE: 0.7694\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8322 - RMSE: 0.7460 - val_loss: 0.8520 - val_RMSE: 0.7694\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8370 - RMSE: 0.7510 - val_loss: 0.8518 - val_RMSE: 0.7694\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8425 - RMSE: 0.7567 - val_loss: 0.8516 - val_RMSE: 0.7694\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8324 - RMSE: 0.7467 - val_loss: 0.8514 - val_RMSE: 0.7693\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8340 - RMSE: 0.7486 - val_loss: 0.8513 - val_RMSE: 0.7693\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8352 - RMSE: 0.7499 - val_loss: 0.8511 - val_RMSE: 0.7693\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8351 - RMSE: 0.7500 - val_loss: 0.8509 - val_RMSE: 0.7693\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8331 - RMSE: 0.7482 - val_loss: 0.8507 - val_RMSE: 0.7693\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8403 - RMSE: 0.7555 - val_loss: 0.8505 - val_RMSE: 0.7693\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8372 - RMSE: 0.7525 - val_loss: 0.8503 - val_RMSE: 0.7692\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8352 - RMSE: 0.7507 - val_loss: 0.8502 - val_RMSE: 0.7692\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8277 - RMSE: 0.7434 - val_loss: 0.8500 - val_RMSE: 0.7692\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8312 - RMSE: 0.7471 - val_loss: 0.8498 - val_RMSE: 0.7692\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8253 - RMSE: 0.7413 - val_loss: 0.8496 - val_RMSE: 0.7692\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8372 - RMSE: 0.7534 - val_loss: 0.8494 - val_RMSE: 0.7691\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8417 - RMSE: 0.7580 - val_loss: 0.8493 - val_RMSE: 0.7691\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8255 - RMSE: 0.7420 - val_loss: 0.8491 - val_RMSE: 0.7691\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8402 - RMSE: 0.7569 - val_loss: 0.8489 - val_RMSE: 0.7691\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8398 - RMSE: 0.7566 - val_loss: 0.8487 - val_RMSE: 0.7691\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8350 - RMSE: 0.7520 - val_loss: 0.8485 - val_RMSE: 0.7691\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8249 - RMSE: 0.7421 - val_loss: 0.8484 - val_RMSE: 0.7690\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8387 - RMSE: 0.7561 - val_loss: 0.8482 - val_RMSE: 0.7690\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8264 - RMSE: 0.7438 - val_loss: 0.8480 - val_RMSE: 0.7690\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8237 - RMSE: 0.7413 - val_loss: 0.8478 - val_RMSE: 0.7690\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8476 - RMSE: 0.7654 - val_loss: 0.8477 - val_RMSE: 0.7690\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8374 - RMSE: 0.7554 - val_loss: 0.8475 - val_RMSE: 0.7690\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8269 - RMSE: 0.7450 - val_loss: 0.8473 - val_RMSE: 0.7689\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8311 - RMSE: 0.7493 - val_loss: 0.8471 - val_RMSE: 0.7689\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8382 - RMSE: 0.7566 - val_loss: 0.8470 - val_RMSE: 0.7689\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8310 - RMSE: 0.7496 - val_loss: 0.8468 - val_RMSE: 0.7689\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8342 - RMSE: 0.7529 - val_loss: 0.8466 - val_RMSE: 0.7689\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8233 - RMSE: 0.7422 - val_loss: 0.8464 - val_RMSE: 0.7688\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8298 - RMSE: 0.7489 - val_loss: 0.8463 - val_RMSE: 0.7688\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8321 - RMSE: 0.7513 - val_loss: 0.8461 - val_RMSE: 0.7688\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8314 - RMSE: 0.7508 - val_loss: 0.8459 - val_RMSE: 0.7688\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8393 - RMSE: 0.7589 - val_loss: 0.8458 - val_RMSE: 0.7688\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8401 - RMSE: 0.7598 - val_loss: 0.8456 - val_RMSE: 0.7688\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8202 - RMSE: 0.7401 - val_loss: 0.8454 - val_RMSE: 0.7688\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8361 - RMSE: 0.7561 - val_loss: 0.8453 - val_RMSE: 0.7687\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8327 - RMSE: 0.7529 - val_loss: 0.8451 - val_RMSE: 0.7687\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8322 - RMSE: 0.7526 - val_loss: 0.8449 - val_RMSE: 0.7687\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8329 - RMSE: 0.7534 - val_loss: 0.8448 - val_RMSE: 0.7687\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8349 - RMSE: 0.7555 - val_loss: 0.8446 - val_RMSE: 0.7687\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8395 - RMSE: 0.7603 - val_loss: 0.8444 - val_RMSE: 0.7687\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8284 - RMSE: 0.7494 - val_loss: 0.8443 - val_RMSE: 0.7686\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8263 - RMSE: 0.7474 - val_loss: 0.8441 - val_RMSE: 0.7686\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8241 - RMSE: 0.7454 - val_loss: 0.8439 - val_RMSE: 0.7686\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8341 - RMSE: 0.7556 - val_loss: 0.8438 - val_RMSE: 0.7686\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8254 - RMSE: 0.7470 - val_loss: 0.8436 - val_RMSE: 0.7686\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8308 - RMSE: 0.7524 - val_loss: 0.8434 - val_RMSE: 0.7686\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8430 - RMSE: 0.7648 - val_loss: 0.8433 - val_RMSE: 0.7685\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8350 - RMSE: 0.7570 - val_loss: 0.8431 - val_RMSE: 0.7685\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8268 - RMSE: 0.7489 - val_loss: 0.8429 - val_RMSE: 0.7685\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8263 - RMSE: 0.7486 - val_loss: 0.8428 - val_RMSE: 0.7685\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8286 - RMSE: 0.7511 - val_loss: 0.8426 - val_RMSE: 0.7685\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8319 - RMSE: 0.7545 - val_loss: 0.8425 - val_RMSE: 0.7685\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8315 - RMSE: 0.7542 - val_loss: 0.8423 - val_RMSE: 0.7685\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8366 - RMSE: 0.7595 - val_loss: 0.8421 - val_RMSE: 0.7684\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8376 - RMSE: 0.7607 - val_loss: 0.8420 - val_RMSE: 0.7684\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8294 - RMSE: 0.7526 - val_loss: 0.8418 - val_RMSE: 0.7684\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8333 - RMSE: 0.7566 - val_loss: 0.8417 - val_RMSE: 0.7684\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8295 - RMSE: 0.7529 - val_loss: 0.8415 - val_RMSE: 0.7684\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8247 - RMSE: 0.7483 - val_loss: 0.8413 - val_RMSE: 0.7684\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8293 - RMSE: 0.7531 - val_loss: 0.8412 - val_RMSE: 0.7684\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8323 - RMSE: 0.7562 - val_loss: 0.8410 - val_RMSE: 0.7683\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8277 - RMSE: 0.7517 - val_loss: 0.8409 - val_RMSE: 0.7683\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8256 - RMSE: 0.7498 - val_loss: 0.8407 - val_RMSE: 0.7683\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8311 - RMSE: 0.7554 - val_loss: 0.8405 - val_RMSE: 0.7683\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8237 - RMSE: 0.7482 - val_loss: 0.8404 - val_RMSE: 0.7683\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8384 - RMSE: 0.7630 - val_loss: 0.8402 - val_RMSE: 0.7683\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8218 - RMSE: 0.7466 - val_loss: 0.8401 - val_RMSE: 0.7682\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8254 - RMSE: 0.7503 - val_loss: 0.8399 - val_RMSE: 0.7682\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8339 - RMSE: 0.7590 - val_loss: 0.8398 - val_RMSE: 0.7682\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8250 - RMSE: 0.7502 - val_loss: 0.8396 - val_RMSE: 0.7682\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8286 - RMSE: 0.7539 - val_loss: 0.8395 - val_RMSE: 0.7682\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8206 - RMSE: 0.7461 - val_loss: 0.8393 - val_RMSE: 0.7682\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8261 - RMSE: 0.7517 - val_loss: 0.8391 - val_RMSE: 0.7682\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8384 - RMSE: 0.7642 - val_loss: 0.8390 - val_RMSE: 0.7682\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8231 - RMSE: 0.7490 - val_loss: 0.8388 - val_RMSE: 0.7681\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8248 - RMSE: 0.7508 - val_loss: 0.8387 - val_RMSE: 0.7681\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8156 - RMSE: 0.7418 - val_loss: 0.8385 - val_RMSE: 0.7681\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8196 - RMSE: 0.7459 - val_loss: 0.8384 - val_RMSE: 0.7681\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8253 - RMSE: 0.7517 - val_loss: 0.8382 - val_RMSE: 0.7681\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8307 - RMSE: 0.7573 - val_loss: 0.8381 - val_RMSE: 0.7681\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8278 - RMSE: 0.7546 - val_loss: 0.8379 - val_RMSE: 0.7681\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8261 - RMSE: 0.7530 - val_loss: 0.8378 - val_RMSE: 0.7680\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8156 - RMSE: 0.7426 - val_loss: 0.8376 - val_RMSE: 0.7680\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8257 - RMSE: 0.7528 - val_loss: 0.8375 - val_RMSE: 0.7680\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8267 - RMSE: 0.7540 - val_loss: 0.8373 - val_RMSE: 0.7680\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8164 - RMSE: 0.7438 - val_loss: 0.8372 - val_RMSE: 0.7680\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8240 - RMSE: 0.7516 - val_loss: 0.8370 - val_RMSE: 0.7680\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8264 - RMSE: 0.7542 - val_loss: 0.8369 - val_RMSE: 0.7680\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8146 - RMSE: 0.7425 - val_loss: 0.8367 - val_RMSE: 0.7679\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8284 - RMSE: 0.7564 - val_loss: 0.8366 - val_RMSE: 0.7679\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8332 - RMSE: 0.7613 - val_loss: 0.8364 - val_RMSE: 0.7679\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8225 - RMSE: 0.7508 - val_loss: 0.8363 - val_RMSE: 0.7679\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8331 - RMSE: 0.7615 - val_loss: 0.8362 - val_RMSE: 0.7679\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8249 - RMSE: 0.7534 - val_loss: 0.8360 - val_RMSE: 0.7679\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8136 - RMSE: 0.7423 - val_loss: 0.8359 - val_RMSE: 0.7679\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8375 - RMSE: 0.7663 - val_loss: 0.8357 - val_RMSE: 0.7678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=14 / Init = GlorotUniform / min_loss = 0.767848014831543\n",
            "------------------------------------------------------------------\n",
            "Init = GlorotUniform\n",
            "Model: \"model_22\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_45 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_46 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_88 (Embedding)        (None, 1, 15)        435         input_45[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_89 (Embedding)        (None, 1, 15)        2430        input_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_22 (Dot)                    (None, 1, 1)         0           embedding_88[0][0]               \n",
            "                                                                 embedding_89[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_90 (Embedding)        (None, 1, 1)         29          input_45[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_91 (Embedding)        (None, 1, 1)         162         input_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 1, 1)         0           dot_22[0][0]                     \n",
            "                                                                 embedding_90[0][0]               \n",
            "                                                                 embedding_91[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten_22 (Flatten)            (None, 1)            0           add_22[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 3,056\n",
            "Trainable params: 3,056\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 1.1256 - RMSE: 0.7817 - val_loss: 0.9111 - val_RMSE: 0.7748\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8995 - RMSE: 0.7621 - val_loss: 0.9081 - val_RMSE: 0.7745\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8951 - RMSE: 0.7589 - val_loss: 0.9078 - val_RMSE: 0.7744\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8935 - RMSE: 0.7576 - val_loss: 0.9074 - val_RMSE: 0.7744\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8961 - RMSE: 0.7605 - val_loss: 0.9071 - val_RMSE: 0.7744\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8930 - RMSE: 0.7577 - val_loss: 0.9068 - val_RMSE: 0.7743\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8873 - RMSE: 0.7523 - val_loss: 0.9064 - val_RMSE: 0.7743\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8923 - RMSE: 0.7576 - val_loss: 0.9061 - val_RMSE: 0.7742\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8835 - RMSE: 0.7490 - val_loss: 0.9058 - val_RMSE: 0.7742\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8844 - RMSE: 0.7502 - val_loss: 0.9055 - val_RMSE: 0.7742\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8888 - RMSE: 0.7549 - val_loss: 0.9051 - val_RMSE: 0.7741\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8926 - RMSE: 0.7590 - val_loss: 0.9048 - val_RMSE: 0.7741\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8913 - RMSE: 0.7580 - val_loss: 0.9045 - val_RMSE: 0.7741\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8929 - RMSE: 0.7599 - val_loss: 0.9042 - val_RMSE: 0.7740\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8900 - RMSE: 0.7573 - val_loss: 0.9038 - val_RMSE: 0.7740\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8928 - RMSE: 0.7604 - val_loss: 0.9035 - val_RMSE: 0.7739\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8929 - RMSE: 0.7607 - val_loss: 0.9032 - val_RMSE: 0.7739\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8803 - RMSE: 0.7484 - val_loss: 0.9029 - val_RMSE: 0.7739\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8912 - RMSE: 0.7596 - val_loss: 0.9026 - val_RMSE: 0.7738\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8863 - RMSE: 0.7550 - val_loss: 0.9022 - val_RMSE: 0.7738\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8913 - RMSE: 0.7602 - val_loss: 0.9019 - val_RMSE: 0.7737\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8851 - RMSE: 0.7543 - val_loss: 0.9016 - val_RMSE: 0.7737\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8888 - RMSE: 0.7583 - val_loss: 0.9013 - val_RMSE: 0.7737\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8765 - RMSE: 0.7463 - val_loss: 0.9010 - val_RMSE: 0.7736\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8884 - RMSE: 0.7585 - val_loss: 0.9007 - val_RMSE: 0.7736\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8866 - RMSE: 0.7569 - val_loss: 0.9003 - val_RMSE: 0.7736\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8889 - RMSE: 0.7595 - val_loss: 0.9000 - val_RMSE: 0.7735\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8956 - RMSE: 0.7665 - val_loss: 0.8997 - val_RMSE: 0.7735\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8906 - RMSE: 0.7618 - val_loss: 0.8994 - val_RMSE: 0.7735\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8785 - RMSE: 0.7499 - val_loss: 0.8991 - val_RMSE: 0.7734\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8825 - RMSE: 0.7542 - val_loss: 0.8988 - val_RMSE: 0.7734\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8755 - RMSE: 0.7475 - val_loss: 0.8985 - val_RMSE: 0.7733\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8852 - RMSE: 0.7575 - val_loss: 0.8982 - val_RMSE: 0.7733\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8795 - RMSE: 0.7520 - val_loss: 0.8979 - val_RMSE: 0.7733\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8841 - RMSE: 0.7569 - val_loss: 0.8975 - val_RMSE: 0.7732\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8801 - RMSE: 0.7531 - val_loss: 0.8972 - val_RMSE: 0.7732\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8863 - RMSE: 0.7596 - val_loss: 0.8969 - val_RMSE: 0.7732\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8824 - RMSE: 0.7560 - val_loss: 0.8966 - val_RMSE: 0.7731\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8719 - RMSE: 0.7458 - val_loss: 0.8963 - val_RMSE: 0.7731\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8823 - RMSE: 0.7564 - val_loss: 0.8960 - val_RMSE: 0.7731\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8828 - RMSE: 0.7572 - val_loss: 0.8957 - val_RMSE: 0.7730\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8875 - RMSE: 0.7622 - val_loss: 0.8954 - val_RMSE: 0.7730\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8893 - RMSE: 0.7643 - val_loss: 0.8951 - val_RMSE: 0.7730\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8837 - RMSE: 0.7590 - val_loss: 0.8948 - val_RMSE: 0.7729\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8755 - RMSE: 0.7510 - val_loss: 0.8945 - val_RMSE: 0.7729\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8778 - RMSE: 0.7536 - val_loss: 0.8942 - val_RMSE: 0.7728\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8782 - RMSE: 0.7542 - val_loss: 0.8939 - val_RMSE: 0.7728\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8686 - RMSE: 0.7449 - val_loss: 0.8936 - val_RMSE: 0.7728\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8728 - RMSE: 0.7493 - val_loss: 0.8933 - val_RMSE: 0.7728\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8879 - RMSE: 0.7647 - val_loss: 0.8931 - val_RMSE: 0.7727\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8873 - RMSE: 0.7643 - val_loss: 0.8928 - val_RMSE: 0.7727\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8912 - RMSE: 0.7685 - val_loss: 0.8925 - val_RMSE: 0.7727\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8758 - RMSE: 0.7534 - val_loss: 0.8922 - val_RMSE: 0.7726\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8766 - RMSE: 0.7544 - val_loss: 0.8919 - val_RMSE: 0.7726\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8731 - RMSE: 0.7512 - val_loss: 0.8916 - val_RMSE: 0.7726\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8735 - RMSE: 0.7518 - val_loss: 0.8913 - val_RMSE: 0.7725\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8662 - RMSE: 0.7449 - val_loss: 0.8910 - val_RMSE: 0.7725\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8806 - RMSE: 0.7594 - val_loss: 0.8907 - val_RMSE: 0.7725\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8689 - RMSE: 0.7481 - val_loss: 0.8904 - val_RMSE: 0.7724\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8763 - RMSE: 0.7556 - val_loss: 0.8902 - val_RMSE: 0.7724\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8823 - RMSE: 0.7620 - val_loss: 0.8899 - val_RMSE: 0.7724\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8797 - RMSE: 0.7596 - val_loss: 0.8896 - val_RMSE: 0.7723\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8780 - RMSE: 0.7581 - val_loss: 0.8893 - val_RMSE: 0.7723\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8788 - RMSE: 0.7592 - val_loss: 0.8890 - val_RMSE: 0.7723\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8778 - RMSE: 0.7585 - val_loss: 0.8887 - val_RMSE: 0.7722\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8692 - RMSE: 0.7501 - val_loss: 0.8885 - val_RMSE: 0.7722\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8800 - RMSE: 0.7612 - val_loss: 0.8882 - val_RMSE: 0.7722\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8809 - RMSE: 0.7623 - val_loss: 0.8879 - val_RMSE: 0.7721\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8756 - RMSE: 0.7572 - val_loss: 0.8876 - val_RMSE: 0.7721\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8903 - RMSE: 0.7722 - val_loss: 0.8873 - val_RMSE: 0.7721\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8723 - RMSE: 0.7545 - val_loss: 0.8871 - val_RMSE: 0.7721\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8714 - RMSE: 0.7538 - val_loss: 0.8868 - val_RMSE: 0.7720\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8784 - RMSE: 0.7611 - val_loss: 0.8865 - val_RMSE: 0.7720\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8602 - RMSE: 0.7431 - val_loss: 0.8862 - val_RMSE: 0.7720\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8756 - RMSE: 0.7587 - val_loss: 0.8860 - val_RMSE: 0.7719\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8747 - RMSE: 0.7581 - val_loss: 0.8857 - val_RMSE: 0.7719\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8677 - RMSE: 0.7513 - val_loss: 0.8854 - val_RMSE: 0.7719\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8685 - RMSE: 0.7523 - val_loss: 0.8851 - val_RMSE: 0.7718\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8591 - RMSE: 0.7432 - val_loss: 0.8849 - val_RMSE: 0.7718\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8671 - RMSE: 0.7514 - val_loss: 0.8846 - val_RMSE: 0.7718\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8643 - RMSE: 0.7489 - val_loss: 0.8843 - val_RMSE: 0.7718\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8698 - RMSE: 0.7547 - val_loss: 0.8841 - val_RMSE: 0.7717\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8700 - RMSE: 0.7550 - val_loss: 0.8838 - val_RMSE: 0.7717\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8619 - RMSE: 0.7473 - val_loss: 0.8835 - val_RMSE: 0.7717\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8666 - RMSE: 0.7522 - val_loss: 0.8833 - val_RMSE: 0.7716\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8765 - RMSE: 0.7623 - val_loss: 0.8830 - val_RMSE: 0.7716\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8659 - RMSE: 0.7520 - val_loss: 0.8827 - val_RMSE: 0.7716\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8641 - RMSE: 0.7504 - val_loss: 0.8824 - val_RMSE: 0.7716\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8810 - RMSE: 0.7675 - val_loss: 0.8822 - val_RMSE: 0.7715\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8738 - RMSE: 0.7605 - val_loss: 0.8819 - val_RMSE: 0.7715\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8656 - RMSE: 0.7526 - val_loss: 0.8817 - val_RMSE: 0.7715\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8677 - RMSE: 0.7549 - val_loss: 0.8814 - val_RMSE: 0.7714\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8725 - RMSE: 0.7600 - val_loss: 0.8811 - val_RMSE: 0.7714\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8631 - RMSE: 0.7508 - val_loss: 0.8809 - val_RMSE: 0.7714\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8829 - RMSE: 0.7708 - val_loss: 0.8806 - val_RMSE: 0.7714\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8691 - RMSE: 0.7573 - val_loss: 0.8804 - val_RMSE: 0.7713\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8612 - RMSE: 0.7496 - val_loss: 0.8801 - val_RMSE: 0.7713\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8648 - RMSE: 0.7535 - val_loss: 0.8798 - val_RMSE: 0.7713\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8635 - RMSE: 0.7524 - val_loss: 0.8796 - val_RMSE: 0.7712\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8693 - RMSE: 0.7584 - val_loss: 0.8793 - val_RMSE: 0.7712\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8717 - RMSE: 0.7611 - val_loss: 0.8791 - val_RMSE: 0.7712\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8679 - RMSE: 0.7575 - val_loss: 0.8788 - val_RMSE: 0.7712\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8633 - RMSE: 0.7531 - val_loss: 0.8786 - val_RMSE: 0.7711\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8616 - RMSE: 0.7516 - val_loss: 0.8783 - val_RMSE: 0.7711\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8604 - RMSE: 0.7506 - val_loss: 0.8781 - val_RMSE: 0.7711\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8598 - RMSE: 0.7503 - val_loss: 0.8778 - val_RMSE: 0.7711\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8610 - RMSE: 0.7516 - val_loss: 0.8775 - val_RMSE: 0.7710\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8658 - RMSE: 0.7567 - val_loss: 0.8773 - val_RMSE: 0.7710\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8469 - RMSE: 0.7381 - val_loss: 0.8770 - val_RMSE: 0.7710\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8740 - RMSE: 0.7653 - val_loss: 0.8768 - val_RMSE: 0.7710\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8603 - RMSE: 0.7519 - val_loss: 0.8765 - val_RMSE: 0.7709\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8593 - RMSE: 0.7511 - val_loss: 0.8763 - val_RMSE: 0.7709\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8629 - RMSE: 0.7549 - val_loss: 0.8760 - val_RMSE: 0.7709\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8590 - RMSE: 0.7512 - val_loss: 0.8758 - val_RMSE: 0.7709\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8581 - RMSE: 0.7506 - val_loss: 0.8756 - val_RMSE: 0.7708\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8587 - RMSE: 0.7514 - val_loss: 0.8753 - val_RMSE: 0.7708\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8551 - RMSE: 0.7480 - val_loss: 0.8751 - val_RMSE: 0.7708\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8657 - RMSE: 0.7588 - val_loss: 0.8748 - val_RMSE: 0.7708\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8660 - RMSE: 0.7594 - val_loss: 0.8746 - val_RMSE: 0.7707\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8597 - RMSE: 0.7533 - val_loss: 0.8743 - val_RMSE: 0.7707\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8497 - RMSE: 0.7435 - val_loss: 0.8741 - val_RMSE: 0.7707\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8540 - RMSE: 0.7480 - val_loss: 0.8738 - val_RMSE: 0.7707\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8643 - RMSE: 0.7585 - val_loss: 0.8736 - val_RMSE: 0.7706\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8614 - RMSE: 0.7558 - val_loss: 0.8734 - val_RMSE: 0.7706\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8628 - RMSE: 0.7575 - val_loss: 0.8731 - val_RMSE: 0.7706\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8587 - RMSE: 0.7536 - val_loss: 0.8729 - val_RMSE: 0.7706\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8644 - RMSE: 0.7595 - val_loss: 0.8726 - val_RMSE: 0.7705\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8585 - RMSE: 0.7538 - val_loss: 0.8724 - val_RMSE: 0.7705\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8611 - RMSE: 0.7566 - val_loss: 0.8722 - val_RMSE: 0.7705\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8550 - RMSE: 0.7508 - val_loss: 0.8719 - val_RMSE: 0.7705\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8610 - RMSE: 0.7570 - val_loss: 0.8717 - val_RMSE: 0.7704\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8620 - RMSE: 0.7581 - val_loss: 0.8715 - val_RMSE: 0.7704\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8603 - RMSE: 0.7566 - val_loss: 0.8712 - val_RMSE: 0.7704\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8593 - RMSE: 0.7559 - val_loss: 0.8710 - val_RMSE: 0.7704\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8534 - RMSE: 0.7502 - val_loss: 0.8708 - val_RMSE: 0.7703\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8495 - RMSE: 0.7465 - val_loss: 0.8705 - val_RMSE: 0.7703\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8561 - RMSE: 0.7533 - val_loss: 0.8703 - val_RMSE: 0.7703\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8460 - RMSE: 0.7434 - val_loss: 0.8701 - val_RMSE: 0.7703\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8522 - RMSE: 0.7499 - val_loss: 0.8698 - val_RMSE: 0.7703\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8517 - RMSE: 0.7495 - val_loss: 0.8696 - val_RMSE: 0.7702\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8638 - RMSE: 0.7618 - val_loss: 0.8694 - val_RMSE: 0.7702\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8525 - RMSE: 0.7508 - val_loss: 0.8691 - val_RMSE: 0.7702\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8544 - RMSE: 0.7529 - val_loss: 0.8689 - val_RMSE: 0.7702\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8615 - RMSE: 0.7602 - val_loss: 0.8687 - val_RMSE: 0.7701\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8551 - RMSE: 0.7540 - val_loss: 0.8684 - val_RMSE: 0.7701\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8604 - RMSE: 0.7594 - val_loss: 0.8682 - val_RMSE: 0.7701\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8618 - RMSE: 0.7611 - val_loss: 0.8680 - val_RMSE: 0.7701\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8573 - RMSE: 0.7568 - val_loss: 0.8678 - val_RMSE: 0.7701\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8547 - RMSE: 0.7544 - val_loss: 0.8675 - val_RMSE: 0.7700\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8582 - RMSE: 0.7581 - val_loss: 0.8673 - val_RMSE: 0.7700\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8500 - RMSE: 0.7501 - val_loss: 0.8671 - val_RMSE: 0.7700\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8482 - RMSE: 0.7485 - val_loss: 0.8669 - val_RMSE: 0.7700\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8515 - RMSE: 0.7520 - val_loss: 0.8666 - val_RMSE: 0.7699\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8583 - RMSE: 0.7590 - val_loss: 0.8664 - val_RMSE: 0.7699\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8483 - RMSE: 0.7492 - val_loss: 0.8662 - val_RMSE: 0.7699\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8578 - RMSE: 0.7589 - val_loss: 0.8660 - val_RMSE: 0.7699\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8544 - RMSE: 0.7557 - val_loss: 0.8658 - val_RMSE: 0.7699\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8457 - RMSE: 0.7472 - val_loss: 0.8655 - val_RMSE: 0.7698\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8523 - RMSE: 0.7540 - val_loss: 0.8653 - val_RMSE: 0.7698\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8568 - RMSE: 0.7587 - val_loss: 0.8651 - val_RMSE: 0.7698\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8556 - RMSE: 0.7578 - val_loss: 0.8649 - val_RMSE: 0.7698\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8483 - RMSE: 0.7506 - val_loss: 0.8647 - val_RMSE: 0.7697\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8448 - RMSE: 0.7473 - val_loss: 0.8644 - val_RMSE: 0.7697\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8529 - RMSE: 0.7556 - val_loss: 0.8642 - val_RMSE: 0.7697\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8497 - RMSE: 0.7526 - val_loss: 0.8640 - val_RMSE: 0.7697\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8489 - RMSE: 0.7520 - val_loss: 0.8638 - val_RMSE: 0.7697\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8376 - RMSE: 0.7409 - val_loss: 0.8636 - val_RMSE: 0.7696\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8510 - RMSE: 0.7545 - val_loss: 0.8634 - val_RMSE: 0.7696\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8520 - RMSE: 0.7557 - val_loss: 0.8632 - val_RMSE: 0.7696\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8509 - RMSE: 0.7548 - val_loss: 0.8629 - val_RMSE: 0.7696\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8430 - RMSE: 0.7470 - val_loss: 0.8627 - val_RMSE: 0.7696\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8471 - RMSE: 0.7514 - val_loss: 0.8625 - val_RMSE: 0.7695\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8565 - RMSE: 0.7609 - val_loss: 0.8623 - val_RMSE: 0.7695\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8456 - RMSE: 0.7502 - val_loss: 0.8621 - val_RMSE: 0.7695\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8391 - RMSE: 0.7439 - val_loss: 0.8619 - val_RMSE: 0.7695\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8534 - RMSE: 0.7584 - val_loss: 0.8617 - val_RMSE: 0.7695\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8522 - RMSE: 0.7574 - val_loss: 0.8615 - val_RMSE: 0.7694\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8494 - RMSE: 0.7548 - val_loss: 0.8612 - val_RMSE: 0.7694\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8445 - RMSE: 0.7501 - val_loss: 0.8610 - val_RMSE: 0.7694\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8476 - RMSE: 0.7533 - val_loss: 0.8608 - val_RMSE: 0.7694\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8490 - RMSE: 0.7550 - val_loss: 0.8606 - val_RMSE: 0.7694\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8448 - RMSE: 0.7510 - val_loss: 0.8604 - val_RMSE: 0.7693\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8462 - RMSE: 0.7525 - val_loss: 0.8602 - val_RMSE: 0.7693\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8380 - RMSE: 0.7445 - val_loss: 0.8600 - val_RMSE: 0.7693\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8441 - RMSE: 0.7509 - val_loss: 0.8598 - val_RMSE: 0.7693\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8532 - RMSE: 0.7601 - val_loss: 0.8596 - val_RMSE: 0.7693\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8529 - RMSE: 0.7600 - val_loss: 0.8594 - val_RMSE: 0.7692\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8541 - RMSE: 0.7614 - val_loss: 0.8592 - val_RMSE: 0.7692\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8369 - RMSE: 0.7443 - val_loss: 0.8590 - val_RMSE: 0.7692\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8485 - RMSE: 0.7562 - val_loss: 0.8588 - val_RMSE: 0.7692\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8418 - RMSE: 0.7496 - val_loss: 0.8586 - val_RMSE: 0.7692\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8458 - RMSE: 0.7538 - val_loss: 0.8584 - val_RMSE: 0.7691\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8369 - RMSE: 0.7451 - val_loss: 0.8582 - val_RMSE: 0.7691\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8465 - RMSE: 0.7549 - val_loss: 0.8580 - val_RMSE: 0.7691\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8443 - RMSE: 0.7529 - val_loss: 0.8578 - val_RMSE: 0.7691\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8450 - RMSE: 0.7537 - val_loss: 0.8576 - val_RMSE: 0.7691\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8396 - RMSE: 0.7485 - val_loss: 0.8574 - val_RMSE: 0.7691\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8431 - RMSE: 0.7522 - val_loss: 0.8572 - val_RMSE: 0.7690\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8473 - RMSE: 0.7565 - val_loss: 0.8570 - val_RMSE: 0.7690\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8464 - RMSE: 0.7559 - val_loss: 0.8568 - val_RMSE: 0.7690\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8337 - RMSE: 0.7434 - val_loss: 0.8566 - val_RMSE: 0.7690\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8400 - RMSE: 0.7498 - val_loss: 0.8564 - val_RMSE: 0.7690\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8471 - RMSE: 0.7571 - val_loss: 0.8562 - val_RMSE: 0.7689\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8565 - RMSE: 0.7667 - val_loss: 0.8560 - val_RMSE: 0.7689\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8430 - RMSE: 0.7534 - val_loss: 0.8558 - val_RMSE: 0.7689\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8412 - RMSE: 0.7517 - val_loss: 0.8556 - val_RMSE: 0.7689\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8437 - RMSE: 0.7544 - val_loss: 0.8554 - val_RMSE: 0.7689\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8450 - RMSE: 0.7559 - val_loss: 0.8552 - val_RMSE: 0.7689\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8456 - RMSE: 0.7567 - val_loss: 0.8550 - val_RMSE: 0.7688\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8393 - RMSE: 0.7506 - val_loss: 0.8548 - val_RMSE: 0.7688\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8389 - RMSE: 0.7503 - val_loss: 0.8546 - val_RMSE: 0.7688\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8397 - RMSE: 0.7513 - val_loss: 0.8545 - val_RMSE: 0.7688\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8380 - RMSE: 0.7497 - val_loss: 0.8543 - val_RMSE: 0.7688\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8466 - RMSE: 0.7585 - val_loss: 0.8541 - val_RMSE: 0.7688\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8340 - RMSE: 0.7461 - val_loss: 0.8539 - val_RMSE: 0.7687\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8391 - RMSE: 0.7514 - val_loss: 0.8537 - val_RMSE: 0.7687\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8338 - RMSE: 0.7463 - val_loss: 0.8535 - val_RMSE: 0.7687\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8353 - RMSE: 0.7479 - val_loss: 0.8533 - val_RMSE: 0.7687\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8289 - RMSE: 0.7416 - val_loss: 0.8531 - val_RMSE: 0.7687\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8357 - RMSE: 0.7487 - val_loss: 0.8529 - val_RMSE: 0.7686\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8312 - RMSE: 0.7444 - val_loss: 0.8527 - val_RMSE: 0.7686\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8438 - RMSE: 0.7571 - val_loss: 0.8526 - val_RMSE: 0.7686\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8362 - RMSE: 0.7497 - val_loss: 0.8524 - val_RMSE: 0.7686\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8341 - RMSE: 0.7478 - val_loss: 0.8522 - val_RMSE: 0.7686\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8362 - RMSE: 0.7500 - val_loss: 0.8520 - val_RMSE: 0.7686\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8360 - RMSE: 0.7500 - val_loss: 0.8518 - val_RMSE: 0.7685\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8365 - RMSE: 0.7507 - val_loss: 0.8516 - val_RMSE: 0.7685\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8363 - RMSE: 0.7507 - val_loss: 0.8515 - val_RMSE: 0.7685\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8329 - RMSE: 0.7474 - val_loss: 0.8513 - val_RMSE: 0.7685\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8255 - RMSE: 0.7402 - val_loss: 0.8511 - val_RMSE: 0.7685\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8294 - RMSE: 0.7442 - val_loss: 0.8509 - val_RMSE: 0.7685\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8398 - RMSE: 0.7548 - val_loss: 0.8507 - val_RMSE: 0.7684\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8446 - RMSE: 0.7597 - val_loss: 0.8505 - val_RMSE: 0.7684\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8413 - RMSE: 0.7567 - val_loss: 0.8504 - val_RMSE: 0.7684\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8349 - RMSE: 0.7504 - val_loss: 0.8502 - val_RMSE: 0.7684\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8391 - RMSE: 0.7547 - val_loss: 0.8500 - val_RMSE: 0.7684\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8402 - RMSE: 0.7560 - val_loss: 0.8498 - val_RMSE: 0.7684\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8331 - RMSE: 0.7491 - val_loss: 0.8496 - val_RMSE: 0.7684\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8342 - RMSE: 0.7504 - val_loss: 0.8495 - val_RMSE: 0.7683\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8478 - RMSE: 0.7641 - val_loss: 0.8493 - val_RMSE: 0.7683\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8343 - RMSE: 0.7508 - val_loss: 0.8491 - val_RMSE: 0.7683\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8377 - RMSE: 0.7544 - val_loss: 0.8489 - val_RMSE: 0.7683\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8318 - RMSE: 0.7486 - val_loss: 0.8488 - val_RMSE: 0.7683\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8428 - RMSE: 0.7597 - val_loss: 0.8486 - val_RMSE: 0.7683\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8268 - RMSE: 0.7439 - val_loss: 0.8484 - val_RMSE: 0.7682\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8197 - RMSE: 0.7369 - val_loss: 0.8482 - val_RMSE: 0.7682\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8541 - RMSE: 0.7716 - val_loss: 0.8481 - val_RMSE: 0.7682\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8313 - RMSE: 0.7489 - val_loss: 0.8479 - val_RMSE: 0.7682\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8260 - RMSE: 0.7438 - val_loss: 0.8477 - val_RMSE: 0.7682\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8383 - RMSE: 0.7562 - val_loss: 0.8475 - val_RMSE: 0.7682\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8386 - RMSE: 0.7567 - val_loss: 0.8474 - val_RMSE: 0.7681\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8344 - RMSE: 0.7527 - val_loss: 0.8472 - val_RMSE: 0.7681\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8182 - RMSE: 0.7365 - val_loss: 0.8470 - val_RMSE: 0.7681\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8343 - RMSE: 0.7528 - val_loss: 0.8468 - val_RMSE: 0.7681\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8307 - RMSE: 0.7494 - val_loss: 0.8467 - val_RMSE: 0.7681\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8211 - RMSE: 0.7400 - val_loss: 0.8465 - val_RMSE: 0.7681\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8337 - RMSE: 0.7527 - val_loss: 0.8463 - val_RMSE: 0.7681\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8279 - RMSE: 0.7470 - val_loss: 0.8462 - val_RMSE: 0.7680\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8359 - RMSE: 0.7553 - val_loss: 0.8460 - val_RMSE: 0.7680\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8335 - RMSE: 0.7530 - val_loss: 0.8458 - val_RMSE: 0.7680\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8307 - RMSE: 0.7503 - val_loss: 0.8456 - val_RMSE: 0.7680\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8407 - RMSE: 0.7604 - val_loss: 0.8455 - val_RMSE: 0.7680\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8272 - RMSE: 0.7472 - val_loss: 0.8453 - val_RMSE: 0.7680\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8460 - RMSE: 0.7661 - val_loss: 0.8451 - val_RMSE: 0.7679\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8336 - RMSE: 0.7539 - val_loss: 0.8450 - val_RMSE: 0.7679\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8309 - RMSE: 0.7513 - val_loss: 0.8448 - val_RMSE: 0.7679\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8335 - RMSE: 0.7541 - val_loss: 0.8446 - val_RMSE: 0.7679\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8302 - RMSE: 0.7510 - val_loss: 0.8445 - val_RMSE: 0.7679\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8301 - RMSE: 0.7509 - val_loss: 0.8443 - val_RMSE: 0.7679\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8300 - RMSE: 0.7510 - val_loss: 0.8441 - val_RMSE: 0.7679\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8256 - RMSE: 0.7468 - val_loss: 0.8440 - val_RMSE: 0.7678\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8283 - RMSE: 0.7496 - val_loss: 0.8438 - val_RMSE: 0.7678\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8427 - RMSE: 0.7642 - val_loss: 0.8436 - val_RMSE: 0.7678\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8161 - RMSE: 0.7377 - val_loss: 0.8435 - val_RMSE: 0.7678\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8353 - RMSE: 0.7571 - val_loss: 0.8433 - val_RMSE: 0.7678\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8280 - RMSE: 0.7500 - val_loss: 0.8432 - val_RMSE: 0.7678\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8243 - RMSE: 0.7464 - val_loss: 0.8430 - val_RMSE: 0.7678\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8339 - RMSE: 0.7561 - val_loss: 0.8428 - val_RMSE: 0.7677\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8294 - RMSE: 0.7517 - val_loss: 0.8427 - val_RMSE: 0.7677\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8326 - RMSE: 0.7551 - val_loss: 0.8425 - val_RMSE: 0.7677\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8268 - RMSE: 0.7494 - val_loss: 0.8423 - val_RMSE: 0.7677\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8211 - RMSE: 0.7439 - val_loss: 0.8422 - val_RMSE: 0.7677\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8255 - RMSE: 0.7484 - val_loss: 0.8420 - val_RMSE: 0.7677\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8274 - RMSE: 0.7505 - val_loss: 0.8419 - val_RMSE: 0.7677\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8313 - RMSE: 0.7545 - val_loss: 0.8417 - val_RMSE: 0.7677\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8320 - RMSE: 0.7554 - val_loss: 0.8415 - val_RMSE: 0.7676\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8228 - RMSE: 0.7463 - val_loss: 0.8414 - val_RMSE: 0.7676\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8277 - RMSE: 0.7513 - val_loss: 0.8412 - val_RMSE: 0.7676\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8259 - RMSE: 0.7497 - val_loss: 0.8411 - val_RMSE: 0.7676\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8294 - RMSE: 0.7534 - val_loss: 0.8409 - val_RMSE: 0.7676\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8239 - RMSE: 0.7480 - val_loss: 0.8408 - val_RMSE: 0.7676\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8301 - RMSE: 0.7543 - val_loss: 0.8406 - val_RMSE: 0.7676\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8182 - RMSE: 0.7426 - val_loss: 0.8404 - val_RMSE: 0.7675\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8301 - RMSE: 0.7546 - val_loss: 0.8403 - val_RMSE: 0.7675\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8346 - RMSE: 0.7593 - val_loss: 0.8401 - val_RMSE: 0.7675\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8318 - RMSE: 0.7566 - val_loss: 0.8400 - val_RMSE: 0.7675\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8310 - RMSE: 0.7559 - val_loss: 0.8398 - val_RMSE: 0.7675\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8297 - RMSE: 0.7548 - val_loss: 0.8397 - val_RMSE: 0.7675\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8290 - RMSE: 0.7542 - val_loss: 0.8395 - val_RMSE: 0.7675\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8277 - RMSE: 0.7531 - val_loss: 0.8394 - val_RMSE: 0.7675\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "K=15 / Init = GlorotUniform / min_loss = 0.7674558162689209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC0GJ7JF6WQ2",
        "outputId": "618669a7-1174-4135-a2fc-b6a461ab3f4a"
      },
      "source": [
        "NN_Models['GlorotNormal'].items()\n",
        "# K =13, min_loss = 0.7626"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('K', 13), ('model', <tensorflow.python.keras.engine.functional.Functional object at 0x7f51e75775d0>), ('loss', [0.7629801630973816, 0.7626529932022095, 0.76265549659729, 0.7626515030860901, 0.7626562714576721, 0.7626504898071289, 0.7626493573188782, 0.7626535892486572, 0.7626463174819946, 0.7626454830169678, 0.7626461982727051, 0.7626467347145081, 0.7626441121101379, 0.7626422047615051, 0.7626441717147827, 0.7626416683197021, 0.7626434564590454, 0.7626412510871887, 0.7626395225524902, 0.762639045715332, 0.7626416683197021, 0.7626379728317261, 0.7626387476921082, 0.7626358270645142, 0.7626364231109619, 0.7626328468322754, 0.7626327276229858, 0.7626343369483948, 0.7626311779022217, 0.7626352310180664, 0.7626383900642395, 0.7626357674598694, 0.7626364231109619, 0.7626396417617798, 0.7626347541809082, 0.762635350227356, 0.7626402378082275, 0.7626399397850037, 0.7626349925994873, 0.7626366019248962, 0.7626382112503052, 0.762636661529541, 0.7626381516456604, 0.7626388669013977, 0.7626417875289917, 0.7626423835754395, 0.7626392841339111, 0.7626405954360962, 0.7626436948776245, 0.7626497149467468, 0.7626424431800842, 0.7626464366912842, 0.7626396417617798, 0.7626426815986633, 0.7626436352729797, 0.762643575668335, 0.7626466155052185, 0.7626457810401917, 0.7626469135284424, 0.762646496295929, 0.7626471519470215, 0.7626437544822693, 0.7626474499702454, 0.7626447677612305, 0.762651801109314, 0.762645423412323, 0.7626463770866394, 0.7626488208770752, 0.7626447081565857, 0.7626470923423767, 0.7626517415046692, 0.7626559138298035, 0.7626528739929199, 0.7626485824584961, 0.762653112411499, 0.762652575969696, 0.7626571655273438, 0.7626574635505676, 0.7626620531082153, 0.7626597285270691, 0.7626610398292542, 0.7626660466194153, 0.7626633048057556, 0.7626663446426392, 0.762664258480072, 0.7626681923866272, 0.7626678347587585, 0.7626662254333496, 0.762679934501648, 0.7626776695251465, 0.7626736164093018, 0.7626796364784241, 0.7626811861991882, 0.7626807689666748, 0.7626780271530151, 0.7626839280128479, 0.7626887559890747, 0.7626863121986389, 0.762688934803009, 0.7626857161521912, 0.7626923322677612, 0.7626948952674866, 0.7626937627792358, 0.7627032995223999, 0.7627012133598328, 0.7627038359642029, 0.762703001499176, 0.762706458568573, 0.7627063989639282, 0.7627089619636536, 0.7627137899398804, 0.7627139091491699, 0.7627195119857788, 0.7627149224281311, 0.7627178430557251, 0.7627205848693848, 0.7627227902412415, 0.7627211809158325, 0.7627249360084534, 0.762732982635498, 0.76273113489151, 0.7627317905426025, 0.7627371549606323, 0.7627360224723816, 0.7627404928207397, 0.7627421021461487, 0.7627438306808472, 0.7627441883087158, 0.7627459764480591, 0.762750506401062, 0.7627478837966919, 0.762746274471283, 0.7627492547035217, 0.7627496123313904, 0.7627543210983276, 0.762757420539856, 0.7627618908882141, 0.7627642750740051, 0.7627582550048828, 0.7627606391906738, 0.7627619504928589, 0.7627612352371216, 0.7627664804458618, 0.762756884098053, 0.7627645134925842, 0.7627702355384827, 0.7627721428871155, 0.762774646282196, 0.7627729177474976, 0.7627794742584229, 0.7627797722816467, 0.7627794742584229, 0.7627848982810974, 0.7627880573272705, 0.7627850770950317, 0.7627918124198914, 0.7627876400947571, 0.7627943754196167, 0.7627964615821838, 0.7627930045127869, 0.7627972960472107, 0.7628027200698853, 0.7628059387207031, 0.7628042697906494, 0.7628138065338135, 0.7628030180931091, 0.7628104090690613, 0.7628133296966553, 0.7628124952316284, 0.7628182172775269, 0.7628159523010254, 0.7628148198127747, 0.7628148794174194, 0.7628248333930969, 0.762823760509491, 0.7628271579742432, 0.7628225088119507, 0.7628234028816223, 0.7628263235092163, 0.7628260850906372, 0.7628306150436401, 0.762832760810852, 0.7628341317176819, 0.7628366947174072, 0.762837827205658, 0.7628340721130371, 0.7628419995307922, 0.7628415822982788, 0.762845516204834, 0.7628499865531921, 0.762847900390625, 0.7628496885299683, 0.7628563046455383, 0.7628580927848816, 0.7628576159477234, 0.7628616094589233, 0.7628611326217651, 0.7628577947616577, 0.7628664970397949, 0.7628687620162964, 0.7628636956214905, 0.7628685832023621, 0.7628703713417053, 0.7628738880157471, 0.7628754377365112, 0.7628772854804993, 0.7628756761550903, 0.7628793716430664, 0.7628811001777649, 0.7628836035728455, 0.7628903388977051, 0.7628839015960693, 0.7628960609436035, 0.7628965377807617, 0.7628995776176453, 0.7628964185714722, 0.7628999948501587, 0.7628969550132751, 0.7628976106643677, 0.762897253036499, 0.7629052400588989, 0.7628989219665527, 0.762907862663269, 0.7629069685935974, 0.7629050612449646, 0.7629141807556152, 0.7629170417785645, 0.7629110813140869, 0.7629144191741943, 0.7629193067550659, 0.7629184722900391, 0.7629230618476868, 0.7629166841506958, 0.7629202604293823, 0.7629246711730957, 0.7629284858703613, 0.7629250884056091, 0.7629320621490479, 0.7629279494285583, 0.7629223465919495, 0.7629318237304688, 0.7629356980323792, 0.7629283666610718, 0.7629352807998657, 0.7629386186599731, 0.7629358768463135, 0.7629354596138, 0.762942373752594, 0.7629392147064209, 0.7629403471946716, 0.7629382014274597, 0.7629432678222656, 0.7629389762878418, 0.7629477977752686, 0.7629478573799133, 0.762948751449585, 0.7629500031471252, 0.7629529237747192, 0.7629467844963074, 0.7629554271697998, 0.7629581093788147, 0.7629590034484863, 0.7629604339599609, 0.7629589438438416, 0.762965738773346, 0.7629614472389221, 0.7629640102386475, 0.7629629969596863, 0.762967586517334, 0.7629719972610474, 0.762973427772522, 0.7629731893539429, 0.7629717588424683, 0.762976884841919, 0.7629738450050354, 0.7629791498184204, 0.7629809379577637, 0.7629820704460144, 0.762984037399292, 0.762995183467865, 0.7629855871200562, 0.7629878520965576, 0.7629925012588501, 0.7629932761192322, 0.762991189956665, 0.7629880309104919, 0.762992262840271, 0.76299649477005, 0.7629952430725098, 0.7630004286766052, 0.7630004286766052, 0.7629985809326172, 0.7629985213279724, 0.7630054354667664, 0.7630059719085693, 0.7630050778388977, 0.763008713722229, 0.7630102634429932, 0.763013482093811, 0.7630069255828857]), ('min_loss', 0.7626311779022217)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWia-MWj4iy1",
        "outputId": "d26215dc-6b7f-4fc8-d84a-f822cd721995"
      },
      "source": [
        "def RMSE2(y_true, y_pred):\n",
        "    return round(np.sqrt(np.mean((np.array(y_true) - np.array(y_pred))**2)),4)\n",
        "model = NN_Models['GlorotNormal']['model']\n",
        "\n",
        "user_ids = ratings_test.users.values\n",
        "track_ids = ratings_test.tracks.values\n",
        "y_pred = model.predict([user_ids, track_ids]) + mu\n",
        "y_pred = np.ravel(y_pred, order='C')\n",
        "y_true = np.array(ratings_test.score)\n",
        "\n",
        "RMSE2(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7604"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15omDa7dSeJg"
      },
      "source": [
        "### (2) Layer Added"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSUuDBA3Ti_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e6fa7a-93d6-473c-c5d4-ce97721494e8"
      },
      "source": [
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt\n",
        "from kerastuner import RandomSearch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 16.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 17.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAt9BIYZGtkr"
      },
      "source": [
        "#### Tuner 없는 버전"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aa3u2IdXeWg",
        "outputId": "7d9e0a08-3134-4d86-ed43-73cc63796efa"
      },
      "source": [
        "# Variable 초기화 \n",
        "K = 15\n",
        "epochs=500                            # Latent factor 수 \n",
        "mu = round(ratings_train.score.mean(),4)  # 전체 평균 \n",
        "M = len(ratings.index)    # Number of users\n",
        "N = len(ratings.columns) # Number of movies\n",
        "K,mu,M,N     \n",
        "\n",
        "# Keras model\n",
        "user = Input(shape=(1, ))                                               # User input\n",
        "item = Input(shape=(1, ))                                               # Item input\n",
        "P_embedding = Embedding(M, K, embeddings_regularizer=l2())(user)        # (M, 1, K)\n",
        "Q_embedding = Embedding(N, K, embeddings_regularizer=l2())(item)        # (N, 1, K)\n",
        "user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)          # User bias term (M, 1, )\n",
        "item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)          # Item bias term (N, 1, )\n",
        "\n",
        "# Concatenate layers\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Activation\n",
        "P_embedding = Flatten()(P_embedding)                                    # (K, )\n",
        "Q_embedding = Flatten()(Q_embedding)                                    # (K, )\n",
        "user_bias = Flatten()(user_bias)                                        # (1, )\n",
        "item_bias = Flatten()(item_bias)                                        # (1, )\n",
        "R = Concatenate()([P_embedding, Q_embedding, user_bias, item_bias])     # (2K + 2, )\n",
        "\n",
        "# Neural network\n",
        "R = Dense(128,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "R = Dropout(0.1)(R)\n",
        "R = Dense(64,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "R = Dropout(0.1)(R)\n",
        "R = Dense(32,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "R = Dense(1)(R)\n",
        "\n",
        "# Model setting\n",
        "starter_learning_rate = 0.1\n",
        "end_learning_rate = 0.0001\n",
        "decay_steps = int(0.1*epochs)\n",
        "learning_rate_fn = PolynomialDecay(\n",
        "    starter_learning_rate,\n",
        "    decay_steps,\n",
        "    end_learning_rate,\n",
        "    power=0.5)\n",
        "model = Model(inputs=[user, item], outputs=R)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "  loss=RMSE,\n",
        "  #optimizer=SGD(learning_rate=learning_rate_fn,momentum=0.9),\n",
        "  optimizer=Adam(learning_rate=learning_rate_fn),\n",
        "  metrics=[RMSE],\n",
        "  \n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Model fitting\n",
        "result = model.fit(\n",
        "  x=[ratings_train.users.values, ratings_train.tracks.values],\n",
        "  y=ratings_train.score.values - mu,\n",
        "  epochs=epochs,\n",
        "  batch_size=64,\n",
        "  validation_data=(\n",
        "    [ratings_test.users.values, ratings_test.tracks.values],\n",
        "    ratings_test.score.values - mu\n",
        "  )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_27\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_73 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_74 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_148 (Embedding)       (None, 1, 15)        435         input_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_149 (Embedding)       (None, 1, 15)        2430        input_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_150 (Embedding)       (None, 1, 1)         29          input_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_151 (Embedding)       (None, 1, 1)         162         input_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_78 (Flatten)            (None, 15)           0           embedding_148[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_79 (Flatten)            (None, 15)           0           embedding_149[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_80 (Flatten)            (None, 1)            0           embedding_150[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_81 (Flatten)            (None, 1)            0           embedding_151[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 32)           0           flatten_78[0][0]                 \n",
            "                                                                 flatten_79[0][0]                 \n",
            "                                                                 flatten_80[0][0]                 \n",
            "                                                                 flatten_81[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_61 (Dense)                (None, 128)          4224        concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 128)          0           dense_61[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_62 (Dense)                (None, 64)           8256        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 64)           0           dense_62[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_63 (Dense)                (None, 32)           2080        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_64 (Dense)                (None, 1)            33          dense_63[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 17,649\n",
            "Trainable params: 17,649\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "59/59 [==============================] - 1s 7ms/step - loss: 1.6112 - RMSE: 1.2513 - val_loss: 0.8274 - val_RMSE: 0.7803\n",
            "Epoch 2/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8155 - RMSE: 0.7972 - val_loss: 0.7870 - val_RMSE: 0.7860\n",
            "Epoch 3/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7917 - RMSE: 0.7916 - val_loss: 0.7870 - val_RMSE: 0.7861\n",
            "Epoch 4/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7986 - RMSE: 0.7986 - val_loss: 0.7838 - val_RMSE: 0.7820\n",
            "Epoch 5/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7892 - RMSE: 0.7892 - val_loss: 0.7862 - val_RMSE: 0.7853\n",
            "Epoch 6/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8034 - RMSE: 0.8034 - val_loss: 0.7834 - val_RMSE: 0.7817\n",
            "Epoch 7/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7935 - RMSE: 0.7935 - val_loss: 0.7812 - val_RMSE: 0.7799\n",
            "Epoch 8/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7933 - RMSE: 0.7933 - val_loss: 0.7823 - val_RMSE: 0.7807\n",
            "Epoch 9/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7996 - RMSE: 0.7996 - val_loss: 0.7823 - val_RMSE: 0.7807\n",
            "Epoch 10/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7932 - RMSE: 0.7931 - val_loss: 0.7815 - val_RMSE: 0.7802\n",
            "Epoch 11/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7989 - RMSE: 0.7989 - val_loss: 0.7869 - val_RMSE: 0.7860\n",
            "Epoch 12/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7879 - RMSE: 0.7879 - val_loss: 0.7867 - val_RMSE: 0.7858\n",
            "Epoch 13/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7938 - RMSE: 0.7938 - val_loss: 0.7825 - val_RMSE: 0.7814\n",
            "Epoch 14/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7881 - RMSE: 0.7881 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 15/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7961 - RMSE: 0.7961 - val_loss: 0.7818 - val_RMSE: 0.7802\n",
            "Epoch 16/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7851 - RMSE: 0.7851 - val_loss: 0.7850 - val_RMSE: 0.7832\n",
            "Epoch 17/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7948 - RMSE: 0.7948 - val_loss: 0.7823 - val_RMSE: 0.7812\n",
            "Epoch 18/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7915 - RMSE: 0.7915 - val_loss: 0.7848 - val_RMSE: 0.7838\n",
            "Epoch 19/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7996 - RMSE: 0.7996 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 20/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7987 - RMSE: 0.7987 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 21/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8083 - RMSE: 0.8083 - val_loss: 0.7821 - val_RMSE: 0.7809\n",
            "Epoch 22/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7842 - RMSE: 0.7842 - val_loss: 0.7823 - val_RMSE: 0.7807\n",
            "Epoch 23/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8081 - RMSE: 0.8081 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 24/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7890 - RMSE: 0.7890 - val_loss: 0.7825 - val_RMSE: 0.7808\n",
            "Epoch 25/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7913 - RMSE: 0.7913 - val_loss: 0.7812 - val_RMSE: 0.7797\n",
            "Epoch 26/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7975 - RMSE: 0.7975 - val_loss: 0.7819 - val_RMSE: 0.7803\n",
            "Epoch 27/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7863 - RMSE: 0.7863 - val_loss: 0.7833 - val_RMSE: 0.7822\n",
            "Epoch 28/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7945 - RMSE: 0.7945 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 29/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7906 - RMSE: 0.7906 - val_loss: 0.7813 - val_RMSE: 0.7800\n",
            "Epoch 30/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7913 - RMSE: 0.7912 - val_loss: 0.7870 - val_RMSE: 0.7861\n",
            "Epoch 31/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7898 - RMSE: 0.7898 - val_loss: 0.7833 - val_RMSE: 0.7816\n",
            "Epoch 32/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7861 - RMSE: 0.7861 - val_loss: 0.7853 - val_RMSE: 0.7844\n",
            "Epoch 33/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7973 - RMSE: 0.7973 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 34/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7930 - RMSE: 0.7930 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 35/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7944 - RMSE: 0.7944 - val_loss: 0.7834 - val_RMSE: 0.7817\n",
            "Epoch 36/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7968 - RMSE: 0.7968 - val_loss: 0.7815 - val_RMSE: 0.7802\n",
            "Epoch 37/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7981 - RMSE: 0.7981 - val_loss: 0.7827 - val_RMSE: 0.7810\n",
            "Epoch 38/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8002 - RMSE: 0.8002 - val_loss: 0.7869 - val_RMSE: 0.7860\n",
            "Epoch 39/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7997 - RMSE: 0.7997 - val_loss: 0.7817 - val_RMSE: 0.7805\n",
            "Epoch 40/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8005 - RMSE: 0.8005 - val_loss: 0.7829 - val_RMSE: 0.7812\n",
            "Epoch 41/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8012 - RMSE: 0.8011 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 42/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8012 - RMSE: 0.8012 - val_loss: 0.7827 - val_RMSE: 0.7816\n",
            "Epoch 43/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7954 - RMSE: 0.7954 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 44/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7938 - RMSE: 0.7938 - val_loss: 0.7823 - val_RMSE: 0.7811\n",
            "Epoch 45/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7784 - RMSE: 0.7784 - val_loss: 0.7850 - val_RMSE: 0.7841\n",
            "Epoch 46/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7921 - RMSE: 0.7921 - val_loss: 0.7835 - val_RMSE: 0.7817\n",
            "Epoch 47/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7905 - RMSE: 0.7905 - val_loss: 0.7814 - val_RMSE: 0.7801\n",
            "Epoch 48/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7936 - RMSE: 0.7935 - val_loss: 0.7906 - val_RMSE: 0.7899\n",
            "Epoch 49/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7906 - RMSE: 0.7906 - val_loss: 0.7815 - val_RMSE: 0.7802\n",
            "Epoch 50/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7970 - RMSE: 0.7970 - val_loss: 0.7835 - val_RMSE: 0.7825\n",
            "Epoch 51/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7960 - RMSE: 0.7959 - val_loss: 0.7813 - val_RMSE: 0.7800\n",
            "Epoch 52/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7947 - RMSE: 0.7947 - val_loss: 0.7886 - val_RMSE: 0.7878\n",
            "Epoch 53/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7968 - RMSE: 0.7968 - val_loss: 0.7823 - val_RMSE: 0.7807\n",
            "Epoch 54/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7883 - RMSE: 0.7883 - val_loss: 0.7815 - val_RMSE: 0.7803\n",
            "Epoch 55/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7978 - RMSE: 0.7978 - val_loss: 0.7814 - val_RMSE: 0.7801\n",
            "Epoch 56/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7990 - RMSE: 0.7990 - val_loss: 0.7878 - val_RMSE: 0.7858\n",
            "Epoch 57/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8000 - RMSE: 0.8000 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 58/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7917 - RMSE: 0.7917 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 59/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7895 - RMSE: 0.7896 - val_loss: 0.7819 - val_RMSE: 0.7807\n",
            "Epoch 60/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8003 - RMSE: 0.8003 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 61/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7963 - RMSE: 0.7963 - val_loss: 0.7820 - val_RMSE: 0.7804\n",
            "Epoch 62/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7999 - RMSE: 0.7999 - val_loss: 0.7815 - val_RMSE: 0.7800\n",
            "Epoch 63/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7885 - RMSE: 0.7886 - val_loss: 0.7815 - val_RMSE: 0.7802\n",
            "Epoch 64/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7960 - RMSE: 0.7960 - val_loss: 0.7824 - val_RMSE: 0.7807\n",
            "Epoch 65/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8002 - RMSE: 0.8003 - val_loss: 0.7812 - val_RMSE: 0.7799\n",
            "Epoch 66/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7999 - RMSE: 0.7998 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 67/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7945 - RMSE: 0.7945 - val_loss: 0.7821 - val_RMSE: 0.7805\n",
            "Epoch 68/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7872 - RMSE: 0.7872 - val_loss: 0.7878 - val_RMSE: 0.7859\n",
            "Epoch 69/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8066 - RMSE: 0.8066 - val_loss: 0.7913 - val_RMSE: 0.7906\n",
            "Epoch 70/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7927 - RMSE: 0.7927 - val_loss: 0.7856 - val_RMSE: 0.7846\n",
            "Epoch 71/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8036 - RMSE: 0.8036 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 72/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7977 - RMSE: 0.7977 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 73/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7959 - RMSE: 0.7959 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 74/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7928 - RMSE: 0.7928 - val_loss: 0.7816 - val_RMSE: 0.7803\n",
            "Epoch 75/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7966 - RMSE: 0.7966 - val_loss: 0.7901 - val_RMSE: 0.7893\n",
            "Epoch 76/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7965 - RMSE: 0.7965 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 77/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7995 - RMSE: 0.7995 - val_loss: 0.7918 - val_RMSE: 0.7911\n",
            "Epoch 78/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8035 - RMSE: 0.8035 - val_loss: 0.7887 - val_RMSE: 0.7879\n",
            "Epoch 79/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7965 - RMSE: 0.7965 - val_loss: 0.7821 - val_RMSE: 0.7805\n",
            "Epoch 80/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8054 - RMSE: 0.8054 - val_loss: 0.7832 - val_RMSE: 0.7815\n",
            "Epoch 81/200\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.7941 - RMSE: 0.7941 - val_loss: 0.7821 - val_RMSE: 0.7805\n",
            "Epoch 82/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7918 - RMSE: 0.7918 - val_loss: 0.7828 - val_RMSE: 0.7812\n",
            "Epoch 83/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8038 - RMSE: 0.8038 - val_loss: 0.7873 - val_RMSE: 0.7854\n",
            "Epoch 84/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7991 - RMSE: 0.7991 - val_loss: 0.7878 - val_RMSE: 0.7859\n",
            "Epoch 85/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8055 - RMSE: 0.8055 - val_loss: 0.7816 - val_RMSE: 0.7800\n",
            "Epoch 86/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7989 - RMSE: 0.7989 - val_loss: 0.7822 - val_RMSE: 0.7810\n",
            "Epoch 87/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7961 - RMSE: 0.7961 - val_loss: 0.7813 - val_RMSE: 0.7799\n",
            "Epoch 88/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8059 - RMSE: 0.8059 - val_loss: 0.7821 - val_RMSE: 0.7809\n",
            "Epoch 89/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8023 - RMSE: 0.8023 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 90/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8019 - RMSE: 0.8019 - val_loss: 0.7828 - val_RMSE: 0.7817\n",
            "Epoch 91/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7955 - RMSE: 0.7955 - val_loss: 0.7821 - val_RMSE: 0.7805\n",
            "Epoch 92/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7896 - RMSE: 0.7896 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 93/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7932 - RMSE: 0.7932 - val_loss: 0.7873 - val_RMSE: 0.7864\n",
            "Epoch 94/200\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8006 - RMSE: 0.8006 - val_loss: 0.7813 - val_RMSE: 0.7800\n",
            "Epoch 95/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7986 - RMSE: 0.7986 - val_loss: 0.7859 - val_RMSE: 0.7850\n",
            "Epoch 96/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7967 - RMSE: 0.7967 - val_loss: 0.7840 - val_RMSE: 0.7830\n",
            "Epoch 97/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7959 - RMSE: 0.7959 - val_loss: 0.7824 - val_RMSE: 0.7808\n",
            "Epoch 98/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7962 - RMSE: 0.7962 - val_loss: 0.7851 - val_RMSE: 0.7841\n",
            "Epoch 99/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8017 - RMSE: 0.8018 - val_loss: 0.7814 - val_RMSE: 0.7801\n",
            "Epoch 100/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7999 - RMSE: 0.7999 - val_loss: 0.7854 - val_RMSE: 0.7845\n",
            "Epoch 101/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7920 - RMSE: 0.7920 - val_loss: 0.7825 - val_RMSE: 0.7809\n",
            "Epoch 102/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7907 - RMSE: 0.7907 - val_loss: 0.7815 - val_RMSE: 0.7803\n",
            "Epoch 103/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7957 - RMSE: 0.7958 - val_loss: 0.7821 - val_RMSE: 0.7809\n",
            "Epoch 104/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8066 - RMSE: 0.8066 - val_loss: 0.7825 - val_RMSE: 0.7813\n",
            "Epoch 105/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7969 - RMSE: 0.7969 - val_loss: 0.7851 - val_RMSE: 0.7841\n",
            "Epoch 106/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7956 - RMSE: 0.7956 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 107/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7914 - RMSE: 0.7914 - val_loss: 0.7881 - val_RMSE: 0.7873\n",
            "Epoch 108/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8041 - RMSE: 0.8041 - val_loss: 0.7819 - val_RMSE: 0.7807\n",
            "Epoch 109/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7992 - RMSE: 0.7992 - val_loss: 0.7842 - val_RMSE: 0.7831\n",
            "Epoch 110/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7902 - RMSE: 0.7902 - val_loss: 0.7818 - val_RMSE: 0.7806\n",
            "Epoch 111/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7905 - RMSE: 0.7905 - val_loss: 0.7870 - val_RMSE: 0.7861\n",
            "Epoch 112/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8021 - RMSE: 0.8021 - val_loss: 0.7851 - val_RMSE: 0.7833\n",
            "Epoch 113/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7879 - RMSE: 0.7880 - val_loss: 0.7814 - val_RMSE: 0.7799\n",
            "Epoch 114/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7916 - RMSE: 0.7916 - val_loss: 0.7814 - val_RMSE: 0.7801\n",
            "Epoch 115/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8001 - RMSE: 0.8001 - val_loss: 0.7848 - val_RMSE: 0.7838\n",
            "Epoch 116/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8038 - RMSE: 0.8038 - val_loss: 0.7826 - val_RMSE: 0.7810\n",
            "Epoch 117/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7958 - RMSE: 0.7958 - val_loss: 0.7831 - val_RMSE: 0.7820\n",
            "Epoch 118/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7985 - RMSE: 0.7985 - val_loss: 0.7822 - val_RMSE: 0.7810\n",
            "Epoch 119/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7957 - RMSE: 0.7957 - val_loss: 0.7827 - val_RMSE: 0.7811\n",
            "Epoch 120/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8036 - RMSE: 0.8036 - val_loss: 0.7815 - val_RMSE: 0.7802\n",
            "Epoch 121/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7983 - RMSE: 0.7983 - val_loss: 0.7832 - val_RMSE: 0.7815\n",
            "Epoch 122/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7851 - RMSE: 0.7851 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 123/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7901 - RMSE: 0.7901 - val_loss: 0.7820 - val_RMSE: 0.7808\n",
            "Epoch 124/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7917 - RMSE: 0.7917 - val_loss: 0.7824 - val_RMSE: 0.7813\n",
            "Epoch 125/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7940 - RMSE: 0.7941 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 126/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7953 - RMSE: 0.7953 - val_loss: 0.7815 - val_RMSE: 0.7803\n",
            "Epoch 127/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7864 - RMSE: 0.7864 - val_loss: 0.7833 - val_RMSE: 0.7822\n",
            "Epoch 128/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7969 - RMSE: 0.7969 - val_loss: 0.7814 - val_RMSE: 0.7801\n",
            "Epoch 129/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7952 - RMSE: 0.7952 - val_loss: 0.7823 - val_RMSE: 0.7811\n",
            "Epoch 130/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7824 - RMSE: 0.7824 - val_loss: 0.7835 - val_RMSE: 0.7824\n",
            "Epoch 131/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7911 - RMSE: 0.7911 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 132/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7959 - RMSE: 0.7959 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 133/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8026 - RMSE: 0.8026 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 134/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7879 - RMSE: 0.7879 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 135/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7911 - RMSE: 0.7911 - val_loss: 0.7816 - val_RMSE: 0.7800\n",
            "Epoch 136/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7864 - RMSE: 0.7864 - val_loss: 0.7814 - val_RMSE: 0.7799\n",
            "Epoch 137/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7966 - RMSE: 0.7966 - val_loss: 0.7837 - val_RMSE: 0.7820\n",
            "Epoch 138/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7939 - RMSE: 0.7939 - val_loss: 0.7826 - val_RMSE: 0.7810\n",
            "Epoch 139/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8014 - RMSE: 0.8014 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 140/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8037 - RMSE: 0.8037 - val_loss: 0.7816 - val_RMSE: 0.7800\n",
            "Epoch 141/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7927 - RMSE: 0.7927 - val_loss: 0.7816 - val_RMSE: 0.7803\n",
            "Epoch 142/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7964 - RMSE: 0.7964 - val_loss: 0.7815 - val_RMSE: 0.7799\n",
            "Epoch 143/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7969 - RMSE: 0.7968 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 144/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7953 - RMSE: 0.7953 - val_loss: 0.7855 - val_RMSE: 0.7837\n",
            "Epoch 145/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7964 - RMSE: 0.7964 - val_loss: 0.7812 - val_RMSE: 0.7797\n",
            "Epoch 146/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7984 - RMSE: 0.7984 - val_loss: 0.7812 - val_RMSE: 0.7797\n",
            "Epoch 147/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7961 - RMSE: 0.7961 - val_loss: 0.7814 - val_RMSE: 0.7799\n",
            "Epoch 148/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8026 - RMSE: 0.8026 - val_loss: 0.7814 - val_RMSE: 0.7801\n",
            "Epoch 149/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7954 - RMSE: 0.7954 - val_loss: 0.7838 - val_RMSE: 0.7820\n",
            "Epoch 150/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7964 - RMSE: 0.7964 - val_loss: 0.7813 - val_RMSE: 0.7799\n",
            "Epoch 151/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7865 - RMSE: 0.7865 - val_loss: 0.7820 - val_RMSE: 0.7808\n",
            "Epoch 152/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8070 - RMSE: 0.8070 - val_loss: 0.7846 - val_RMSE: 0.7828\n",
            "Epoch 153/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8019 - RMSE: 0.8019 - val_loss: 0.7832 - val_RMSE: 0.7815\n",
            "Epoch 154/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8022 - RMSE: 0.8022 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 155/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7928 - RMSE: 0.7929 - val_loss: 0.7814 - val_RMSE: 0.7799\n",
            "Epoch 156/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7913 - RMSE: 0.7913 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 157/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7956 - RMSE: 0.7956 - val_loss: 0.7815 - val_RMSE: 0.7800\n",
            "Epoch 158/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7947 - RMSE: 0.7947 - val_loss: 0.7816 - val_RMSE: 0.7804\n",
            "Epoch 159/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7953 - RMSE: 0.7953 - val_loss: 0.7815 - val_RMSE: 0.7802\n",
            "Epoch 160/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7875 - RMSE: 0.7875 - val_loss: 0.7813 - val_RMSE: 0.7800\n",
            "Epoch 161/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7968 - RMSE: 0.7968 - val_loss: 0.7815 - val_RMSE: 0.7802\n",
            "Epoch 162/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7977 - RMSE: 0.7977 - val_loss: 0.7812 - val_RMSE: 0.7797\n",
            "Epoch 163/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7957 - RMSE: 0.7957 - val_loss: 0.7813 - val_RMSE: 0.7800\n",
            "Epoch 164/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7865 - RMSE: 0.7865 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 165/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8008 - RMSE: 0.8008 - val_loss: 0.7815 - val_RMSE: 0.7802\n",
            "Epoch 166/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7967 - RMSE: 0.7967 - val_loss: 0.7812 - val_RMSE: 0.7799\n",
            "Epoch 167/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7853 - RMSE: 0.7853 - val_loss: 0.7813 - val_RMSE: 0.7800\n",
            "Epoch 168/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7944 - RMSE: 0.7944 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 169/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7940 - RMSE: 0.7940 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 170/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7876 - RMSE: 0.7876 - val_loss: 0.7815 - val_RMSE: 0.7800\n",
            "Epoch 171/200\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.7908 - RMSE: 0.7908 - val_loss: 0.7815 - val_RMSE: 0.7799\n",
            "Epoch 172/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8028 - RMSE: 0.8028 - val_loss: 0.7814 - val_RMSE: 0.7799\n",
            "Epoch 173/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7912 - RMSE: 0.7912 - val_loss: 0.7814 - val_RMSE: 0.7799\n",
            "Epoch 174/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8008 - RMSE: 0.8008 - val_loss: 0.7814 - val_RMSE: 0.7799\n",
            "Epoch 175/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7881 - RMSE: 0.7881 - val_loss: 0.7814 - val_RMSE: 0.7799\n",
            "Epoch 176/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8073 - RMSE: 0.8073 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 177/200\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.8007 - RMSE: 0.8007 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 178/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8058 - RMSE: 0.8058 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 179/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7907 - RMSE: 0.7907 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 180/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7915 - RMSE: 0.7916 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 181/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7943 - RMSE: 0.7943 - val_loss: 0.7813 - val_RMSE: 0.7798\n",
            "Epoch 182/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7942 - RMSE: 0.7942 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 183/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7947 - RMSE: 0.7947 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 184/200\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.7914 - RMSE: 0.7914 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 185/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7869 - RMSE: 0.7869 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 186/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7905 - RMSE: 0.7905 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 187/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7906 - RMSE: 0.7907 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 188/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7890 - RMSE: 0.7890 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 189/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7868 - RMSE: 0.7867 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 190/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7880 - RMSE: 0.7880 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 191/200\n",
            "59/59 [==============================] - 0s 4ms/step - loss: 0.7892 - RMSE: 0.7892 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 192/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7994 - RMSE: 0.7994 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 193/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7930 - RMSE: 0.7930 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 194/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7933 - RMSE: 0.7933 - val_loss: 0.7812 - val_RMSE: 0.7798\n",
            "Epoch 195/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7863 - RMSE: 0.7863 - val_loss: 0.7812 - val_RMSE: 0.7797\n",
            "Epoch 196/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7970 - RMSE: 0.7970 - val_loss: 0.7812 - val_RMSE: 0.7797\n",
            "Epoch 197/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7861 - RMSE: 0.7861 - val_loss: 0.7812 - val_RMSE: 0.7797\n",
            "Epoch 198/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7869 - RMSE: 0.7869 - val_loss: 0.7812 - val_RMSE: 0.7797\n",
            "Epoch 199/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7788 - RMSE: 0.7787 - val_loss: 0.7812 - val_RMSE: 0.7797\n",
            "Epoch 200/200\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7960 - RMSE: 0.7960 - val_loss: 0.7812 - val_RMSE: 0.7797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhswTrFxjxIs",
        "outputId": "5ec7ceff-0dd0-4c11-d7c6-62caaede7bf9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(result.history['RMSE'], label=\"Train RMSE\")\n",
        "plt.plot(result.history['val_RMSE'], label=\"Test RMSE\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TRryUfckGR_",
        "outputId": "423336ed-8066-420a-d440-bf9757f6934b"
      },
      "source": [
        "# Prediction\n",
        "user = ratings_test.users.values[0:5]\n",
        "track = ratings_test.tracks.values[0:5]\n",
        "predictions = model.predict([user, track]) + mu\n",
        "print(\"Actuals: \\n\", ratings_test.iloc[0:5])\n",
        "print()\n",
        "print(\"Predictions: \\n\", predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actuals: \n",
            "       users  tracks  score\n",
            "3037     18     121   -1.0\n",
            "3038     18     122    1.0\n",
            "3039     18     123    1.0\n",
            "3040     18     124    1.0\n",
            "3041     18     125    0.0\n",
            "\n",
            "Predictions: \n",
            " [[0.3445786]\n",
            " [0.3445786]\n",
            " [0.3445786]\n",
            " [0.3445786]\n",
            " [0.3445786]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58mf-9zWexxg"
      },
      "source": [
        "#### Tuner 있는 버전"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "holxnd84zMej",
        "outputId": "1fdfc018-7b0c-47e2-a3cf-0e9bfada318f"
      },
      "source": [
        "EPOCHS = 400\n",
        "def build_model(hp):\n",
        "# Keras model\n",
        "\n",
        "  K = hp.Int('K',min_value=int(0.5*M),max_value=M,step=1)\n",
        "  user = Input(shape=(1, ))                                               # User input\n",
        "  item = Input(shape=(1, ))                                               # Item input\n",
        "  P_embedding = Embedding(M, K, embeddings_regularizer=l2())(user)        # (M, 1, K)\n",
        "  Q_embedding = Embedding(N, K, embeddings_regularizer=l2())(item)        # (N, 1, K)\n",
        "  user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)          # User bias term (M, 1, )\n",
        "  item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)          # Item bias term (N, 1, )\n",
        "\n",
        "  # Concatenate layers\n",
        "  \n",
        "  P_embedding = Flatten()(P_embedding)                                    # (K, )\n",
        "  Q_embedding = Flatten()(Q_embedding)                                    # (K, )\n",
        "  user_bias = Flatten()(user_bias)                                        # (1, )\n",
        "  item_bias = Flatten()(item_bias)                                        # (1, )\n",
        "  R = Concatenate()([P_embedding, Q_embedding, user_bias, item_bias])     # (2K + 2, )\n",
        "\n",
        "  # Neural network\n",
        "  case = hp.Int('Case' , min_value=1,max_value=2,step=1)\n",
        "  if case ==1:\n",
        "    neurons = hp.Int('Nuerons' , min_value=16,max_value=128,step=16)\n",
        "    R = Dense(neurons,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "  \n",
        "  elif case ==2:\n",
        "    neuron_1 = hp.Int('Nuerons' , min_value=64,max_value=128,step=16)\n",
        "    neuron_2 = int(0.5*neuron_1)\n",
        "    R = Dense(neuron_1,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "    R = Dense(neuron_2,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "  R = Dense(1)(R)\n",
        "\n",
        "\n",
        "  # Model setting\n",
        "  starter_learning_rate = 0.1\n",
        "  end_learning_rate = 0.0001\n",
        "  decay_steps = int(0.1*EPOCHS)\n",
        "  learning_rate_fn = PolynomialDecay(\n",
        "      starter_learning_rate,\n",
        "      decay_steps,\n",
        "      end_learning_rate,\n",
        "      power=0.5)\n",
        "  \n",
        "  model = Model(inputs=[user, item], outputs=R)\n",
        "  optimizer = hp.Choice('Optimizer',['SGD','Adam'])\n",
        "  if optimizer == 'Adam':\n",
        "    model.compile(\n",
        "      loss=RMSE,\n",
        "      optimizer=Adam(learning_rate=learning_rate_fn),\n",
        "      metrics=[RMSE]\n",
        "      \n",
        "    )\n",
        "  elif optimizer =='SGD':\n",
        "    model.compile(\n",
        "      loss=RMSE,\n",
        "      optimizer=SGD(learning_rate=learning_rate_fn,momentum=0.9),\n",
        "      metrics=[RMSE]\n",
        "    )\n",
        "  return model\n",
        "\n",
        "hp = kt.HyperParameters()\n",
        "\n",
        "# Initialize the tuner by passing the `build_model` function\n",
        "# and specifying key search constraints: maximize val_acc (objective),\n",
        "# and the number of trials to do. More efficient tuners like UltraBand() can\n",
        "# be used.\n",
        "TRIALS = 30\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_loss\",\n",
        "    max_trials=TRIALS,\n",
        "    project_name=\"Spotify Recommendation Systems\",\n",
        ")\n",
        "\n",
        "# Display search space overview\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Perform the model search. The search function has the same signature\n",
        "# as `model.fit()`.\n",
        "tuner.search(\n",
        "    [ratings_train.users.values, ratings_train.tracks.values],ratings_train.score.values - mu,batch_size=64, epochs=EPOCHS, \n",
        "    validation_data=([ratings_valid.users.values, ratings_valid.tracks.values], ratings_valid.score.values - mu)\n",
        ")\n",
        "\n",
        "# Display the best models, their hyperparameters, and the resulting metrics.\n",
        "tuner.results_summary()\n",
        "\n",
        "# Retrieve the best model and display its architecture\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "result = best_model.fit([ratings_train.users.values, ratings_train.tracks.values],ratings_train.score.values - mu,\n",
        "               batch_size=64, \n",
        "               epochs=EPOCHS,\n",
        "               validation_data=([ratings_valid.users.values, ratings_valid.tracks.values], ratings_valid.score.values - mu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 30 Complete [00h 01m 36s]\n",
            "val_loss: 0.7727070450782776\n",
            "\n",
            "Best val_loss So Far: 0.7622820734977722\n",
            "Total elapsed time: 00h 47m 20s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Results summary\n",
            "Results in ./Spotify Recommendation Systems\n",
            "Showing 10 best trials\n",
            "Objective(name='val_loss', direction='min')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 27\n",
            "Case: 2\n",
            "Nuerons: 32\n",
            "Optimizer: False\n",
            "Score: 0.7622820734977722\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 22\n",
            "Case: 1\n",
            "Nuerons: 32\n",
            "Optimizer: True\n",
            "Score: 0.7622963786125183\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 14\n",
            "Case: 2\n",
            "Nuerons: 112\n",
            "Optimizer: False\n",
            "Score: 0.7631160616874695\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 24\n",
            "Case: 2\n",
            "Nuerons: 16\n",
            "Optimizer: False\n",
            "Score: 0.7635301351547241\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 20\n",
            "Case: 2\n",
            "Nuerons: 96\n",
            "Optimizer: False\n",
            "Score: 0.7645853161811829\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 15\n",
            "Case: 2\n",
            "Nuerons: 64\n",
            "Optimizer: False\n",
            "Score: 0.76480633020401\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 18\n",
            "Case: 2\n",
            "Nuerons: 16\n",
            "Optimizer: False\n",
            "Score: 0.7654535174369812\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 23\n",
            "Case: 2\n",
            "Nuerons: 48\n",
            "Optimizer: False\n",
            "Score: 0.7662022113800049\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 28\n",
            "Case: 1\n",
            "Nuerons: 48\n",
            "Optimizer: False\n",
            "Score: 0.7670589089393616\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 16\n",
            "Case: 2\n",
            "Nuerons: 96\n",
            "Optimizer: False\n",
            "Score: 0.76804518699646\n",
            "Epoch 1/500\n",
            "59/59 [==============================] - 1s 6ms/step - loss: 0.7382 - RMSE: 0.7268 - val_loss: 0.7623 - val_RMSE: 0.7525\n",
            "Epoch 2/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7313 - RMSE: 0.7198 - val_loss: 0.7624 - val_RMSE: 0.7525\n",
            "Epoch 3/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7345 - RMSE: 0.7231 - val_loss: 0.7623 - val_RMSE: 0.7524\n",
            "Epoch 4/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7369 - RMSE: 0.7255 - val_loss: 0.7624 - val_RMSE: 0.7525\n",
            "Epoch 5/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7406 - RMSE: 0.7291 - val_loss: 0.7623 - val_RMSE: 0.7524\n",
            "Epoch 6/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7370 - RMSE: 0.7255 - val_loss: 0.7624 - val_RMSE: 0.7525\n",
            "Epoch 7/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7302 - RMSE: 0.7188 - val_loss: 0.7624 - val_RMSE: 0.7525\n",
            "Epoch 8/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7368 - RMSE: 0.7253 - val_loss: 0.7624 - val_RMSE: 0.7524\n",
            "Epoch 9/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7281 - RMSE: 0.7166 - val_loss: 0.7624 - val_RMSE: 0.7524\n",
            "Epoch 10/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7341 - RMSE: 0.7227 - val_loss: 0.7624 - val_RMSE: 0.7524\n",
            "Epoch 11/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7368 - RMSE: 0.7253 - val_loss: 0.7624 - val_RMSE: 0.7524\n",
            "Epoch 12/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7299 - RMSE: 0.7184 - val_loss: 0.7625 - val_RMSE: 0.7524\n",
            "Epoch 13/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7368 - RMSE: 0.7253 - val_loss: 0.7625 - val_RMSE: 0.7524\n",
            "Epoch 14/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7340 - RMSE: 0.7225 - val_loss: 0.7625 - val_RMSE: 0.7524\n",
            "Epoch 15/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7346 - RMSE: 0.7231 - val_loss: 0.7625 - val_RMSE: 0.7524\n",
            "Epoch 16/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7314 - RMSE: 0.7199 - val_loss: 0.7625 - val_RMSE: 0.7525\n",
            "Epoch 17/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7415 - RMSE: 0.7300 - val_loss: 0.7626 - val_RMSE: 0.7525\n",
            "Epoch 18/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7371 - RMSE: 0.7256 - val_loss: 0.7625 - val_RMSE: 0.7524\n",
            "Epoch 19/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7341 - RMSE: 0.7226 - val_loss: 0.7626 - val_RMSE: 0.7524\n",
            "Epoch 20/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7374 - RMSE: 0.7259 - val_loss: 0.7625 - val_RMSE: 0.7524\n",
            "Epoch 21/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7350 - RMSE: 0.7235 - val_loss: 0.7626 - val_RMSE: 0.7524\n",
            "Epoch 22/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7333 - RMSE: 0.7218 - val_loss: 0.7627 - val_RMSE: 0.7525\n",
            "Epoch 23/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7348 - RMSE: 0.7233 - val_loss: 0.7627 - val_RMSE: 0.7525\n",
            "Epoch 24/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7245 - RMSE: 0.7129 - val_loss: 0.7627 - val_RMSE: 0.7524\n",
            "Epoch 25/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7282 - RMSE: 0.7167 - val_loss: 0.7628 - val_RMSE: 0.7525\n",
            "Epoch 26/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7300 - RMSE: 0.7185 - val_loss: 0.7627 - val_RMSE: 0.7525\n",
            "Epoch 27/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7417 - RMSE: 0.7301 - val_loss: 0.7628 - val_RMSE: 0.7525\n",
            "Epoch 28/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7195 - RMSE: 0.7079 - val_loss: 0.7628 - val_RMSE: 0.7525\n",
            "Epoch 29/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7242 - RMSE: 0.7126 - val_loss: 0.7628 - val_RMSE: 0.7525\n",
            "Epoch 30/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7272 - RMSE: 0.7156 - val_loss: 0.7629 - val_RMSE: 0.7526\n",
            "Epoch 31/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7208 - RMSE: 0.7093 - val_loss: 0.7628 - val_RMSE: 0.7525\n",
            "Epoch 32/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7256 - RMSE: 0.7141 - val_loss: 0.7629 - val_RMSE: 0.7526\n",
            "Epoch 33/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7230 - RMSE: 0.7114 - val_loss: 0.7629 - val_RMSE: 0.7526\n",
            "Epoch 34/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7401 - RMSE: 0.7285 - val_loss: 0.7629 - val_RMSE: 0.7526\n",
            "Epoch 35/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7315 - RMSE: 0.7199 - val_loss: 0.7630 - val_RMSE: 0.7526\n",
            "Epoch 36/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7218 - RMSE: 0.7102 - val_loss: 0.7630 - val_RMSE: 0.7526\n",
            "Epoch 37/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7379 - RMSE: 0.7263 - val_loss: 0.7631 - val_RMSE: 0.7526\n",
            "Epoch 38/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7291 - RMSE: 0.7175 - val_loss: 0.7631 - val_RMSE: 0.7526\n",
            "Epoch 39/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7281 - RMSE: 0.7165 - val_loss: 0.7631 - val_RMSE: 0.7527\n",
            "Epoch 40/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7291 - RMSE: 0.7175 - val_loss: 0.7632 - val_RMSE: 0.7527\n",
            "Epoch 41/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7373 - RMSE: 0.7258 - val_loss: 0.7632 - val_RMSE: 0.7527\n",
            "Epoch 42/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7319 - RMSE: 0.7203 - val_loss: 0.7632 - val_RMSE: 0.7527\n",
            "Epoch 43/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7299 - RMSE: 0.7183 - val_loss: 0.7633 - val_RMSE: 0.7528\n",
            "Epoch 44/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7310 - RMSE: 0.7194 - val_loss: 0.7633 - val_RMSE: 0.7528\n",
            "Epoch 45/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7371 - RMSE: 0.7255 - val_loss: 0.7634 - val_RMSE: 0.7528\n",
            "Epoch 46/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7288 - RMSE: 0.7171 - val_loss: 0.7634 - val_RMSE: 0.7528\n",
            "Epoch 47/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7405 - RMSE: 0.7288 - val_loss: 0.7634 - val_RMSE: 0.7528\n",
            "Epoch 48/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7295 - RMSE: 0.7179 - val_loss: 0.7634 - val_RMSE: 0.7528\n",
            "Epoch 49/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7324 - RMSE: 0.7208 - val_loss: 0.7634 - val_RMSE: 0.7528\n",
            "Epoch 50/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7279 - RMSE: 0.7163 - val_loss: 0.7635 - val_RMSE: 0.7529\n",
            "Epoch 51/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7298 - RMSE: 0.7181 - val_loss: 0.7635 - val_RMSE: 0.7529\n",
            "Epoch 52/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7300 - RMSE: 0.7184 - val_loss: 0.7636 - val_RMSE: 0.7530\n",
            "Epoch 53/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7289 - RMSE: 0.7173 - val_loss: 0.7636 - val_RMSE: 0.7530\n",
            "Epoch 54/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7332 - RMSE: 0.7216 - val_loss: 0.7637 - val_RMSE: 0.7530\n",
            "Epoch 55/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7412 - RMSE: 0.7296 - val_loss: 0.7637 - val_RMSE: 0.7531\n",
            "Epoch 56/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7349 - RMSE: 0.7232 - val_loss: 0.7638 - val_RMSE: 0.7531\n",
            "Epoch 57/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7278 - RMSE: 0.7161 - val_loss: 0.7638 - val_RMSE: 0.7531\n",
            "Epoch 58/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7233 - RMSE: 0.7117 - val_loss: 0.7638 - val_RMSE: 0.7531\n",
            "Epoch 59/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7260 - RMSE: 0.7144 - val_loss: 0.7639 - val_RMSE: 0.7532\n",
            "Epoch 60/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7258 - RMSE: 0.7141 - val_loss: 0.7639 - val_RMSE: 0.7532\n",
            "Epoch 61/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7339 - RMSE: 0.7222 - val_loss: 0.7640 - val_RMSE: 0.7532\n",
            "Epoch 62/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7243 - RMSE: 0.7127 - val_loss: 0.7640 - val_RMSE: 0.7533\n",
            "Epoch 63/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7294 - RMSE: 0.7177 - val_loss: 0.7641 - val_RMSE: 0.7533\n",
            "Epoch 64/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7394 - RMSE: 0.7277 - val_loss: 0.7642 - val_RMSE: 0.7534\n",
            "Epoch 65/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7370 - RMSE: 0.7253 - val_loss: 0.7642 - val_RMSE: 0.7534\n",
            "Epoch 66/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7261 - RMSE: 0.7145 - val_loss: 0.7643 - val_RMSE: 0.7535\n",
            "Epoch 67/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7346 - RMSE: 0.7229 - val_loss: 0.7643 - val_RMSE: 0.7535\n",
            "Epoch 68/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7295 - RMSE: 0.7179 - val_loss: 0.7644 - val_RMSE: 0.7536\n",
            "Epoch 69/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7306 - RMSE: 0.7190 - val_loss: 0.7644 - val_RMSE: 0.7536\n",
            "Epoch 70/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7247 - RMSE: 0.7130 - val_loss: 0.7644 - val_RMSE: 0.7536\n",
            "Epoch 71/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7385 - RMSE: 0.7269 - val_loss: 0.7645 - val_RMSE: 0.7536\n",
            "Epoch 72/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7328 - RMSE: 0.7212 - val_loss: 0.7645 - val_RMSE: 0.7537\n",
            "Epoch 73/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7271 - RMSE: 0.7155 - val_loss: 0.7646 - val_RMSE: 0.7537\n",
            "Epoch 74/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7360 - RMSE: 0.7243 - val_loss: 0.7646 - val_RMSE: 0.7538\n",
            "Epoch 75/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7378 - RMSE: 0.7262 - val_loss: 0.7646 - val_RMSE: 0.7538\n",
            "Epoch 76/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7228 - RMSE: 0.7111 - val_loss: 0.7647 - val_RMSE: 0.7538\n",
            "Epoch 77/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7250 - RMSE: 0.7134 - val_loss: 0.7648 - val_RMSE: 0.7539\n",
            "Epoch 78/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7229 - RMSE: 0.7113 - val_loss: 0.7648 - val_RMSE: 0.7539\n",
            "Epoch 79/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7321 - RMSE: 0.7204 - val_loss: 0.7648 - val_RMSE: 0.7539\n",
            "Epoch 80/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7327 - RMSE: 0.7210 - val_loss: 0.7649 - val_RMSE: 0.7540\n",
            "Epoch 81/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7202 - RMSE: 0.7085 - val_loss: 0.7649 - val_RMSE: 0.7540\n",
            "Epoch 82/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7261 - RMSE: 0.7145 - val_loss: 0.7650 - val_RMSE: 0.7540\n",
            "Epoch 83/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7326 - RMSE: 0.7210 - val_loss: 0.7650 - val_RMSE: 0.7541\n",
            "Epoch 84/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7230 - RMSE: 0.7114 - val_loss: 0.7651 - val_RMSE: 0.7541\n",
            "Epoch 85/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7326 - RMSE: 0.7210 - val_loss: 0.7652 - val_RMSE: 0.7542\n",
            "Epoch 86/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7253 - RMSE: 0.7137 - val_loss: 0.7652 - val_RMSE: 0.7543\n",
            "Epoch 87/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7361 - RMSE: 0.7245 - val_loss: 0.7652 - val_RMSE: 0.7543\n",
            "Epoch 88/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7241 - RMSE: 0.7124 - val_loss: 0.7653 - val_RMSE: 0.7543\n",
            "Epoch 89/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7440 - RMSE: 0.7324 - val_loss: 0.7653 - val_RMSE: 0.7543\n",
            "Epoch 90/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7320 - RMSE: 0.7203 - val_loss: 0.7654 - val_RMSE: 0.7544\n",
            "Epoch 91/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7320 - RMSE: 0.7203 - val_loss: 0.7654 - val_RMSE: 0.7544\n",
            "Epoch 92/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7200 - RMSE: 0.7084 - val_loss: 0.7654 - val_RMSE: 0.7544\n",
            "Epoch 93/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7227 - RMSE: 0.7110 - val_loss: 0.7655 - val_RMSE: 0.7545\n",
            "Epoch 94/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7243 - RMSE: 0.7127 - val_loss: 0.7656 - val_RMSE: 0.7546\n",
            "Epoch 95/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7352 - RMSE: 0.7235 - val_loss: 0.7656 - val_RMSE: 0.7546\n",
            "Epoch 96/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7240 - RMSE: 0.7124 - val_loss: 0.7656 - val_RMSE: 0.7546\n",
            "Epoch 97/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7263 - RMSE: 0.7147 - val_loss: 0.7657 - val_RMSE: 0.7547\n",
            "Epoch 98/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7389 - RMSE: 0.7273 - val_loss: 0.7658 - val_RMSE: 0.7547\n",
            "Epoch 99/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7367 - RMSE: 0.7251 - val_loss: 0.7658 - val_RMSE: 0.7548\n",
            "Epoch 100/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7317 - RMSE: 0.7201 - val_loss: 0.7659 - val_RMSE: 0.7549\n",
            "Epoch 101/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7409 - RMSE: 0.7294 - val_loss: 0.7659 - val_RMSE: 0.7549\n",
            "Epoch 102/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7350 - RMSE: 0.7234 - val_loss: 0.7660 - val_RMSE: 0.7549\n",
            "Epoch 103/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7191 - RMSE: 0.7075 - val_loss: 0.7660 - val_RMSE: 0.7550\n",
            "Epoch 104/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7315 - RMSE: 0.7199 - val_loss: 0.7660 - val_RMSE: 0.7550\n",
            "Epoch 105/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7284 - RMSE: 0.7168 - val_loss: 0.7661 - val_RMSE: 0.7550\n",
            "Epoch 106/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7186 - RMSE: 0.7071 - val_loss: 0.7661 - val_RMSE: 0.7551\n",
            "Epoch 107/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7224 - RMSE: 0.7108 - val_loss: 0.7662 - val_RMSE: 0.7551\n",
            "Epoch 108/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7286 - RMSE: 0.7170 - val_loss: 0.7662 - val_RMSE: 0.7552\n",
            "Epoch 109/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7244 - RMSE: 0.7128 - val_loss: 0.7663 - val_RMSE: 0.7552\n",
            "Epoch 110/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7167 - RMSE: 0.7051 - val_loss: 0.7663 - val_RMSE: 0.7552\n",
            "Epoch 111/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7249 - RMSE: 0.7133 - val_loss: 0.7663 - val_RMSE: 0.7553\n",
            "Epoch 112/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7166 - RMSE: 0.7050 - val_loss: 0.7664 - val_RMSE: 0.7553\n",
            "Epoch 113/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7333 - RMSE: 0.7217 - val_loss: 0.7664 - val_RMSE: 0.7553\n",
            "Epoch 114/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7306 - RMSE: 0.7190 - val_loss: 0.7665 - val_RMSE: 0.7554\n",
            "Epoch 115/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7192 - RMSE: 0.7077 - val_loss: 0.7665 - val_RMSE: 0.7554\n",
            "Epoch 116/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7166 - RMSE: 0.7051 - val_loss: 0.7665 - val_RMSE: 0.7554\n",
            "Epoch 117/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7218 - RMSE: 0.7103 - val_loss: 0.7666 - val_RMSE: 0.7555\n",
            "Epoch 118/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7202 - RMSE: 0.7086 - val_loss: 0.7666 - val_RMSE: 0.7555\n",
            "Epoch 119/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7403 - RMSE: 0.7288 - val_loss: 0.7667 - val_RMSE: 0.7556\n",
            "Epoch 120/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7205 - RMSE: 0.7090 - val_loss: 0.7667 - val_RMSE: 0.7556\n",
            "Epoch 121/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7262 - RMSE: 0.7146 - val_loss: 0.7668 - val_RMSE: 0.7557\n",
            "Epoch 122/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7202 - RMSE: 0.7087 - val_loss: 0.7669 - val_RMSE: 0.7558\n",
            "Epoch 123/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7231 - RMSE: 0.7116 - val_loss: 0.7669 - val_RMSE: 0.7558\n",
            "Epoch 124/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7240 - RMSE: 0.7125 - val_loss: 0.7669 - val_RMSE: 0.7558\n",
            "Epoch 125/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7244 - RMSE: 0.7129 - val_loss: 0.7670 - val_RMSE: 0.7559\n",
            "Epoch 126/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7230 - RMSE: 0.7115 - val_loss: 0.7670 - val_RMSE: 0.7559\n",
            "Epoch 127/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7220 - RMSE: 0.7105 - val_loss: 0.7670 - val_RMSE: 0.7560\n",
            "Epoch 128/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7245 - RMSE: 0.7130 - val_loss: 0.7671 - val_RMSE: 0.7560\n",
            "Epoch 129/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7207 - RMSE: 0.7092 - val_loss: 0.7671 - val_RMSE: 0.7560\n",
            "Epoch 130/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7248 - RMSE: 0.7133 - val_loss: 0.7672 - val_RMSE: 0.7561\n",
            "Epoch 131/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7278 - RMSE: 0.7164 - val_loss: 0.7672 - val_RMSE: 0.7561\n",
            "Epoch 132/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7288 - RMSE: 0.7173 - val_loss: 0.7673 - val_RMSE: 0.7562\n",
            "Epoch 133/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7240 - RMSE: 0.7125 - val_loss: 0.7673 - val_RMSE: 0.7562\n",
            "Epoch 134/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7254 - RMSE: 0.7140 - val_loss: 0.7673 - val_RMSE: 0.7562\n",
            "Epoch 135/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7254 - RMSE: 0.7140 - val_loss: 0.7673 - val_RMSE: 0.7562\n",
            "Epoch 136/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7305 - RMSE: 0.7191 - val_loss: 0.7674 - val_RMSE: 0.7563\n",
            "Epoch 137/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7316 - RMSE: 0.7201 - val_loss: 0.7675 - val_RMSE: 0.7564\n",
            "Epoch 138/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7194 - RMSE: 0.7079 - val_loss: 0.7674 - val_RMSE: 0.7564\n",
            "Epoch 139/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7369 - RMSE: 0.7255 - val_loss: 0.7675 - val_RMSE: 0.7564\n",
            "Epoch 140/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7286 - RMSE: 0.7172 - val_loss: 0.7675 - val_RMSE: 0.7565\n",
            "Epoch 141/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7197 - RMSE: 0.7082 - val_loss: 0.7675 - val_RMSE: 0.7565\n",
            "Epoch 142/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7305 - RMSE: 0.7191 - val_loss: 0.7676 - val_RMSE: 0.7565\n",
            "Epoch 143/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7264 - RMSE: 0.7150 - val_loss: 0.7676 - val_RMSE: 0.7566\n",
            "Epoch 144/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7321 - RMSE: 0.7208 - val_loss: 0.7677 - val_RMSE: 0.7566\n",
            "Epoch 145/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7196 - RMSE: 0.7082 - val_loss: 0.7677 - val_RMSE: 0.7567\n",
            "Epoch 146/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7192 - RMSE: 0.7078 - val_loss: 0.7678 - val_RMSE: 0.7567\n",
            "Epoch 147/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7198 - RMSE: 0.7085 - val_loss: 0.7677 - val_RMSE: 0.7567\n",
            "Epoch 148/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7209 - RMSE: 0.7096 - val_loss: 0.7678 - val_RMSE: 0.7568\n",
            "Epoch 149/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7100 - RMSE: 0.6986 - val_loss: 0.7678 - val_RMSE: 0.7568\n",
            "Epoch 150/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7191 - RMSE: 0.7077 - val_loss: 0.7679 - val_RMSE: 0.7569\n",
            "Epoch 151/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7177 - RMSE: 0.7064 - val_loss: 0.7679 - val_RMSE: 0.7569\n",
            "Epoch 152/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7318 - RMSE: 0.7204 - val_loss: 0.7680 - val_RMSE: 0.7570\n",
            "Epoch 153/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7197 - RMSE: 0.7084 - val_loss: 0.7680 - val_RMSE: 0.7570\n",
            "Epoch 154/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7192 - RMSE: 0.7079 - val_loss: 0.7680 - val_RMSE: 0.7570\n",
            "Epoch 155/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7331 - RMSE: 0.7218 - val_loss: 0.7680 - val_RMSE: 0.7570\n",
            "Epoch 156/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7314 - RMSE: 0.7202 - val_loss: 0.7680 - val_RMSE: 0.7570\n",
            "Epoch 157/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7467 - RMSE: 0.7354 - val_loss: 0.7681 - val_RMSE: 0.7571\n",
            "Epoch 158/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7214 - RMSE: 0.7102 - val_loss: 0.7681 - val_RMSE: 0.7571\n",
            "Epoch 159/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7250 - RMSE: 0.7137 - val_loss: 0.7681 - val_RMSE: 0.7572\n",
            "Epoch 160/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7202 - RMSE: 0.7090 - val_loss: 0.7682 - val_RMSE: 0.7572\n",
            "Epoch 161/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7171 - RMSE: 0.7059 - val_loss: 0.7682 - val_RMSE: 0.7572\n",
            "Epoch 162/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7080 - RMSE: 0.6968 - val_loss: 0.7683 - val_RMSE: 0.7573\n",
            "Epoch 163/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7325 - RMSE: 0.7213 - val_loss: 0.7683 - val_RMSE: 0.7573\n",
            "Epoch 164/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7291 - RMSE: 0.7179 - val_loss: 0.7683 - val_RMSE: 0.7573\n",
            "Epoch 165/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7135 - RMSE: 0.7023 - val_loss: 0.7683 - val_RMSE: 0.7574\n",
            "Epoch 166/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7227 - RMSE: 0.7115 - val_loss: 0.7683 - val_RMSE: 0.7574\n",
            "Epoch 167/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7293 - RMSE: 0.7181 - val_loss: 0.7683 - val_RMSE: 0.7574\n",
            "Epoch 168/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7245 - RMSE: 0.7133 - val_loss: 0.7684 - val_RMSE: 0.7574\n",
            "Epoch 169/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7415 - RMSE: 0.7303 - val_loss: 0.7684 - val_RMSE: 0.7575\n",
            "Epoch 170/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7283 - RMSE: 0.7172 - val_loss: 0.7685 - val_RMSE: 0.7575\n",
            "Epoch 171/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7376 - RMSE: 0.7265 - val_loss: 0.7685 - val_RMSE: 0.7576\n",
            "Epoch 172/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7244 - RMSE: 0.7133 - val_loss: 0.7685 - val_RMSE: 0.7576\n",
            "Epoch 173/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7255 - RMSE: 0.7144 - val_loss: 0.7685 - val_RMSE: 0.7576\n",
            "Epoch 174/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7181 - RMSE: 0.7070 - val_loss: 0.7686 - val_RMSE: 0.7576\n",
            "Epoch 175/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7250 - RMSE: 0.7139 - val_loss: 0.7686 - val_RMSE: 0.7576\n",
            "Epoch 176/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7127 - RMSE: 0.7016 - val_loss: 0.7686 - val_RMSE: 0.7577\n",
            "Epoch 177/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7183 - RMSE: 0.7072 - val_loss: 0.7686 - val_RMSE: 0.7577\n",
            "Epoch 178/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7135 - RMSE: 0.7024 - val_loss: 0.7687 - val_RMSE: 0.7578\n",
            "Epoch 179/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7285 - RMSE: 0.7174 - val_loss: 0.7687 - val_RMSE: 0.7578\n",
            "Epoch 180/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7273 - RMSE: 0.7163 - val_loss: 0.7687 - val_RMSE: 0.7578\n",
            "Epoch 181/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7110 - RMSE: 0.7000 - val_loss: 0.7688 - val_RMSE: 0.7579\n",
            "Epoch 182/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7180 - RMSE: 0.7069 - val_loss: 0.7687 - val_RMSE: 0.7579\n",
            "Epoch 183/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7330 - RMSE: 0.7219 - val_loss: 0.7688 - val_RMSE: 0.7580\n",
            "Epoch 184/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7338 - RMSE: 0.7228 - val_loss: 0.7688 - val_RMSE: 0.7580\n",
            "Epoch 185/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7235 - RMSE: 0.7124 - val_loss: 0.7688 - val_RMSE: 0.7580\n",
            "Epoch 186/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7187 - RMSE: 0.7077 - val_loss: 0.7689 - val_RMSE: 0.7581\n",
            "Epoch 187/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7276 - RMSE: 0.7165 - val_loss: 0.7690 - val_RMSE: 0.7582\n",
            "Epoch 188/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7278 - RMSE: 0.7168 - val_loss: 0.7689 - val_RMSE: 0.7581\n",
            "Epoch 189/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7245 - RMSE: 0.7135 - val_loss: 0.7689 - val_RMSE: 0.7582\n",
            "Epoch 190/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7272 - RMSE: 0.7161 - val_loss: 0.7690 - val_RMSE: 0.7582\n",
            "Epoch 191/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7203 - RMSE: 0.7093 - val_loss: 0.7690 - val_RMSE: 0.7582\n",
            "Epoch 192/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7189 - RMSE: 0.7079 - val_loss: 0.7690 - val_RMSE: 0.7583\n",
            "Epoch 193/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7140 - RMSE: 0.7030 - val_loss: 0.7691 - val_RMSE: 0.7583\n",
            "Epoch 194/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7157 - RMSE: 0.7048 - val_loss: 0.7691 - val_RMSE: 0.7584\n",
            "Epoch 195/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7288 - RMSE: 0.7178 - val_loss: 0.7691 - val_RMSE: 0.7583\n",
            "Epoch 196/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7291 - RMSE: 0.7182 - val_loss: 0.7691 - val_RMSE: 0.7583\n",
            "Epoch 197/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7251 - RMSE: 0.7141 - val_loss: 0.7691 - val_RMSE: 0.7584\n",
            "Epoch 198/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7223 - RMSE: 0.7113 - val_loss: 0.7692 - val_RMSE: 0.7584\n",
            "Epoch 199/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7276 - RMSE: 0.7168 - val_loss: 0.7692 - val_RMSE: 0.7584\n",
            "Epoch 200/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7276 - RMSE: 0.7167 - val_loss: 0.7692 - val_RMSE: 0.7585\n",
            "Epoch 201/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7242 - RMSE: 0.7133 - val_loss: 0.7692 - val_RMSE: 0.7585\n",
            "Epoch 202/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7194 - RMSE: 0.7085 - val_loss: 0.7693 - val_RMSE: 0.7586\n",
            "Epoch 203/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7330 - RMSE: 0.7221 - val_loss: 0.7693 - val_RMSE: 0.7586\n",
            "Epoch 204/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7228 - RMSE: 0.7119 - val_loss: 0.7693 - val_RMSE: 0.7586\n",
            "Epoch 205/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7265 - RMSE: 0.7157 - val_loss: 0.7693 - val_RMSE: 0.7586\n",
            "Epoch 206/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7239 - RMSE: 0.7130 - val_loss: 0.7693 - val_RMSE: 0.7586\n",
            "Epoch 207/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7308 - RMSE: 0.7200 - val_loss: 0.7693 - val_RMSE: 0.7587\n",
            "Epoch 208/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7163 - RMSE: 0.7055 - val_loss: 0.7693 - val_RMSE: 0.7586\n",
            "Epoch 209/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7328 - RMSE: 0.7220 - val_loss: 0.7694 - val_RMSE: 0.7587\n",
            "Epoch 210/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7166 - RMSE: 0.7058 - val_loss: 0.7694 - val_RMSE: 0.7587\n",
            "Epoch 211/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7235 - RMSE: 0.7127 - val_loss: 0.7694 - val_RMSE: 0.7588\n",
            "Epoch 212/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7182 - RMSE: 0.7074 - val_loss: 0.7695 - val_RMSE: 0.7588\n",
            "Epoch 213/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7155 - RMSE: 0.7048 - val_loss: 0.7694 - val_RMSE: 0.7588\n",
            "Epoch 214/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7250 - RMSE: 0.7142 - val_loss: 0.7694 - val_RMSE: 0.7588\n",
            "Epoch 215/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7208 - RMSE: 0.7100 - val_loss: 0.7695 - val_RMSE: 0.7589\n",
            "Epoch 216/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7152 - RMSE: 0.7045 - val_loss: 0.7695 - val_RMSE: 0.7589\n",
            "Epoch 217/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7314 - RMSE: 0.7207 - val_loss: 0.7695 - val_RMSE: 0.7589\n",
            "Epoch 218/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7284 - RMSE: 0.7177 - val_loss: 0.7695 - val_RMSE: 0.7589\n",
            "Epoch 219/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7045 - RMSE: 0.6938 - val_loss: 0.7695 - val_RMSE: 0.7589\n",
            "Epoch 220/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7237 - RMSE: 0.7130 - val_loss: 0.7696 - val_RMSE: 0.7590\n",
            "Epoch 221/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7339 - RMSE: 0.7232 - val_loss: 0.7695 - val_RMSE: 0.7590\n",
            "Epoch 222/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7122 - RMSE: 0.7015 - val_loss: 0.7696 - val_RMSE: 0.7590\n",
            "Epoch 223/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7277 - RMSE: 0.7170 - val_loss: 0.7696 - val_RMSE: 0.7590\n",
            "Epoch 224/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7164 - RMSE: 0.7057 - val_loss: 0.7696 - val_RMSE: 0.7591\n",
            "Epoch 225/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7314 - RMSE: 0.7208 - val_loss: 0.7696 - val_RMSE: 0.7591\n",
            "Epoch 226/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7263 - RMSE: 0.7156 - val_loss: 0.7696 - val_RMSE: 0.7591\n",
            "Epoch 227/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7245 - RMSE: 0.7139 - val_loss: 0.7696 - val_RMSE: 0.7591\n",
            "Epoch 228/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7233 - RMSE: 0.7127 - val_loss: 0.7696 - val_RMSE: 0.7591\n",
            "Epoch 229/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7204 - RMSE: 0.7098 - val_loss: 0.7697 - val_RMSE: 0.7592\n",
            "Epoch 230/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7270 - RMSE: 0.7164 - val_loss: 0.7696 - val_RMSE: 0.7592\n",
            "Epoch 231/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7211 - RMSE: 0.7105 - val_loss: 0.7696 - val_RMSE: 0.7592\n",
            "Epoch 232/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7289 - RMSE: 0.7184 - val_loss: 0.7696 - val_RMSE: 0.7592\n",
            "Epoch 233/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7201 - RMSE: 0.7096 - val_loss: 0.7697 - val_RMSE: 0.7592\n",
            "Epoch 234/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7265 - RMSE: 0.7160 - val_loss: 0.7697 - val_RMSE: 0.7592\n",
            "Epoch 235/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7195 - RMSE: 0.7090 - val_loss: 0.7696 - val_RMSE: 0.7592\n",
            "Epoch 236/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7277 - RMSE: 0.7171 - val_loss: 0.7697 - val_RMSE: 0.7592\n",
            "Epoch 237/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7325 - RMSE: 0.7220 - val_loss: 0.7697 - val_RMSE: 0.7593\n",
            "Epoch 238/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7146 - RMSE: 0.7041 - val_loss: 0.7696 - val_RMSE: 0.7592\n",
            "Epoch 239/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7243 - RMSE: 0.7138 - val_loss: 0.7696 - val_RMSE: 0.7592\n",
            "Epoch 240/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7281 - RMSE: 0.7177 - val_loss: 0.7697 - val_RMSE: 0.7593\n",
            "Epoch 241/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7268 - RMSE: 0.7163 - val_loss: 0.7697 - val_RMSE: 0.7593\n",
            "Epoch 242/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7246 - RMSE: 0.7141 - val_loss: 0.7697 - val_RMSE: 0.7593\n",
            "Epoch 243/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7254 - RMSE: 0.7149 - val_loss: 0.7697 - val_RMSE: 0.7593\n",
            "Epoch 244/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7215 - RMSE: 0.7111 - val_loss: 0.7697 - val_RMSE: 0.7593\n",
            "Epoch 245/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7249 - RMSE: 0.7145 - val_loss: 0.7697 - val_RMSE: 0.7594\n",
            "Epoch 246/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7269 - RMSE: 0.7165 - val_loss: 0.7697 - val_RMSE: 0.7593\n",
            "Epoch 247/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7180 - RMSE: 0.7075 - val_loss: 0.7697 - val_RMSE: 0.7594\n",
            "Epoch 248/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7150 - RMSE: 0.7046 - val_loss: 0.7697 - val_RMSE: 0.7594\n",
            "Epoch 249/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7149 - RMSE: 0.7046 - val_loss: 0.7697 - val_RMSE: 0.7594\n",
            "Epoch 250/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7192 - RMSE: 0.7088 - val_loss: 0.7697 - val_RMSE: 0.7594\n",
            "Epoch 251/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7268 - RMSE: 0.7164 - val_loss: 0.7698 - val_RMSE: 0.7595\n",
            "Epoch 252/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7195 - RMSE: 0.7092 - val_loss: 0.7697 - val_RMSE: 0.7595\n",
            "Epoch 253/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7212 - RMSE: 0.7109 - val_loss: 0.7697 - val_RMSE: 0.7595\n",
            "Epoch 254/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7180 - RMSE: 0.7076 - val_loss: 0.7697 - val_RMSE: 0.7595\n",
            "Epoch 255/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7316 - RMSE: 0.7212 - val_loss: 0.7697 - val_RMSE: 0.7595\n",
            "Epoch 256/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7354 - RMSE: 0.7251 - val_loss: 0.7697 - val_RMSE: 0.7595\n",
            "Epoch 257/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7254 - RMSE: 0.7151 - val_loss: 0.7697 - val_RMSE: 0.7595\n",
            "Epoch 258/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7264 - RMSE: 0.7161 - val_loss: 0.7698 - val_RMSE: 0.7596\n",
            "Epoch 259/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7355 - RMSE: 0.7252 - val_loss: 0.7698 - val_RMSE: 0.7596\n",
            "Epoch 260/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7306 - RMSE: 0.7204 - val_loss: 0.7697 - val_RMSE: 0.7595\n",
            "Epoch 261/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7105 - RMSE: 0.7002 - val_loss: 0.7697 - val_RMSE: 0.7596\n",
            "Epoch 262/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7146 - RMSE: 0.7044 - val_loss: 0.7698 - val_RMSE: 0.7596\n",
            "Epoch 263/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7231 - RMSE: 0.7128 - val_loss: 0.7698 - val_RMSE: 0.7596\n",
            "Epoch 264/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7262 - RMSE: 0.7160 - val_loss: 0.7698 - val_RMSE: 0.7597\n",
            "Epoch 265/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7325 - RMSE: 0.7223 - val_loss: 0.7697 - val_RMSE: 0.7596\n",
            "Epoch 266/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7176 - RMSE: 0.7074 - val_loss: 0.7698 - val_RMSE: 0.7597\n",
            "Epoch 267/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7233 - RMSE: 0.7132 - val_loss: 0.7698 - val_RMSE: 0.7597\n",
            "Epoch 268/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7200 - RMSE: 0.7098 - val_loss: 0.7698 - val_RMSE: 0.7597\n",
            "Epoch 269/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7212 - RMSE: 0.7110 - val_loss: 0.7698 - val_RMSE: 0.7597\n",
            "Epoch 270/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7139 - RMSE: 0.7037 - val_loss: 0.7698 - val_RMSE: 0.7597\n",
            "Epoch 271/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7255 - RMSE: 0.7154 - val_loss: 0.7698 - val_RMSE: 0.7597\n",
            "Epoch 272/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7236 - RMSE: 0.7135 - val_loss: 0.7699 - val_RMSE: 0.7598\n",
            "Epoch 273/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7241 - RMSE: 0.7139 - val_loss: 0.7698 - val_RMSE: 0.7597\n",
            "Epoch 274/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7379 - RMSE: 0.7278 - val_loss: 0.7698 - val_RMSE: 0.7598\n",
            "Epoch 275/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7198 - RMSE: 0.7096 - val_loss: 0.7698 - val_RMSE: 0.7597\n",
            "Epoch 276/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7315 - RMSE: 0.7214 - val_loss: 0.7699 - val_RMSE: 0.7599\n",
            "Epoch 277/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7110 - RMSE: 0.7009 - val_loss: 0.7699 - val_RMSE: 0.7599\n",
            "Epoch 278/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7189 - RMSE: 0.7089 - val_loss: 0.7698 - val_RMSE: 0.7598\n",
            "Epoch 279/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7314 - RMSE: 0.7213 - val_loss: 0.7698 - val_RMSE: 0.7598\n",
            "Epoch 280/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7228 - RMSE: 0.7128 - val_loss: 0.7699 - val_RMSE: 0.7599\n",
            "Epoch 281/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7155 - RMSE: 0.7054 - val_loss: 0.7699 - val_RMSE: 0.7599\n",
            "Epoch 282/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7219 - RMSE: 0.7118 - val_loss: 0.7698 - val_RMSE: 0.7598\n",
            "Epoch 283/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7239 - RMSE: 0.7139 - val_loss: 0.7699 - val_RMSE: 0.7599\n",
            "Epoch 284/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7289 - RMSE: 0.7189 - val_loss: 0.7699 - val_RMSE: 0.7599\n",
            "Epoch 285/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7324 - RMSE: 0.7224 - val_loss: 0.7699 - val_RMSE: 0.7600\n",
            "Epoch 286/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7087 - RMSE: 0.6987 - val_loss: 0.7699 - val_RMSE: 0.7600\n",
            "Epoch 287/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7234 - RMSE: 0.7135 - val_loss: 0.7699 - val_RMSE: 0.7599\n",
            "Epoch 288/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7177 - RMSE: 0.7078 - val_loss: 0.7699 - val_RMSE: 0.7600\n",
            "Epoch 289/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7294 - RMSE: 0.7194 - val_loss: 0.7699 - val_RMSE: 0.7600\n",
            "Epoch 290/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7119 - RMSE: 0.7019 - val_loss: 0.7699 - val_RMSE: 0.7599\n",
            "Epoch 291/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7107 - RMSE: 0.7007 - val_loss: 0.7699 - val_RMSE: 0.7600\n",
            "Epoch 292/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7177 - RMSE: 0.7077 - val_loss: 0.7699 - val_RMSE: 0.7600\n",
            "Epoch 293/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7208 - RMSE: 0.7109 - val_loss: 0.7698 - val_RMSE: 0.7600\n",
            "Epoch 294/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7288 - RMSE: 0.7189 - val_loss: 0.7699 - val_RMSE: 0.7600\n",
            "Epoch 295/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7186 - RMSE: 0.7087 - val_loss: 0.7698 - val_RMSE: 0.7600\n",
            "Epoch 296/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7336 - RMSE: 0.7237 - val_loss: 0.7699 - val_RMSE: 0.7600\n",
            "Epoch 297/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7266 - RMSE: 0.7167 - val_loss: 0.7699 - val_RMSE: 0.7600\n",
            "Epoch 298/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7153 - RMSE: 0.7055 - val_loss: 0.7699 - val_RMSE: 0.7601\n",
            "Epoch 299/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7201 - RMSE: 0.7102 - val_loss: 0.7698 - val_RMSE: 0.7600\n",
            "Epoch 300/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7162 - RMSE: 0.7064 - val_loss: 0.7698 - val_RMSE: 0.7600\n",
            "Epoch 301/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7286 - RMSE: 0.7187 - val_loss: 0.7699 - val_RMSE: 0.7601\n",
            "Epoch 302/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7118 - RMSE: 0.7020 - val_loss: 0.7699 - val_RMSE: 0.7601\n",
            "Epoch 303/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7249 - RMSE: 0.7151 - val_loss: 0.7698 - val_RMSE: 0.7601\n",
            "Epoch 304/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7269 - RMSE: 0.7171 - val_loss: 0.7699 - val_RMSE: 0.7601\n",
            "Epoch 305/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7107 - RMSE: 0.7009 - val_loss: 0.7699 - val_RMSE: 0.7601\n",
            "Epoch 306/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7173 - RMSE: 0.7075 - val_loss: 0.7699 - val_RMSE: 0.7601\n",
            "Epoch 307/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7184 - RMSE: 0.7086 - val_loss: 0.7699 - val_RMSE: 0.7601\n",
            "Epoch 308/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7331 - RMSE: 0.7234 - val_loss: 0.7699 - val_RMSE: 0.7602\n",
            "Epoch 309/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7154 - RMSE: 0.7056 - val_loss: 0.7699 - val_RMSE: 0.7602\n",
            "Epoch 310/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7231 - RMSE: 0.7134 - val_loss: 0.7699 - val_RMSE: 0.7602\n",
            "Epoch 311/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7224 - RMSE: 0.7126 - val_loss: 0.7699 - val_RMSE: 0.7602\n",
            "Epoch 312/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7281 - RMSE: 0.7183 - val_loss: 0.7698 - val_RMSE: 0.7601\n",
            "Epoch 313/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7181 - RMSE: 0.7084 - val_loss: 0.7698 - val_RMSE: 0.7602\n",
            "Epoch 314/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7119 - RMSE: 0.7022 - val_loss: 0.7698 - val_RMSE: 0.7602\n",
            "Epoch 315/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7137 - RMSE: 0.7040 - val_loss: 0.7698 - val_RMSE: 0.7602\n",
            "Epoch 316/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7229 - RMSE: 0.7132 - val_loss: 0.7699 - val_RMSE: 0.7602\n",
            "Epoch 317/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7185 - RMSE: 0.7088 - val_loss: 0.7699 - val_RMSE: 0.7602\n",
            "Epoch 318/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7242 - RMSE: 0.7145 - val_loss: 0.7698 - val_RMSE: 0.7602\n",
            "Epoch 319/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7225 - RMSE: 0.7128 - val_loss: 0.7699 - val_RMSE: 0.7603\n",
            "Epoch 320/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7331 - RMSE: 0.7234 - val_loss: 0.7698 - val_RMSE: 0.7602\n",
            "Epoch 321/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7348 - RMSE: 0.7251 - val_loss: 0.7699 - val_RMSE: 0.7603\n",
            "Epoch 322/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7213 - RMSE: 0.7116 - val_loss: 0.7698 - val_RMSE: 0.7602\n",
            "Epoch 323/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7293 - RMSE: 0.7197 - val_loss: 0.7699 - val_RMSE: 0.7603\n",
            "Epoch 324/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7173 - RMSE: 0.7077 - val_loss: 0.7698 - val_RMSE: 0.7602\n",
            "Epoch 325/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7138 - RMSE: 0.7042 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 326/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7265 - RMSE: 0.7170 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 327/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7260 - RMSE: 0.7164 - val_loss: 0.7698 - val_RMSE: 0.7602\n",
            "Epoch 328/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7253 - RMSE: 0.7157 - val_loss: 0.7698 - val_RMSE: 0.7602\n",
            "Epoch 329/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7146 - RMSE: 0.7050 - val_loss: 0.7698 - val_RMSE: 0.7602\n",
            "Epoch 330/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7171 - RMSE: 0.7075 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 331/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7210 - RMSE: 0.7115 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 332/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7182 - RMSE: 0.7087 - val_loss: 0.7697 - val_RMSE: 0.7603\n",
            "Epoch 333/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7215 - RMSE: 0.7120 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 334/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7294 - RMSE: 0.7199 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 335/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7092 - RMSE: 0.6997 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 336/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7228 - RMSE: 0.7133 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 337/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7180 - RMSE: 0.7085 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 338/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7267 - RMSE: 0.7173 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 339/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7245 - RMSE: 0.7150 - val_loss: 0.7697 - val_RMSE: 0.7603\n",
            "Epoch 340/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7113 - RMSE: 0.7019 - val_loss: 0.7697 - val_RMSE: 0.7603\n",
            "Epoch 341/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7239 - RMSE: 0.7144 - val_loss: 0.7697 - val_RMSE: 0.7602\n",
            "Epoch 342/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7165 - RMSE: 0.7071 - val_loss: 0.7697 - val_RMSE: 0.7603\n",
            "Epoch 343/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7140 - RMSE: 0.7046 - val_loss: 0.7697 - val_RMSE: 0.7603\n",
            "Epoch 344/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7202 - RMSE: 0.7108 - val_loss: 0.7697 - val_RMSE: 0.7603\n",
            "Epoch 345/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7257 - RMSE: 0.7162 - val_loss: 0.7696 - val_RMSE: 0.7602\n",
            "Epoch 346/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7180 - RMSE: 0.7086 - val_loss: 0.7697 - val_RMSE: 0.7603\n",
            "Epoch 347/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7244 - RMSE: 0.7150 - val_loss: 0.7696 - val_RMSE: 0.7603\n",
            "Epoch 348/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7234 - RMSE: 0.7140 - val_loss: 0.7697 - val_RMSE: 0.7603\n",
            "Epoch 349/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7161 - RMSE: 0.7067 - val_loss: 0.7696 - val_RMSE: 0.7602\n",
            "Epoch 350/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7221 - RMSE: 0.7128 - val_loss: 0.7697 - val_RMSE: 0.7603\n",
            "Epoch 351/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7002 - RMSE: 0.6908 - val_loss: 0.7697 - val_RMSE: 0.7603\n",
            "Epoch 352/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7113 - RMSE: 0.7019 - val_loss: 0.7696 - val_RMSE: 0.7603\n",
            "Epoch 353/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7283 - RMSE: 0.7190 - val_loss: 0.7696 - val_RMSE: 0.7603\n",
            "Epoch 354/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7138 - RMSE: 0.7045 - val_loss: 0.7696 - val_RMSE: 0.7603\n",
            "Epoch 355/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7175 - RMSE: 0.7082 - val_loss: 0.7696 - val_RMSE: 0.7604\n",
            "Epoch 356/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7224 - RMSE: 0.7131 - val_loss: 0.7696 - val_RMSE: 0.7603\n",
            "Epoch 357/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7270 - RMSE: 0.7177 - val_loss: 0.7696 - val_RMSE: 0.7604\n",
            "Epoch 358/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7242 - RMSE: 0.7150 - val_loss: 0.7696 - val_RMSE: 0.7603\n",
            "Epoch 359/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7296 - RMSE: 0.7204 - val_loss: 0.7696 - val_RMSE: 0.7604\n",
            "Epoch 360/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7165 - RMSE: 0.7072 - val_loss: 0.7696 - val_RMSE: 0.7603\n",
            "Epoch 361/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7161 - RMSE: 0.7069 - val_loss: 0.7696 - val_RMSE: 0.7604\n",
            "Epoch 362/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7225 - RMSE: 0.7133 - val_loss: 0.7695 - val_RMSE: 0.7603\n",
            "Epoch 363/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7205 - RMSE: 0.7113 - val_loss: 0.7696 - val_RMSE: 0.7604\n",
            "Epoch 364/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7304 - RMSE: 0.7212 - val_loss: 0.7695 - val_RMSE: 0.7603\n",
            "Epoch 365/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7259 - RMSE: 0.7167 - val_loss: 0.7695 - val_RMSE: 0.7603\n",
            "Epoch 366/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7278 - RMSE: 0.7186 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 367/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7287 - RMSE: 0.7195 - val_loss: 0.7696 - val_RMSE: 0.7604\n",
            "Epoch 368/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7108 - RMSE: 0.7016 - val_loss: 0.7696 - val_RMSE: 0.7604\n",
            "Epoch 369/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7101 - RMSE: 0.7009 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 370/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7232 - RMSE: 0.7141 - val_loss: 0.7695 - val_RMSE: 0.7603\n",
            "Epoch 371/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7282 - RMSE: 0.7190 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 372/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7146 - RMSE: 0.7054 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 373/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7245 - RMSE: 0.7154 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 374/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7193 - RMSE: 0.7102 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 375/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7350 - RMSE: 0.7259 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 376/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7214 - RMSE: 0.7123 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 377/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7260 - RMSE: 0.7168 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 378/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7278 - RMSE: 0.7187 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 379/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7221 - RMSE: 0.7130 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 380/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7216 - RMSE: 0.7125 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 381/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7160 - RMSE: 0.7069 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 382/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7217 - RMSE: 0.7126 - val_loss: 0.7695 - val_RMSE: 0.7604\n",
            "Epoch 383/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7350 - RMSE: 0.7260 - val_loss: 0.7694 - val_RMSE: 0.7604\n",
            "Epoch 384/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7267 - RMSE: 0.7177 - val_loss: 0.7694 - val_RMSE: 0.7604\n",
            "Epoch 385/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7114 - RMSE: 0.7023 - val_loss: 0.7695 - val_RMSE: 0.7605\n",
            "Epoch 386/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7247 - RMSE: 0.7156 - val_loss: 0.7694 - val_RMSE: 0.7604\n",
            "Epoch 387/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7185 - RMSE: 0.7095 - val_loss: 0.7694 - val_RMSE: 0.7604\n",
            "Epoch 388/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7164 - RMSE: 0.7074 - val_loss: 0.7693 - val_RMSE: 0.7604\n",
            "Epoch 389/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7215 - RMSE: 0.7125 - val_loss: 0.7694 - val_RMSE: 0.7604\n",
            "Epoch 390/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7165 - RMSE: 0.7075 - val_loss: 0.7694 - val_RMSE: 0.7604\n",
            "Epoch 391/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7226 - RMSE: 0.7136 - val_loss: 0.7694 - val_RMSE: 0.7604\n",
            "Epoch 392/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7283 - RMSE: 0.7193 - val_loss: 0.7694 - val_RMSE: 0.7604\n",
            "Epoch 393/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7247 - RMSE: 0.7157 - val_loss: 0.7694 - val_RMSE: 0.7604\n",
            "Epoch 394/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7196 - RMSE: 0.7107 - val_loss: 0.7693 - val_RMSE: 0.7604\n",
            "Epoch 395/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7198 - RMSE: 0.7109 - val_loss: 0.7694 - val_RMSE: 0.7605\n",
            "Epoch 396/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7149 - RMSE: 0.7060 - val_loss: 0.7694 - val_RMSE: 0.7605\n",
            "Epoch 397/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7188 - RMSE: 0.7098 - val_loss: 0.7694 - val_RMSE: 0.7604\n",
            "Epoch 398/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7211 - RMSE: 0.7122 - val_loss: 0.7693 - val_RMSE: 0.7604\n",
            "Epoch 399/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7109 - RMSE: 0.7020 - val_loss: 0.7693 - val_RMSE: 0.7604\n",
            "Epoch 400/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7212 - RMSE: 0.7123 - val_loss: 0.7693 - val_RMSE: 0.7604\n",
            "Epoch 401/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7257 - RMSE: 0.7168 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 402/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7233 - RMSE: 0.7144 - val_loss: 0.7694 - val_RMSE: 0.7605\n",
            "Epoch 403/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7164 - RMSE: 0.7075 - val_loss: 0.7693 - val_RMSE: 0.7604\n",
            "Epoch 404/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7149 - RMSE: 0.7060 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 405/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7254 - RMSE: 0.7165 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 406/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7193 - RMSE: 0.7105 - val_loss: 0.7694 - val_RMSE: 0.7605\n",
            "Epoch 407/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7155 - RMSE: 0.7066 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 408/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7083 - RMSE: 0.6995 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 409/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7169 - RMSE: 0.7080 - val_loss: 0.7692 - val_RMSE: 0.7604\n",
            "Epoch 410/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7235 - RMSE: 0.7147 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 411/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7194 - RMSE: 0.7105 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 412/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7206 - RMSE: 0.7118 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 413/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7239 - RMSE: 0.7151 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 414/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7200 - RMSE: 0.7112 - val_loss: 0.7692 - val_RMSE: 0.7604\n",
            "Epoch 415/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7173 - RMSE: 0.7085 - val_loss: 0.7692 - val_RMSE: 0.7605\n",
            "Epoch 416/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7123 - RMSE: 0.7036 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 417/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7275 - RMSE: 0.7187 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 418/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7139 - RMSE: 0.7051 - val_loss: 0.7693 - val_RMSE: 0.7605\n",
            "Epoch 419/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7208 - RMSE: 0.7121 - val_loss: 0.7692 - val_RMSE: 0.7605\n",
            "Epoch 420/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7139 - RMSE: 0.7052 - val_loss: 0.7692 - val_RMSE: 0.7605\n",
            "Epoch 421/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7173 - RMSE: 0.7085 - val_loss: 0.7692 - val_RMSE: 0.7605\n",
            "Epoch 422/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7181 - RMSE: 0.7094 - val_loss: 0.7692 - val_RMSE: 0.7605\n",
            "Epoch 423/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7101 - RMSE: 0.7014 - val_loss: 0.7692 - val_RMSE: 0.7605\n",
            "Epoch 424/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7108 - RMSE: 0.7021 - val_loss: 0.7692 - val_RMSE: 0.7605\n",
            "Epoch 425/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7297 - RMSE: 0.7210 - val_loss: 0.7691 - val_RMSE: 0.7604\n",
            "Epoch 426/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7185 - RMSE: 0.7098 - val_loss: 0.7691 - val_RMSE: 0.7605\n",
            "Epoch 427/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7244 - RMSE: 0.7157 - val_loss: 0.7691 - val_RMSE: 0.7605\n",
            "Epoch 428/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7222 - RMSE: 0.7135 - val_loss: 0.7691 - val_RMSE: 0.7604\n",
            "Epoch 429/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7168 - RMSE: 0.7081 - val_loss: 0.7691 - val_RMSE: 0.7604\n",
            "Epoch 430/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7196 - RMSE: 0.7110 - val_loss: 0.7692 - val_RMSE: 0.7605\n",
            "Epoch 431/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7211 - RMSE: 0.7125 - val_loss: 0.7691 - val_RMSE: 0.7604\n",
            "Epoch 432/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7136 - RMSE: 0.7049 - val_loss: 0.7691 - val_RMSE: 0.7605\n",
            "Epoch 433/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7127 - RMSE: 0.7041 - val_loss: 0.7691 - val_RMSE: 0.7605\n",
            "Epoch 434/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7127 - RMSE: 0.7040 - val_loss: 0.7691 - val_RMSE: 0.7605\n",
            "Epoch 435/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7075 - RMSE: 0.6988 - val_loss: 0.7690 - val_RMSE: 0.7604\n",
            "Epoch 436/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7155 - RMSE: 0.7069 - val_loss: 0.7691 - val_RMSE: 0.7605\n",
            "Epoch 437/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7173 - RMSE: 0.7086 - val_loss: 0.7691 - val_RMSE: 0.7605\n",
            "Epoch 438/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7125 - RMSE: 0.7039 - val_loss: 0.7691 - val_RMSE: 0.7605\n",
            "Epoch 439/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7056 - RMSE: 0.6970 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 440/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7270 - RMSE: 0.7184 - val_loss: 0.7690 - val_RMSE: 0.7604\n",
            "Epoch 441/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7205 - RMSE: 0.7119 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 442/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7257 - RMSE: 0.7171 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 443/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7164 - RMSE: 0.7079 - val_loss: 0.7689 - val_RMSE: 0.7604\n",
            "Epoch 444/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7226 - RMSE: 0.7140 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 445/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7223 - RMSE: 0.7137 - val_loss: 0.7690 - val_RMSE: 0.7604\n",
            "Epoch 446/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7193 - RMSE: 0.7108 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 447/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7216 - RMSE: 0.7131 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 448/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7154 - RMSE: 0.7069 - val_loss: 0.7689 - val_RMSE: 0.7604\n",
            "Epoch 449/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7051 - RMSE: 0.6966 - val_loss: 0.7689 - val_RMSE: 0.7605\n",
            "Epoch 450/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7219 - RMSE: 0.7134 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 451/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7154 - RMSE: 0.7069 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 452/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7198 - RMSE: 0.7112 - val_loss: 0.7689 - val_RMSE: 0.7604\n",
            "Epoch 453/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7173 - RMSE: 0.7088 - val_loss: 0.7689 - val_RMSE: 0.7605\n",
            "Epoch 454/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7160 - RMSE: 0.7076 - val_loss: 0.7689 - val_RMSE: 0.7604\n",
            "Epoch 455/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7179 - RMSE: 0.7094 - val_loss: 0.7689 - val_RMSE: 0.7604\n",
            "Epoch 456/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7235 - RMSE: 0.7150 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 457/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7105 - RMSE: 0.7020 - val_loss: 0.7689 - val_RMSE: 0.7604\n",
            "Epoch 458/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7234 - RMSE: 0.7149 - val_loss: 0.7688 - val_RMSE: 0.7604\n",
            "Epoch 459/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7193 - RMSE: 0.7108 - val_loss: 0.7688 - val_RMSE: 0.7604\n",
            "Epoch 460/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7162 - RMSE: 0.7077 - val_loss: 0.7689 - val_RMSE: 0.7605\n",
            "Epoch 461/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7259 - RMSE: 0.7175 - val_loss: 0.7688 - val_RMSE: 0.7604\n",
            "Epoch 462/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7234 - RMSE: 0.7150 - val_loss: 0.7688 - val_RMSE: 0.7603\n",
            "Epoch 463/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7173 - RMSE: 0.7088 - val_loss: 0.7688 - val_RMSE: 0.7604\n",
            "Epoch 464/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7299 - RMSE: 0.7215 - val_loss: 0.7688 - val_RMSE: 0.7603\n",
            "Epoch 465/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7142 - RMSE: 0.7058 - val_loss: 0.7688 - val_RMSE: 0.7603\n",
            "Epoch 466/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7130 - RMSE: 0.7046 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 467/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7191 - RMSE: 0.7107 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 468/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7004 - RMSE: 0.6920 - val_loss: 0.7688 - val_RMSE: 0.7604\n",
            "Epoch 469/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7259 - RMSE: 0.7175 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 470/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7216 - RMSE: 0.7132 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 471/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7180 - RMSE: 0.7097 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 472/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7235 - RMSE: 0.7151 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 473/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7210 - RMSE: 0.7127 - val_loss: 0.7686 - val_RMSE: 0.7602\n",
            "Epoch 474/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7130 - RMSE: 0.7046 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 475/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7089 - RMSE: 0.7005 - val_loss: 0.7687 - val_RMSE: 0.7602\n",
            "Epoch 476/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7220 - RMSE: 0.7137 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 477/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7138 - RMSE: 0.7055 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 478/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7007 - RMSE: 0.6923 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 479/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6986 - RMSE: 0.6903 - val_loss: 0.7685 - val_RMSE: 0.7602\n",
            "Epoch 480/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7156 - RMSE: 0.7073 - val_loss: 0.7686 - val_RMSE: 0.7603\n",
            "Epoch 481/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7277 - RMSE: 0.7194 - val_loss: 0.7687 - val_RMSE: 0.7603\n",
            "Epoch 482/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7167 - RMSE: 0.7084 - val_loss: 0.7686 - val_RMSE: 0.7603\n",
            "Epoch 483/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7128 - RMSE: 0.7046 - val_loss: 0.7685 - val_RMSE: 0.7602\n",
            "Epoch 484/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7269 - RMSE: 0.7186 - val_loss: 0.7686 - val_RMSE: 0.7603\n",
            "Epoch 485/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7193 - RMSE: 0.7110 - val_loss: 0.7686 - val_RMSE: 0.7603\n",
            "Epoch 486/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7156 - RMSE: 0.7073 - val_loss: 0.7686 - val_RMSE: 0.7603\n",
            "Epoch 487/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7099 - RMSE: 0.7016 - val_loss: 0.7687 - val_RMSE: 0.7604\n",
            "Epoch 488/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7139 - RMSE: 0.7056 - val_loss: 0.7686 - val_RMSE: 0.7603\n",
            "Epoch 489/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7139 - RMSE: 0.7056 - val_loss: 0.7686 - val_RMSE: 0.7603\n",
            "Epoch 490/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7161 - RMSE: 0.7078 - val_loss: 0.7687 - val_RMSE: 0.7604\n",
            "Epoch 491/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7203 - RMSE: 0.7120 - val_loss: 0.7685 - val_RMSE: 0.7602\n",
            "Epoch 492/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7219 - RMSE: 0.7136 - val_loss: 0.7685 - val_RMSE: 0.7602\n",
            "Epoch 493/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7165 - RMSE: 0.7083 - val_loss: 0.7685 - val_RMSE: 0.7602\n",
            "Epoch 494/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7171 - RMSE: 0.7089 - val_loss: 0.7685 - val_RMSE: 0.7602\n",
            "Epoch 495/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7089 - RMSE: 0.7007 - val_loss: 0.7685 - val_RMSE: 0.7602\n",
            "Epoch 496/500\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7215 - RMSE: 0.7133 - val_loss: 0.7686 - val_RMSE: 0.7603\n",
            "Epoch 497/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7135 - RMSE: 0.7053 - val_loss: 0.7686 - val_RMSE: 0.7603\n",
            "Epoch 498/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7242 - RMSE: 0.7160 - val_loss: 0.7685 - val_RMSE: 0.7603\n",
            "Epoch 499/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7107 - RMSE: 0.7025 - val_loss: 0.7685 - val_RMSE: 0.7603\n",
            "Epoch 500/500\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7156 - RMSE: 0.7074 - val_loss: 0.7685 - val_RMSE: 0.7603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "1iFkPxL4_Mpa",
        "outputId": "c56a98b0-3f3c-4048-f6f5-447cf560acbc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(result.history['RMSE'], label=\"Train RMSE\")\n",
        "plt.plot(result.history['val_RMSE'], label=\"Test RMSE\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi2belMfAp9Z",
        "outputId": "dbcd608c-20b4-49d3-9711-ec40d262b981"
      },
      "source": [
        "model=best_model\n",
        "user_ids = ratings_test.users.values\n",
        "track_ids = ratings_test.tracks.values\n",
        "y_pred = model.predict([user_ids, track_ids]) + mu\n",
        "y_pred = np.ravel(y_pred, order='C')\n",
        "y_true = np.array(ratings_test.score)\n",
        "\n",
        "RMSE2(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.726"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMVdc4UUXm_5"
      },
      "source": [
        "#fname='/gdrive/Shareddrives/Spotify Recommendation/Recommendation Models/NeuralNet_RandomSearch_210402.h5'\n",
        "best_model.save('/gdrive/Shareddrives/Spotify Recommendation/Recommendation Models/NeuralNet_RandomSearch_210402.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBjL83u5lGfC"
      },
      "source": [
        "### (3) Nationality variable Added"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RPnIqxXglhbL",
        "outputId": "af256c4a-baca-401e-f948-cb3254ec4a73"
      },
      "source": [
        "USERS = data_dict['User'][['users','nationality']]\n",
        "\n",
        "user2idx={name:i for i, name in enumerate(USERS.users)}\n",
        "for i in range(29):\n",
        "  USERS.users[i]=user2idx[USERS.users[i]]\n",
        "USERS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>users</th>\n",
              "      <th>nationality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>singapore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>uk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>uk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>uk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>singapore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>singapore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>honkong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>hongkong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>singapore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>uk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>singapore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>singapore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>singapore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>uk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>hongkong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>hongkong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>uk</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   users nationality\n",
              "0      0       korea\n",
              "1      1   singapore\n",
              "2      2          uk\n",
              "3      3          uk\n",
              "4      4          uk\n",
              "5      5   singapore\n",
              "6      6   singapore\n",
              "7      7     honkong\n",
              "8      8       korea\n",
              "9      9    hongkong\n",
              "10    10       korea\n",
              "11    11   singapore\n",
              "12    12       korea\n",
              "13    13          uk\n",
              "14    14       korea\n",
              "15    15       korea\n",
              "16    16   singapore\n",
              "17    17   singapore\n",
              "18    18       korea\n",
              "19    19       korea\n",
              "20    20   singapore\n",
              "21    21       korea\n",
              "22    22       korea\n",
              "23    23          uk\n",
              "24    24    hongkong\n",
              "25    25       korea\n",
              "26    26    hongkong\n",
              "27    27       korea\n",
              "28    28          uk"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Av6EhZnCH8",
        "outputId": "017c64e1-0552-42bb-ea9b-70f7dd916d83"
      },
      "source": [
        "nationality={}\n",
        "def convert_nat(x):\n",
        "    if x in nationality:\n",
        "        return nationality[x]\n",
        "    else:\n",
        "        nationality[x] = len(nationality)\n",
        "        return nationality[x]\n",
        "USERS['nationality'] = USERS['nationality'].apply(convert_nat)\n",
        "USERS;L=len(nationality)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDHQbSbomS3e"
      },
      "source": [
        "train_nat = pd.merge(ratings_train,USERS,on='users')['nationality']\n",
        "valid_nat = pd.merge(ratings_valid,USERS,on='users')['nationality']\n",
        "test_nat = pd.merge(ratings_test,USERS,on='users')['nationality']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzgEcgu6o4tU"
      },
      "source": [
        "#### Tuner 없는 버전"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TflSP1Qto4tX",
        "outputId": "eda154c0-65bb-4ce1-b614-473cc88b1e65"
      },
      "source": [
        "# Variable 초기화 \n",
        "K = 15\n",
        "epochs=300                            # Latent factor 수 \n",
        "mu = round(ratings_train.score.mean(),4)  # 전체 평균 \n",
        "M = len(ratings.index)    # Number of users\n",
        "N = len(ratings.columns) # Number of movies\n",
        "   \n",
        "\n",
        "# Keras model\n",
        "user = Input(shape=(1, ))                                               # User input\n",
        "item = Input(shape=(1, ))                                               # Item input\n",
        "P_embedding = Embedding(M, K, embeddings_regularizer=l2())(user)        # (M, 1, K)\n",
        "Q_embedding = Embedding(N, K, embeddings_regularizer=l2())(item)        # (N, 1, K)\n",
        "user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)          # User bias term (M, 1, )\n",
        "item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)          # Item bias term (N, 1, )\n",
        "\n",
        "nat = Input(shape=(1, ))\n",
        "nat_embedding = Embedding(L, 3, embeddings_regularizer=l2())(nat)\n",
        "nat_layer = Flatten()(nat_embedding)\n",
        "\n",
        "\n",
        "# Concatenate layers\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Activation\n",
        "P_embedding = Flatten()(P_embedding)                                    # (K, )\n",
        "Q_embedding = Flatten()(Q_embedding)                                    # (K, )\n",
        "user_bias = Flatten()(user_bias)                                        # (1, )\n",
        "item_bias = Flatten()(item_bias)                                        # (1, )\n",
        "R = Concatenate()([P_embedding, Q_embedding, user_bias, item_bias, nat_layer])\n",
        "\n",
        "# Neural network\n",
        "R = Dense(128,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "R = Dropout(0.5)(R)\n",
        "#R = Dense(64,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "#R = Dropout(0.1)(R)\n",
        "#R = Dense(32,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "R = Dense(1)(R)\n",
        "\n",
        "# Model setting\n",
        "starter_learning_rate = 0.1\n",
        "end_learning_rate = 0.0001\n",
        "decay_steps = int(0.1*epochs)\n",
        "learning_rate_fn = PolynomialDecay(\n",
        "    starter_learning_rate,\n",
        "    decay_steps,\n",
        "    end_learning_rate,\n",
        "    power=0.5)\n",
        "model = Model(inputs=[user, item,nat], outputs=R)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "  loss=RMSE,\n",
        "  #optimizer=SGD(learning_rate=learning_rate_fn,momentum=0.9),\n",
        "  optimizer=Adam(learning_rate=learning_rate_fn),\n",
        "  metrics=[RMSE],\n",
        "  \n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Model fitting\n",
        "result = model.fit(\n",
        "  x=[ratings_train.users.values, ratings_train.tracks.values,train_nat.values],\n",
        "  y=ratings_train.score.values - mu,\n",
        "  epochs=epochs,\n",
        "  batch_size=64,\n",
        "  validation_data=(\n",
        "    [ratings_test.users.values, ratings_test.tracks.values,test_nat.values],\n",
        "    ratings_test.score.values - mu\n",
        "  )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 15)        435         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1, 15)        2430        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 1)         29          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1, 1)         162         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 1, 3)         15          input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 15)           0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 15)           0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 1)            0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 1)            0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 3)            0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 35)           0           flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "                                                                 flatten_3[0][0]                  \n",
            "                                                                 flatten_4[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          4608        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 128)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            129         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 7,808\n",
            "Trainable params: 7,808\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 2s 10ms/step - loss: 1.1386 - RMSE: 0.8894 - val_loss: 0.9177 - val_RMSE: 0.7851\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9156 - RMSE: 0.7861 - val_loss: 0.9094 - val_RMSE: 0.7840\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9228 - RMSE: 0.8004 - val_loss: 0.9012 - val_RMSE: 0.7828\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.9036 - RMSE: 0.7882 - val_loss: 0.8935 - val_RMSE: 0.7818\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8996 - RMSE: 0.7909 - val_loss: 0.8863 - val_RMSE: 0.7810\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8872 - RMSE: 0.7848 - val_loss: 0.8795 - val_RMSE: 0.7804\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8732 - RMSE: 0.7768 - val_loss: 0.8732 - val_RMSE: 0.7798\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8881 - RMSE: 0.7973 - val_loss: 0.8673 - val_RMSE: 0.7792\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8641 - RMSE: 0.7785 - val_loss: 0.8618 - val_RMSE: 0.7788\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8613 - RMSE: 0.7806 - val_loss: 0.8568 - val_RMSE: 0.7784\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8579 - RMSE: 0.7818 - val_loss: 0.8519 - val_RMSE: 0.7780\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8504 - RMSE: 0.7786 - val_loss: 0.8475 - val_RMSE: 0.7776\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8428 - RMSE: 0.7751 - val_loss: 0.8435 - val_RMSE: 0.7773\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8538 - RMSE: 0.7898 - val_loss: 0.8396 - val_RMSE: 0.7771\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8352 - RMSE: 0.7746 - val_loss: 0.8361 - val_RMSE: 0.7768\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8300 - RMSE: 0.7727 - val_loss: 0.8327 - val_RMSE: 0.7764\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8280 - RMSE: 0.7736 - val_loss: 0.8296 - val_RMSE: 0.7762\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8305 - RMSE: 0.7789 - val_loss: 0.8266 - val_RMSE: 0.7758\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8202 - RMSE: 0.7711 - val_loss: 0.8239 - val_RMSE: 0.7755\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8235 - RMSE: 0.7768 - val_loss: 0.8212 - val_RMSE: 0.7751\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8193 - RMSE: 0.7748 - val_loss: 0.8187 - val_RMSE: 0.7746\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8228 - RMSE: 0.7802 - val_loss: 0.8162 - val_RMSE: 0.7740\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8244 - RMSE: 0.7837 - val_loss: 0.8141 - val_RMSE: 0.7736\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8048 - RMSE: 0.7658 - val_loss: 0.8120 - val_RMSE: 0.7731\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8184 - RMSE: 0.7810 - val_loss: 0.8098 - val_RMSE: 0.7724\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.8061 - RMSE: 0.7701 - val_loss: 0.8080 - val_RMSE: 0.7719\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8058 - RMSE: 0.7711 - val_loss: 0.8061 - val_RMSE: 0.7713\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.8020 - RMSE: 0.7685 - val_loss: 0.8043 - val_RMSE: 0.7706\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7990 - RMSE: 0.7666 - val_loss: 0.8024 - val_RMSE: 0.7698\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7967 - RMSE: 0.7653 - val_loss: 0.8007 - val_RMSE: 0.7689\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7933 - RMSE: 0.7629 - val_loss: 0.7991 - val_RMSE: 0.7682\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7883 - RMSE: 0.7587 - val_loss: 0.7975 - val_RMSE: 0.7673\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7962 - RMSE: 0.7673 - val_loss: 0.7960 - val_RMSE: 0.7665\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7921 - RMSE: 0.7638 - val_loss: 0.7946 - val_RMSE: 0.7657\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7877 - RMSE: 0.7601 - val_loss: 0.7930 - val_RMSE: 0.7647\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7901 - RMSE: 0.7631 - val_loss: 0.7916 - val_RMSE: 0.7638\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7887 - RMSE: 0.7622 - val_loss: 0.7902 - val_RMSE: 0.7628\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7860 - RMSE: 0.7600 - val_loss: 0.7888 - val_RMSE: 0.7618\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7816 - RMSE: 0.7559 - val_loss: 0.7878 - val_RMSE: 0.7612\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7620 - RMSE: 0.7367 - val_loss: 0.7864 - val_RMSE: 0.7601\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7855 - RMSE: 0.7606 - val_loss: 0.7851 - val_RMSE: 0.7591\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7794 - RMSE: 0.7548 - val_loss: 0.7839 - val_RMSE: 0.7581\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7766 - RMSE: 0.7522 - val_loss: 0.7826 - val_RMSE: 0.7571\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7711 - RMSE: 0.7470 - val_loss: 0.7812 - val_RMSE: 0.7559\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7606 - RMSE: 0.7367 - val_loss: 0.7801 - val_RMSE: 0.7551\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7758 - RMSE: 0.7521 - val_loss: 0.7790 - val_RMSE: 0.7542\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7736 - RMSE: 0.7502 - val_loss: 0.7779 - val_RMSE: 0.7532\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7721 - RMSE: 0.7489 - val_loss: 0.7768 - val_RMSE: 0.7523\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7673 - RMSE: 0.7442 - val_loss: 0.7758 - val_RMSE: 0.7514\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7532 - RMSE: 0.7302 - val_loss: 0.7751 - val_RMSE: 0.7507\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7667 - RMSE: 0.7438 - val_loss: 0.7740 - val_RMSE: 0.7498\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7580 - RMSE: 0.7354 - val_loss: 0.7730 - val_RMSE: 0.7489\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7606 - RMSE: 0.7382 - val_loss: 0.7720 - val_RMSE: 0.7480\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7565 - RMSE: 0.7342 - val_loss: 0.7712 - val_RMSE: 0.7473\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7535 - RMSE: 0.7313 - val_loss: 0.7703 - val_RMSE: 0.7465\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7652 - RMSE: 0.7430 - val_loss: 0.7694 - val_RMSE: 0.7457\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7367 - RMSE: 0.7146 - val_loss: 0.7685 - val_RMSE: 0.7447\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7359 - RMSE: 0.7139 - val_loss: 0.7678 - val_RMSE: 0.7440\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7445 - RMSE: 0.7226 - val_loss: 0.7671 - val_RMSE: 0.7434\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7578 - RMSE: 0.7360 - val_loss: 0.7663 - val_RMSE: 0.7428\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7425 - RMSE: 0.7208 - val_loss: 0.7655 - val_RMSE: 0.7420\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7492 - RMSE: 0.7276 - val_loss: 0.7650 - val_RMSE: 0.7416\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7492 - RMSE: 0.7277 - val_loss: 0.7643 - val_RMSE: 0.7410\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7446 - RMSE: 0.7232 - val_loss: 0.7637 - val_RMSE: 0.7405\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7438 - RMSE: 0.7225 - val_loss: 0.7631 - val_RMSE: 0.7400\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7380 - RMSE: 0.7167 - val_loss: 0.7628 - val_RMSE: 0.7396\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7476 - RMSE: 0.7264 - val_loss: 0.7619 - val_RMSE: 0.7389\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7394 - RMSE: 0.7184 - val_loss: 0.7615 - val_RMSE: 0.7386\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7359 - RMSE: 0.7149 - val_loss: 0.7609 - val_RMSE: 0.7381\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7325 - RMSE: 0.7116 - val_loss: 0.7605 - val_RMSE: 0.7377\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7437 - RMSE: 0.7229 - val_loss: 0.7601 - val_RMSE: 0.7374\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7330 - RMSE: 0.7124 - val_loss: 0.7596 - val_RMSE: 0.7371\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7364 - RMSE: 0.7160 - val_loss: 0.7589 - val_RMSE: 0.7365\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7378 - RMSE: 0.7175 - val_loss: 0.7587 - val_RMSE: 0.7364\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7423 - RMSE: 0.7220 - val_loss: 0.7581 - val_RMSE: 0.7358\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7487 - RMSE: 0.7285 - val_loss: 0.7579 - val_RMSE: 0.7358\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7300 - RMSE: 0.7100 - val_loss: 0.7577 - val_RMSE: 0.7357\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7289 - RMSE: 0.7090 - val_loss: 0.7571 - val_RMSE: 0.7352\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7309 - RMSE: 0.7111 - val_loss: 0.7567 - val_RMSE: 0.7349\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7482 - RMSE: 0.7286 - val_loss: 0.7569 - val_RMSE: 0.7353\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7385 - RMSE: 0.7190 - val_loss: 0.7564 - val_RMSE: 0.7349\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7426 - RMSE: 0.7232 - val_loss: 0.7563 - val_RMSE: 0.7349\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7366 - RMSE: 0.7173 - val_loss: 0.7562 - val_RMSE: 0.7348\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7194 - RMSE: 0.7002 - val_loss: 0.7558 - val_RMSE: 0.7345\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7318 - RMSE: 0.7126 - val_loss: 0.7556 - val_RMSE: 0.7344\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7310 - RMSE: 0.7119 - val_loss: 0.7554 - val_RMSE: 0.7344\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7361 - RMSE: 0.7171 - val_loss: 0.7552 - val_RMSE: 0.7342\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7372 - RMSE: 0.7183 - val_loss: 0.7551 - val_RMSE: 0.7342\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7312 - RMSE: 0.7125 - val_loss: 0.7547 - val_RMSE: 0.7338\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7335 - RMSE: 0.7148 - val_loss: 0.7545 - val_RMSE: 0.7337\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7337 - RMSE: 0.7151 - val_loss: 0.7544 - val_RMSE: 0.7337\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7320 - RMSE: 0.7135 - val_loss: 0.7547 - val_RMSE: 0.7340\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7259 - RMSE: 0.7074 - val_loss: 0.7545 - val_RMSE: 0.7339\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7252 - RMSE: 0.7068 - val_loss: 0.7541 - val_RMSE: 0.7336\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7257 - RMSE: 0.7073 - val_loss: 0.7538 - val_RMSE: 0.7334\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7314 - RMSE: 0.7132 - val_loss: 0.7536 - val_RMSE: 0.7333\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7284 - RMSE: 0.7103 - val_loss: 0.7536 - val_RMSE: 0.7334\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7395 - RMSE: 0.7215 - val_loss: 0.7532 - val_RMSE: 0.7331\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7310 - RMSE: 0.7131 - val_loss: 0.7533 - val_RMSE: 0.7333\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7238 - RMSE: 0.7060 - val_loss: 0.7536 - val_RMSE: 0.7336\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7271 - RMSE: 0.7093 - val_loss: 0.7535 - val_RMSE: 0.7335\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7219 - RMSE: 0.7042 - val_loss: 0.7531 - val_RMSE: 0.7333\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7283 - RMSE: 0.7107 - val_loss: 0.7530 - val_RMSE: 0.7332\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7323 - RMSE: 0.7147 - val_loss: 0.7531 - val_RMSE: 0.7334\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7157 - RMSE: 0.6981 - val_loss: 0.7523 - val_RMSE: 0.7326\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7285 - RMSE: 0.7110 - val_loss: 0.7522 - val_RMSE: 0.7327\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7218 - RMSE: 0.7044 - val_loss: 0.7519 - val_RMSE: 0.7323\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7194 - RMSE: 0.7019 - val_loss: 0.7520 - val_RMSE: 0.7325\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7065 - RMSE: 0.6891 - val_loss: 0.7518 - val_RMSE: 0.7323\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7108 - RMSE: 0.6935 - val_loss: 0.7517 - val_RMSE: 0.7322\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7228 - RMSE: 0.7055 - val_loss: 0.7518 - val_RMSE: 0.7324\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7228 - RMSE: 0.7056 - val_loss: 0.7515 - val_RMSE: 0.7322\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7237 - RMSE: 0.7064 - val_loss: 0.7516 - val_RMSE: 0.7323\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7249 - RMSE: 0.7078 - val_loss: 0.7511 - val_RMSE: 0.7319\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7156 - RMSE: 0.6986 - val_loss: 0.7513 - val_RMSE: 0.7322\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7219 - RMSE: 0.7049 - val_loss: 0.7512 - val_RMSE: 0.7321\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7350 - RMSE: 0.7180 - val_loss: 0.7513 - val_RMSE: 0.7322\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7085 - RMSE: 0.6915 - val_loss: 0.7511 - val_RMSE: 0.7320\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7130 - RMSE: 0.6961 - val_loss: 0.7515 - val_RMSE: 0.7324\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7198 - RMSE: 0.7029 - val_loss: 0.7514 - val_RMSE: 0.7323\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7156 - RMSE: 0.6987 - val_loss: 0.7512 - val_RMSE: 0.7322\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7174 - RMSE: 0.7005 - val_loss: 0.7509 - val_RMSE: 0.7319\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7213 - RMSE: 0.7045 - val_loss: 0.7512 - val_RMSE: 0.7322\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7128 - RMSE: 0.6959 - val_loss: 0.7509 - val_RMSE: 0.7319\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7103 - RMSE: 0.6935 - val_loss: 0.7510 - val_RMSE: 0.7320\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7050 - RMSE: 0.6883 - val_loss: 0.7507 - val_RMSE: 0.7317\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7201 - RMSE: 0.7033 - val_loss: 0.7501 - val_RMSE: 0.7312\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7109 - RMSE: 0.6942 - val_loss: 0.7500 - val_RMSE: 0.7312\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7193 - RMSE: 0.7026 - val_loss: 0.7497 - val_RMSE: 0.7309\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7137 - RMSE: 0.6970 - val_loss: 0.7500 - val_RMSE: 0.7312\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7283 - RMSE: 0.7117 - val_loss: 0.7501 - val_RMSE: 0.7313\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7136 - RMSE: 0.6969 - val_loss: 0.7499 - val_RMSE: 0.7311\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7112 - RMSE: 0.6946 - val_loss: 0.7497 - val_RMSE: 0.7309\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7118 - RMSE: 0.6952 - val_loss: 0.7499 - val_RMSE: 0.7311\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7101 - RMSE: 0.6935 - val_loss: 0.7502 - val_RMSE: 0.7313\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7090 - RMSE: 0.6923 - val_loss: 0.7506 - val_RMSE: 0.7317\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7124 - RMSE: 0.6958 - val_loss: 0.7505 - val_RMSE: 0.7317\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7129 - RMSE: 0.6963 - val_loss: 0.7505 - val_RMSE: 0.7317\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7120 - RMSE: 0.6954 - val_loss: 0.7502 - val_RMSE: 0.7314\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7110 - RMSE: 0.6945 - val_loss: 0.7502 - val_RMSE: 0.7315\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7122 - RMSE: 0.6957 - val_loss: 0.7503 - val_RMSE: 0.7316\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7134 - RMSE: 0.6969 - val_loss: 0.7504 - val_RMSE: 0.7316\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7021 - RMSE: 0.6856 - val_loss: 0.7507 - val_RMSE: 0.7319\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6955 - RMSE: 0.6790 - val_loss: 0.7506 - val_RMSE: 0.7319\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7133 - RMSE: 0.6968 - val_loss: 0.7504 - val_RMSE: 0.7317\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7110 - RMSE: 0.6945 - val_loss: 0.7506 - val_RMSE: 0.7318\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7048 - RMSE: 0.6882 - val_loss: 0.7509 - val_RMSE: 0.7320\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7042 - RMSE: 0.6876 - val_loss: 0.7508 - val_RMSE: 0.7319\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7097 - RMSE: 0.6930 - val_loss: 0.7510 - val_RMSE: 0.7321\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7067 - RMSE: 0.6901 - val_loss: 0.7511 - val_RMSE: 0.7322\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7090 - RMSE: 0.6923 - val_loss: 0.7514 - val_RMSE: 0.7324\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7067 - RMSE: 0.6900 - val_loss: 0.7518 - val_RMSE: 0.7328\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7121 - RMSE: 0.6954 - val_loss: 0.7514 - val_RMSE: 0.7325\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7090 - RMSE: 0.6925 - val_loss: 0.7514 - val_RMSE: 0.7325\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7034 - RMSE: 0.6868 - val_loss: 0.7511 - val_RMSE: 0.7322\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7112 - RMSE: 0.6946 - val_loss: 0.7511 - val_RMSE: 0.7322\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7081 - RMSE: 0.6915 - val_loss: 0.7514 - val_RMSE: 0.7325\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7046 - RMSE: 0.6880 - val_loss: 0.7510 - val_RMSE: 0.7321\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7087 - RMSE: 0.6921 - val_loss: 0.7513 - val_RMSE: 0.7324\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7086 - RMSE: 0.6920 - val_loss: 0.7511 - val_RMSE: 0.7322\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7076 - RMSE: 0.6909 - val_loss: 0.7511 - val_RMSE: 0.7322\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7038 - RMSE: 0.6871 - val_loss: 0.7506 - val_RMSE: 0.7317\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6941 - RMSE: 0.6774 - val_loss: 0.7510 - val_RMSE: 0.7320\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7070 - RMSE: 0.6903 - val_loss: 0.7513 - val_RMSE: 0.7323\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7059 - RMSE: 0.6892 - val_loss: 0.7511 - val_RMSE: 0.7321\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6969 - RMSE: 0.6801 - val_loss: 0.7516 - val_RMSE: 0.7325\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6870 - RMSE: 0.6702 - val_loss: 0.7519 - val_RMSE: 0.7328\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6992 - RMSE: 0.6824 - val_loss: 0.7517 - val_RMSE: 0.7325\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7004 - RMSE: 0.6836 - val_loss: 0.7517 - val_RMSE: 0.7325\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6909 - RMSE: 0.6740 - val_loss: 0.7518 - val_RMSE: 0.7325\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6958 - RMSE: 0.6789 - val_loss: 0.7512 - val_RMSE: 0.7320\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7014 - RMSE: 0.6846 - val_loss: 0.7511 - val_RMSE: 0.7319\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7022 - RMSE: 0.6853 - val_loss: 0.7511 - val_RMSE: 0.7318\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6907 - RMSE: 0.6737 - val_loss: 0.7514 - val_RMSE: 0.7321\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6797 - RMSE: 0.6627 - val_loss: 0.7511 - val_RMSE: 0.7319\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6912 - RMSE: 0.6742 - val_loss: 0.7511 - val_RMSE: 0.7318\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7021 - RMSE: 0.6852 - val_loss: 0.7509 - val_RMSE: 0.7317\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6961 - RMSE: 0.6791 - val_loss: 0.7510 - val_RMSE: 0.7318\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6988 - RMSE: 0.6818 - val_loss: 0.7513 - val_RMSE: 0.7321\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6979 - RMSE: 0.6809 - val_loss: 0.7514 - val_RMSE: 0.7321\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6932 - RMSE: 0.6762 - val_loss: 0.7512 - val_RMSE: 0.7318\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7017 - RMSE: 0.6846 - val_loss: 0.7512 - val_RMSE: 0.7318\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6992 - RMSE: 0.6822 - val_loss: 0.7509 - val_RMSE: 0.7316\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6908 - RMSE: 0.6737 - val_loss: 0.7510 - val_RMSE: 0.7316\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6939 - RMSE: 0.6768 - val_loss: 0.7510 - val_RMSE: 0.7315\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6801 - RMSE: 0.6630 - val_loss: 0.7509 - val_RMSE: 0.7315\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6794 - RMSE: 0.6623 - val_loss: 0.7507 - val_RMSE: 0.7312\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6825 - RMSE: 0.6653 - val_loss: 0.7509 - val_RMSE: 0.7314\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6921 - RMSE: 0.6749 - val_loss: 0.7509 - val_RMSE: 0.7313\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6980 - RMSE: 0.6808 - val_loss: 0.7511 - val_RMSE: 0.7315\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6980 - RMSE: 0.6807 - val_loss: 0.7511 - val_RMSE: 0.7315\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6826 - RMSE: 0.6653 - val_loss: 0.7512 - val_RMSE: 0.7316\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6944 - RMSE: 0.6772 - val_loss: 0.7512 - val_RMSE: 0.7316\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6906 - RMSE: 0.6734 - val_loss: 0.7510 - val_RMSE: 0.7313\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6753 - RMSE: 0.6580 - val_loss: 0.7510 - val_RMSE: 0.7313\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6829 - RMSE: 0.6656 - val_loss: 0.7521 - val_RMSE: 0.7323\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6887 - RMSE: 0.6714 - val_loss: 0.7515 - val_RMSE: 0.7318\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6886 - RMSE: 0.6712 - val_loss: 0.7518 - val_RMSE: 0.7321\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6803 - RMSE: 0.6630 - val_loss: 0.7518 - val_RMSE: 0.7320\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6894 - RMSE: 0.6721 - val_loss: 0.7516 - val_RMSE: 0.7319\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6996 - RMSE: 0.6822 - val_loss: 0.7517 - val_RMSE: 0.7320\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6886 - RMSE: 0.6712 - val_loss: 0.7515 - val_RMSE: 0.7318\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6798 - RMSE: 0.6624 - val_loss: 0.7513 - val_RMSE: 0.7315\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6818 - RMSE: 0.6644 - val_loss: 0.7516 - val_RMSE: 0.7318\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6863 - RMSE: 0.6688 - val_loss: 0.7520 - val_RMSE: 0.7322\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6903 - RMSE: 0.6729 - val_loss: 0.7520 - val_RMSE: 0.7322\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6872 - RMSE: 0.6697 - val_loss: 0.7523 - val_RMSE: 0.7324\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6842 - RMSE: 0.6667 - val_loss: 0.7519 - val_RMSE: 0.7321\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6835 - RMSE: 0.6660 - val_loss: 0.7523 - val_RMSE: 0.7324\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6787 - RMSE: 0.6611 - val_loss: 0.7519 - val_RMSE: 0.7320\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6886 - RMSE: 0.6711 - val_loss: 0.7523 - val_RMSE: 0.7324\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6792 - RMSE: 0.6616 - val_loss: 0.7519 - val_RMSE: 0.7319\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6725 - RMSE: 0.6549 - val_loss: 0.7522 - val_RMSE: 0.7322\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6952 - RMSE: 0.6776 - val_loss: 0.7523 - val_RMSE: 0.7323\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6739 - RMSE: 0.6563 - val_loss: 0.7524 - val_RMSE: 0.7325\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6747 - RMSE: 0.6571 - val_loss: 0.7521 - val_RMSE: 0.7321\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6707 - RMSE: 0.6531 - val_loss: 0.7520 - val_RMSE: 0.7320\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6840 - RMSE: 0.6663 - val_loss: 0.7524 - val_RMSE: 0.7323\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6884 - RMSE: 0.6707 - val_loss: 0.7522 - val_RMSE: 0.7321\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6820 - RMSE: 0.6643 - val_loss: 0.7524 - val_RMSE: 0.7322\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6783 - RMSE: 0.6606 - val_loss: 0.7521 - val_RMSE: 0.7319\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6773 - RMSE: 0.6596 - val_loss: 0.7523 - val_RMSE: 0.7320\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6822 - RMSE: 0.6645 - val_loss: 0.7520 - val_RMSE: 0.7318\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6839 - RMSE: 0.6662 - val_loss: 0.7528 - val_RMSE: 0.7325\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6828 - RMSE: 0.6651 - val_loss: 0.7524 - val_RMSE: 0.7321\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6748 - RMSE: 0.6571 - val_loss: 0.7529 - val_RMSE: 0.7325\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6886 - RMSE: 0.6709 - val_loss: 0.7520 - val_RMSE: 0.7316\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6625 - RMSE: 0.6447 - val_loss: 0.7526 - val_RMSE: 0.7322\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6733 - RMSE: 0.6555 - val_loss: 0.7529 - val_RMSE: 0.7325\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6753 - RMSE: 0.6575 - val_loss: 0.7534 - val_RMSE: 0.7330\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6719 - RMSE: 0.6540 - val_loss: 0.7537 - val_RMSE: 0.7333\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6706 - RMSE: 0.6527 - val_loss: 0.7544 - val_RMSE: 0.7339\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6826 - RMSE: 0.6648 - val_loss: 0.7539 - val_RMSE: 0.7334\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6768 - RMSE: 0.6589 - val_loss: 0.7531 - val_RMSE: 0.7326\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6754 - RMSE: 0.6576 - val_loss: 0.7536 - val_RMSE: 0.7330\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6670 - RMSE: 0.6492 - val_loss: 0.7536 - val_RMSE: 0.7331\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6620 - RMSE: 0.6440 - val_loss: 0.7538 - val_RMSE: 0.7332\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6780 - RMSE: 0.6601 - val_loss: 0.7537 - val_RMSE: 0.7330\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6730 - RMSE: 0.6551 - val_loss: 0.7544 - val_RMSE: 0.7338\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6819 - RMSE: 0.6640 - val_loss: 0.7541 - val_RMSE: 0.7335\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6732 - RMSE: 0.6553 - val_loss: 0.7539 - val_RMSE: 0.7334\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6686 - RMSE: 0.6506 - val_loss: 0.7539 - val_RMSE: 0.7334\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6612 - RMSE: 0.6432 - val_loss: 0.7548 - val_RMSE: 0.7341\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6812 - RMSE: 0.6632 - val_loss: 0.7545 - val_RMSE: 0.7339\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6849 - RMSE: 0.6669 - val_loss: 0.7544 - val_RMSE: 0.7337\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6783 - RMSE: 0.6603 - val_loss: 0.7542 - val_RMSE: 0.7334\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6588 - RMSE: 0.6407 - val_loss: 0.7545 - val_RMSE: 0.7338\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6689 - RMSE: 0.6508 - val_loss: 0.7543 - val_RMSE: 0.7335\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6781 - RMSE: 0.6601 - val_loss: 0.7549 - val_RMSE: 0.7341\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6697 - RMSE: 0.6517 - val_loss: 0.7549 - val_RMSE: 0.7341\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6721 - RMSE: 0.6540 - val_loss: 0.7544 - val_RMSE: 0.7336\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6683 - RMSE: 0.6502 - val_loss: 0.7556 - val_RMSE: 0.7348\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6782 - RMSE: 0.6601 - val_loss: 0.7554 - val_RMSE: 0.7346\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6756 - RMSE: 0.6574 - val_loss: 0.7555 - val_RMSE: 0.7346\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6657 - RMSE: 0.6475 - val_loss: 0.7556 - val_RMSE: 0.7347\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6767 - RMSE: 0.6585 - val_loss: 0.7557 - val_RMSE: 0.7348\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6692 - RMSE: 0.6510 - val_loss: 0.7556 - val_RMSE: 0.7347\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6752 - RMSE: 0.6570 - val_loss: 0.7555 - val_RMSE: 0.7345\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6645 - RMSE: 0.6463 - val_loss: 0.7553 - val_RMSE: 0.7344\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6592 - RMSE: 0.6410 - val_loss: 0.7549 - val_RMSE: 0.7340\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6733 - RMSE: 0.6552 - val_loss: 0.7554 - val_RMSE: 0.7345\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6663 - RMSE: 0.6481 - val_loss: 0.7552 - val_RMSE: 0.7342\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6670 - RMSE: 0.6488 - val_loss: 0.7544 - val_RMSE: 0.7335\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6628 - RMSE: 0.6446 - val_loss: 0.7542 - val_RMSE: 0.7333\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6822 - RMSE: 0.6639 - val_loss: 0.7548 - val_RMSE: 0.7339\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6733 - RMSE: 0.6550 - val_loss: 0.7554 - val_RMSE: 0.7345\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6775 - RMSE: 0.6593 - val_loss: 0.7559 - val_RMSE: 0.7349\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6620 - RMSE: 0.6437 - val_loss: 0.7554 - val_RMSE: 0.7345\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6687 - RMSE: 0.6505 - val_loss: 0.7552 - val_RMSE: 0.7342\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6577 - RMSE: 0.6395 - val_loss: 0.7553 - val_RMSE: 0.7344\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6657 - RMSE: 0.6475 - val_loss: 0.7554 - val_RMSE: 0.7344\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6691 - RMSE: 0.6508 - val_loss: 0.7555 - val_RMSE: 0.7346\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6525 - RMSE: 0.6342 - val_loss: 0.7558 - val_RMSE: 0.7348\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6680 - RMSE: 0.6497 - val_loss: 0.7560 - val_RMSE: 0.7351\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6681 - RMSE: 0.6498 - val_loss: 0.7558 - val_RMSE: 0.7349\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6591 - RMSE: 0.6407 - val_loss: 0.7561 - val_RMSE: 0.7351\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6576 - RMSE: 0.6392 - val_loss: 0.7556 - val_RMSE: 0.7346\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6692 - RMSE: 0.6508 - val_loss: 0.7554 - val_RMSE: 0.7343\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6599 - RMSE: 0.6415 - val_loss: 0.7557 - val_RMSE: 0.7345\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6467 - RMSE: 0.6283 - val_loss: 0.7556 - val_RMSE: 0.7343\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6497 - RMSE: 0.6312 - val_loss: 0.7559 - val_RMSE: 0.7346\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6552 - RMSE: 0.6367 - val_loss: 0.7553 - val_RMSE: 0.7340\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6657 - RMSE: 0.6471 - val_loss: 0.7548 - val_RMSE: 0.7335\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6597 - RMSE: 0.6412 - val_loss: 0.7553 - val_RMSE: 0.7341\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6625 - RMSE: 0.6440 - val_loss: 0.7555 - val_RMSE: 0.7342\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6582 - RMSE: 0.6396 - val_loss: 0.7562 - val_RMSE: 0.7349\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6617 - RMSE: 0.6431 - val_loss: 0.7558 - val_RMSE: 0.7345\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6631 - RMSE: 0.6445 - val_loss: 0.7554 - val_RMSE: 0.7341\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6552 - RMSE: 0.6366 - val_loss: 0.7550 - val_RMSE: 0.7336\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6631 - RMSE: 0.6445 - val_loss: 0.7550 - val_RMSE: 0.7337\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6621 - RMSE: 0.6434 - val_loss: 0.7554 - val_RMSE: 0.7341\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6538 - RMSE: 0.6351 - val_loss: 0.7562 - val_RMSE: 0.7349\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6612 - RMSE: 0.6426 - val_loss: 0.7559 - val_RMSE: 0.7345\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6520 - RMSE: 0.6334 - val_loss: 0.7554 - val_RMSE: 0.7341\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6559 - RMSE: 0.6373 - val_loss: 0.7559 - val_RMSE: 0.7346\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6594 - RMSE: 0.6408 - val_loss: 0.7557 - val_RMSE: 0.7344\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6579 - RMSE: 0.6392 - val_loss: 0.7555 - val_RMSE: 0.7342\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6645 - RMSE: 0.6458 - val_loss: 0.7554 - val_RMSE: 0.7340\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.6701 - RMSE: 0.6514 - val_loss: 0.7552 - val_RMSE: 0.7338\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.6650 - RMSE: 0.6463 - val_loss: 0.7555 - val_RMSE: 0.7341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "iQAnqOULq-b3",
        "outputId": "f3cea7d7-5283-4c54-e28d-d46734398fa6"
      },
      "source": [
        "plt.plot(result.history['RMSE'], label=\"Train RMSE\")\n",
        "plt.plot(result.history['val_RMSE'], label=\"Test RMSE\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZABTMLtIumOH"
      },
      "source": [
        "model.save(f'/gdrive/Shareddrives/Spotify Recommendation/Recommendation Models/NeuralNet_Nationality_{today()}.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AulFxsMKejbe"
      },
      "source": [
        "#### Tuner 있는 버전"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpnnO3LFejbi",
        "outputId": "fe0a647f-aaca-4496-df7b-e5aead7ccfee"
      },
      "source": [
        "EPOCHS = 400\n",
        "def build_model(hp):\n",
        "# Keras model\n",
        "  \n",
        "  K = hp.Int('K',min_value=10,max_value=int(0.5*M),step=1)\n",
        "  # Keras model\n",
        "  user = Input(shape=(1, ))                                               # User input\n",
        "  item = Input(shape=(1, ))                                               # Item input\n",
        "  P_embedding = Embedding(M, K, embeddings_regularizer=l2())(user)        # (M, 1, K)\n",
        "  Q_embedding = Embedding(N, K, embeddings_regularizer=l2())(item)        # (N, 1, K)\n",
        "  user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)          # User bias term (M, 1, )\n",
        "  item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)          # Item bias term (N, 1, )\n",
        "\n",
        "  nat = Input(shape=(1, ))\n",
        "  nat_embedding = Embedding(L, int(0.5*L), embeddings_regularizer=l2())(nat)\n",
        "  nat_layer = Flatten()(nat_embedding)\n",
        "\n",
        "\n",
        "  # Concatenate layers\n",
        "\n",
        "  P_embedding = Flatten()(P_embedding)                                    # (K, )\n",
        "  Q_embedding = Flatten()(Q_embedding)                                    # (K, )\n",
        "  user_bias = Flatten()(user_bias)                                        # (1, )\n",
        "  item_bias = Flatten()(item_bias)                                        # (1, )\n",
        "  R = Concatenate()([P_embedding, Q_embedding, user_bias, item_bias, nat_layer])\n",
        "\n",
        "  # Neural network\n",
        "  case = hp.Int('Case' , min_value=1,max_value=2,step=1)\n",
        "  if case ==1:\n",
        "    neurons = hp.Int('Nuerons' , min_value=16,max_value=128,step=16)\n",
        "    R = Dense(neurons,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "  \n",
        "  elif case ==2:\n",
        "    neuron_1 = hp.Int('Nuerons' , min_value=64,max_value=128,step=16)\n",
        "    neuron_2 = int(0.5*neuron_1)\n",
        "    R = Dense(neuron_1,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "    R = Dense(neuron_2,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "  #neurons = hp.Int('Nuerons' , min_value=16,max_value=128,step=16)\n",
        "  #R = Dense(neurons,activation='relu',kernel_initializer='he_uniform')(R)\n",
        "  \n",
        "  R = Dense(1)(R)\n",
        "\n",
        "\n",
        "  # Model setting\n",
        "  starter_learning_rate = 0.1\n",
        "  end_learning_rate = 0.0001\n",
        "  decay_steps = int(0.2*EPOCHS)\n",
        "  learning_rate_fn = PolynomialDecay(\n",
        "      starter_learning_rate,\n",
        "      decay_steps,\n",
        "      end_learning_rate,\n",
        "      power=0.1)\n",
        "  \n",
        "  model = Model(inputs=[user, item,nat], outputs=R)\n",
        "  \n",
        "  optimizer = hp.Choice('Optimizer',['SGD','Adam'])\n",
        "  if optimizer == 'Adam':\n",
        "    model.compile(\n",
        "      loss=RMSE,\n",
        "      optimizer=Adam(learning_rate=learning_rate_fn),\n",
        "      metrics=[RMSE]\n",
        "      \n",
        "    )\n",
        "  elif optimizer =='SGD':\n",
        "    model.compile(\n",
        "      loss=RMSE,\n",
        "      optimizer=SGD(learning_rate=learning_rate_fn,momentum=0.9),\n",
        "      metrics=[RMSE]\n",
        "    )\n",
        "  return model\n",
        "\n",
        "hp = kt.HyperParameters()\n",
        "\n",
        "# Initialize the tuner by passing the `build_model` function\n",
        "# and specifying key search constraints: maximize val_acc (objective),\n",
        "# and the number of trials to do. More efficient tuners like UltraBand() can\n",
        "# be used.\n",
        "TRIALS = 20\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_loss\",\n",
        "    max_trials=TRIALS,\n",
        "    project_name=f\"{today()}_Spotify Recommendation Systems\"\n",
        ")\n",
        "\n",
        "# Display search space overview\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Perform the model search. The search function has the same signature\n",
        "# as `model.fit()`.\n",
        "tuner.search(\n",
        "    [ratings_train.users.values, ratings_train.tracks.values,train_nat.values],ratings_train.score.values - mu,batch_size=64, epochs=EPOCHS, \n",
        "    validation_data=([ratings_valid.users.values, ratings_valid.tracks.values,valid_nat.values], ratings_valid.score.values - mu)\n",
        ")\n",
        "\n",
        "# Display the best models, their hyperparameters, and the resulting metrics.\n",
        "tuner.results_summary()\n",
        "\n",
        "# Retrieve the best model and display its architecture\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_model.summary()\n",
        "\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "result = best_model.fit([ratings_train.users.values, ratings_train.tracks.values,train_nat.values],ratings_train.score.values - mu,\n",
        "               batch_size=64, \n",
        "               epochs=EPOCHS,\n",
        "               validation_data=([ratings_valid.users.values, ratings_valid.tracks.values,valid_nat.values], ratings_valid.score.values - mu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 20 Complete [00h 01m 19s]\n",
            "val_loss: 0.7790916562080383\n",
            "\n",
            "Best val_loss So Far: 0.758938193321228\n",
            "Total elapsed time: 00h 24m 26s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Results summary\n",
            "Results in ./20210405_13시18분_Spotify Recommendation Systems\n",
            "Showing 10 best trials\n",
            "Objective(name='val_loss', direction='min')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 11\n",
            "Case: 2\n",
            "Nuerons: 80\n",
            "Optimizer: SGD\n",
            "Score: 0.758938193321228\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 11\n",
            "Case: 2\n",
            "Nuerons: 96\n",
            "Optimizer: Adam\n",
            "Score: 0.7643963694572449\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 11\n",
            "Case: 2\n",
            "Nuerons: 96\n",
            "Optimizer: SGD\n",
            "Score: 0.7659991383552551\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 13\n",
            "Case: 1\n",
            "Nuerons: 128\n",
            "Optimizer: SGD\n",
            "Score: 0.7669990062713623\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 12\n",
            "Case: 1\n",
            "Nuerons: 48\n",
            "Optimizer: SGD\n",
            "Score: 0.768923819065094\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 13\n",
            "Case: 1\n",
            "Nuerons: 32\n",
            "Optimizer: SGD\n",
            "Score: 0.7701215744018555\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 12\n",
            "Case: 2\n",
            "Nuerons: 48\n",
            "Optimizer: SGD\n",
            "Score: 0.7705223560333252\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 12\n",
            "Case: 2\n",
            "Nuerons: 16\n",
            "Optimizer: Adam\n",
            "Score: 0.7717562317848206\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 13\n",
            "Case: 1\n",
            "Nuerons: 16\n",
            "Optimizer: SGD\n",
            "Score: 0.7725580334663391\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "K: 13\n",
            "Case: 1\n",
            "Nuerons: 112\n",
            "Optimizer: SGD\n",
            "Score: 0.7733954787254333\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 11)        319         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1, 11)        1782        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 1)         29          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1, 1)         162         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 1, 2)         10          input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 11)           0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 11)           0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 1)            0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 1)            0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2)            0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 26)           0           flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "                                                                 flatten_3[0][0]                  \n",
            "                                                                 flatten_4[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 80)           2160        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 40)           3240        dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            41          dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 7,743\n",
            "Trainable params: 7,743\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Epoch 1/400\n",
            "59/59 [==============================] - 1s 7ms/step - loss: 0.7405 - RMSE: 0.7319 - val_loss: 0.7590 - val_RMSE: 0.7530\n",
            "Epoch 2/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7276 - RMSE: 0.7190 - val_loss: 0.7590 - val_RMSE: 0.7530\n",
            "Epoch 3/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7421 - RMSE: 0.7335 - val_loss: 0.7589 - val_RMSE: 0.7529\n",
            "Epoch 4/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7486 - RMSE: 0.7399 - val_loss: 0.7590 - val_RMSE: 0.7529\n",
            "Epoch 5/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7394 - RMSE: 0.7307 - val_loss: 0.7589 - val_RMSE: 0.7529\n",
            "Epoch 6/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7426 - RMSE: 0.7340 - val_loss: 0.7590 - val_RMSE: 0.7529\n",
            "Epoch 7/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7441 - RMSE: 0.7354 - val_loss: 0.7589 - val_RMSE: 0.7529\n",
            "Epoch 8/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7401 - RMSE: 0.7314 - val_loss: 0.7590 - val_RMSE: 0.7529\n",
            "Epoch 9/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7374 - RMSE: 0.7288 - val_loss: 0.7590 - val_RMSE: 0.7529\n",
            "Epoch 10/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7387 - RMSE: 0.7300 - val_loss: 0.7590 - val_RMSE: 0.7529\n",
            "Epoch 11/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7404 - RMSE: 0.7318 - val_loss: 0.7590 - val_RMSE: 0.7528\n",
            "Epoch 12/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7290 - RMSE: 0.7203 - val_loss: 0.7590 - val_RMSE: 0.7528\n",
            "Epoch 13/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7365 - RMSE: 0.7277 - val_loss: 0.7590 - val_RMSE: 0.7528\n",
            "Epoch 14/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7425 - RMSE: 0.7338 - val_loss: 0.7590 - val_RMSE: 0.7528\n",
            "Epoch 15/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7391 - RMSE: 0.7304 - val_loss: 0.7590 - val_RMSE: 0.7528\n",
            "Epoch 16/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7388 - RMSE: 0.7300 - val_loss: 0.7590 - val_RMSE: 0.7527\n",
            "Epoch 17/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7386 - RMSE: 0.7299 - val_loss: 0.7590 - val_RMSE: 0.7527\n",
            "Epoch 18/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7379 - RMSE: 0.7291 - val_loss: 0.7591 - val_RMSE: 0.7527\n",
            "Epoch 19/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7331 - RMSE: 0.7243 - val_loss: 0.7590 - val_RMSE: 0.7527\n",
            "Epoch 20/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7272 - RMSE: 0.7185 - val_loss: 0.7591 - val_RMSE: 0.7527\n",
            "Epoch 21/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7324 - RMSE: 0.7236 - val_loss: 0.7591 - val_RMSE: 0.7527\n",
            "Epoch 22/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7347 - RMSE: 0.7259 - val_loss: 0.7591 - val_RMSE: 0.7527\n",
            "Epoch 23/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7384 - RMSE: 0.7296 - val_loss: 0.7591 - val_RMSE: 0.7527\n",
            "Epoch 24/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7391 - RMSE: 0.7303 - val_loss: 0.7592 - val_RMSE: 0.7527\n",
            "Epoch 25/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7370 - RMSE: 0.7282 - val_loss: 0.7592 - val_RMSE: 0.7527\n",
            "Epoch 26/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7310 - RMSE: 0.7222 - val_loss: 0.7592 - val_RMSE: 0.7527\n",
            "Epoch 27/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7300 - RMSE: 0.7212 - val_loss: 0.7592 - val_RMSE: 0.7527\n",
            "Epoch 28/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7307 - RMSE: 0.7219 - val_loss: 0.7593 - val_RMSE: 0.7527\n",
            "Epoch 29/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7324 - RMSE: 0.7236 - val_loss: 0.7593 - val_RMSE: 0.7527\n",
            "Epoch 30/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7387 - RMSE: 0.7298 - val_loss: 0.7593 - val_RMSE: 0.7527\n",
            "Epoch 31/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7418 - RMSE: 0.7329 - val_loss: 0.7593 - val_RMSE: 0.7527\n",
            "Epoch 32/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7335 - RMSE: 0.7246 - val_loss: 0.7594 - val_RMSE: 0.7528\n",
            "Epoch 33/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7306 - RMSE: 0.7217 - val_loss: 0.7594 - val_RMSE: 0.7528\n",
            "Epoch 34/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7380 - RMSE: 0.7291 - val_loss: 0.7594 - val_RMSE: 0.7528\n",
            "Epoch 35/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7356 - RMSE: 0.7267 - val_loss: 0.7594 - val_RMSE: 0.7528\n",
            "Epoch 36/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7295 - RMSE: 0.7206 - val_loss: 0.7595 - val_RMSE: 0.7528\n",
            "Epoch 37/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7440 - RMSE: 0.7351 - val_loss: 0.7595 - val_RMSE: 0.7528\n",
            "Epoch 38/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7357 - RMSE: 0.7267 - val_loss: 0.7596 - val_RMSE: 0.7528\n",
            "Epoch 39/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7332 - RMSE: 0.7243 - val_loss: 0.7596 - val_RMSE: 0.7529\n",
            "Epoch 40/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7269 - RMSE: 0.7180 - val_loss: 0.7596 - val_RMSE: 0.7528\n",
            "Epoch 41/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7317 - RMSE: 0.7227 - val_loss: 0.7597 - val_RMSE: 0.7529\n",
            "Epoch 42/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7386 - RMSE: 0.7296 - val_loss: 0.7597 - val_RMSE: 0.7529\n",
            "Epoch 43/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7225 - RMSE: 0.7135 - val_loss: 0.7597 - val_RMSE: 0.7529\n",
            "Epoch 44/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7393 - RMSE: 0.7304 - val_loss: 0.7598 - val_RMSE: 0.7529\n",
            "Epoch 45/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7279 - RMSE: 0.7189 - val_loss: 0.7599 - val_RMSE: 0.7530\n",
            "Epoch 46/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7210 - RMSE: 0.7120 - val_loss: 0.7599 - val_RMSE: 0.7530\n",
            "Epoch 47/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7350 - RMSE: 0.7260 - val_loss: 0.7599 - val_RMSE: 0.7530\n",
            "Epoch 48/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7424 - RMSE: 0.7334 - val_loss: 0.7600 - val_RMSE: 0.7530\n",
            "Epoch 49/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7448 - RMSE: 0.7358 - val_loss: 0.7601 - val_RMSE: 0.7531\n",
            "Epoch 50/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7293 - RMSE: 0.7203 - val_loss: 0.7601 - val_RMSE: 0.7531\n",
            "Epoch 51/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7329 - RMSE: 0.7238 - val_loss: 0.7601 - val_RMSE: 0.7531\n",
            "Epoch 52/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7287 - RMSE: 0.7197 - val_loss: 0.7602 - val_RMSE: 0.7531\n",
            "Epoch 53/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7212 - RMSE: 0.7122 - val_loss: 0.7602 - val_RMSE: 0.7531\n",
            "Epoch 54/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7309 - RMSE: 0.7218 - val_loss: 0.7603 - val_RMSE: 0.7532\n",
            "Epoch 55/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7313 - RMSE: 0.7223 - val_loss: 0.7603 - val_RMSE: 0.7532\n",
            "Epoch 56/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7374 - RMSE: 0.7283 - val_loss: 0.7603 - val_RMSE: 0.7532\n",
            "Epoch 57/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7444 - RMSE: 0.7353 - val_loss: 0.7604 - val_RMSE: 0.7532\n",
            "Epoch 58/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7263 - RMSE: 0.7172 - val_loss: 0.7604 - val_RMSE: 0.7532\n",
            "Epoch 59/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7298 - RMSE: 0.7207 - val_loss: 0.7605 - val_RMSE: 0.7533\n",
            "Epoch 60/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7323 - RMSE: 0.7232 - val_loss: 0.7605 - val_RMSE: 0.7533\n",
            "Epoch 61/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7298 - RMSE: 0.7207 - val_loss: 0.7606 - val_RMSE: 0.7534\n",
            "Epoch 62/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7264 - RMSE: 0.7173 - val_loss: 0.7606 - val_RMSE: 0.7533\n",
            "Epoch 63/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7352 - RMSE: 0.7261 - val_loss: 0.7607 - val_RMSE: 0.7534\n",
            "Epoch 64/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7251 - RMSE: 0.7160 - val_loss: 0.7607 - val_RMSE: 0.7534\n",
            "Epoch 65/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7212 - RMSE: 0.7121 - val_loss: 0.7608 - val_RMSE: 0.7535\n",
            "Epoch 66/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7352 - RMSE: 0.7261 - val_loss: 0.7608 - val_RMSE: 0.7535\n",
            "Epoch 67/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7252 - RMSE: 0.7160 - val_loss: 0.7609 - val_RMSE: 0.7535\n",
            "Epoch 68/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7364 - RMSE: 0.7273 - val_loss: 0.7609 - val_RMSE: 0.7535\n",
            "Epoch 69/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7273 - RMSE: 0.7181 - val_loss: 0.7610 - val_RMSE: 0.7536\n",
            "Epoch 70/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7298 - RMSE: 0.7207 - val_loss: 0.7611 - val_RMSE: 0.7536\n",
            "Epoch 71/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7383 - RMSE: 0.7291 - val_loss: 0.7611 - val_RMSE: 0.7537\n",
            "Epoch 72/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7409 - RMSE: 0.7317 - val_loss: 0.7612 - val_RMSE: 0.7537\n",
            "Epoch 73/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7254 - RMSE: 0.7162 - val_loss: 0.7612 - val_RMSE: 0.7537\n",
            "Epoch 74/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7228 - RMSE: 0.7136 - val_loss: 0.7613 - val_RMSE: 0.7538\n",
            "Epoch 75/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7136 - RMSE: 0.7044 - val_loss: 0.7613 - val_RMSE: 0.7538\n",
            "Epoch 76/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7283 - RMSE: 0.7191 - val_loss: 0.7614 - val_RMSE: 0.7538\n",
            "Epoch 77/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7432 - RMSE: 0.7340 - val_loss: 0.7614 - val_RMSE: 0.7539\n",
            "Epoch 78/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7433 - RMSE: 0.7341 - val_loss: 0.7615 - val_RMSE: 0.7539\n",
            "Epoch 79/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7279 - RMSE: 0.7186 - val_loss: 0.7616 - val_RMSE: 0.7540\n",
            "Epoch 80/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7292 - RMSE: 0.7200 - val_loss: 0.7616 - val_RMSE: 0.7540\n",
            "Epoch 81/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7320 - RMSE: 0.7227 - val_loss: 0.7617 - val_RMSE: 0.7540\n",
            "Epoch 82/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7299 - RMSE: 0.7207 - val_loss: 0.7617 - val_RMSE: 0.7541\n",
            "Epoch 83/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7232 - RMSE: 0.7140 - val_loss: 0.7618 - val_RMSE: 0.7541\n",
            "Epoch 84/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7222 - RMSE: 0.7129 - val_loss: 0.7619 - val_RMSE: 0.7542\n",
            "Epoch 85/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7350 - RMSE: 0.7257 - val_loss: 0.7619 - val_RMSE: 0.7542\n",
            "Epoch 86/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7318 - RMSE: 0.7226 - val_loss: 0.7620 - val_RMSE: 0.7543\n",
            "Epoch 87/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7288 - RMSE: 0.7196 - val_loss: 0.7621 - val_RMSE: 0.7543\n",
            "Epoch 88/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7363 - RMSE: 0.7271 - val_loss: 0.7621 - val_RMSE: 0.7543\n",
            "Epoch 89/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7207 - RMSE: 0.7114 - val_loss: 0.7622 - val_RMSE: 0.7544\n",
            "Epoch 90/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7253 - RMSE: 0.7160 - val_loss: 0.7622 - val_RMSE: 0.7544\n",
            "Epoch 91/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7289 - RMSE: 0.7196 - val_loss: 0.7623 - val_RMSE: 0.7545\n",
            "Epoch 92/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7304 - RMSE: 0.7211 - val_loss: 0.7624 - val_RMSE: 0.7545\n",
            "Epoch 93/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7269 - RMSE: 0.7176 - val_loss: 0.7624 - val_RMSE: 0.7546\n",
            "Epoch 94/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7266 - RMSE: 0.7173 - val_loss: 0.7625 - val_RMSE: 0.7546\n",
            "Epoch 95/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7230 - RMSE: 0.7137 - val_loss: 0.7625 - val_RMSE: 0.7547\n",
            "Epoch 96/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7268 - RMSE: 0.7175 - val_loss: 0.7626 - val_RMSE: 0.7547\n",
            "Epoch 97/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7236 - RMSE: 0.7143 - val_loss: 0.7627 - val_RMSE: 0.7548\n",
            "Epoch 98/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7261 - RMSE: 0.7168 - val_loss: 0.7628 - val_RMSE: 0.7549\n",
            "Epoch 99/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7336 - RMSE: 0.7242 - val_loss: 0.7628 - val_RMSE: 0.7549\n",
            "Epoch 100/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7216 - RMSE: 0.7123 - val_loss: 0.7628 - val_RMSE: 0.7549\n",
            "Epoch 101/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7299 - RMSE: 0.7206 - val_loss: 0.7629 - val_RMSE: 0.7550\n",
            "Epoch 102/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7249 - RMSE: 0.7156 - val_loss: 0.7630 - val_RMSE: 0.7550\n",
            "Epoch 103/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7116 - RMSE: 0.7022 - val_loss: 0.7630 - val_RMSE: 0.7550\n",
            "Epoch 104/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7204 - RMSE: 0.7110 - val_loss: 0.7631 - val_RMSE: 0.7551\n",
            "Epoch 105/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7272 - RMSE: 0.7178 - val_loss: 0.7632 - val_RMSE: 0.7552\n",
            "Epoch 106/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7333 - RMSE: 0.7240 - val_loss: 0.7633 - val_RMSE: 0.7552\n",
            "Epoch 107/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7271 - RMSE: 0.7177 - val_loss: 0.7633 - val_RMSE: 0.7552\n",
            "Epoch 108/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7286 - RMSE: 0.7192 - val_loss: 0.7633 - val_RMSE: 0.7553\n",
            "Epoch 109/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7221 - RMSE: 0.7127 - val_loss: 0.7634 - val_RMSE: 0.7553\n",
            "Epoch 110/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7194 - RMSE: 0.7100 - val_loss: 0.7635 - val_RMSE: 0.7554\n",
            "Epoch 111/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7214 - RMSE: 0.7120 - val_loss: 0.7635 - val_RMSE: 0.7555\n",
            "Epoch 112/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7321 - RMSE: 0.7227 - val_loss: 0.7636 - val_RMSE: 0.7555\n",
            "Epoch 113/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7360 - RMSE: 0.7266 - val_loss: 0.7637 - val_RMSE: 0.7556\n",
            "Epoch 114/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7262 - RMSE: 0.7168 - val_loss: 0.7638 - val_RMSE: 0.7556\n",
            "Epoch 115/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7206 - RMSE: 0.7112 - val_loss: 0.7638 - val_RMSE: 0.7557\n",
            "Epoch 116/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7310 - RMSE: 0.7216 - val_loss: 0.7639 - val_RMSE: 0.7557\n",
            "Epoch 117/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7193 - RMSE: 0.7099 - val_loss: 0.7640 - val_RMSE: 0.7558\n",
            "Epoch 118/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7117 - RMSE: 0.7023 - val_loss: 0.7640 - val_RMSE: 0.7558\n",
            "Epoch 119/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7220 - RMSE: 0.7126 - val_loss: 0.7640 - val_RMSE: 0.7559\n",
            "Epoch 120/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7249 - RMSE: 0.7155 - val_loss: 0.7641 - val_RMSE: 0.7560\n",
            "Epoch 121/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7238 - RMSE: 0.7143 - val_loss: 0.7642 - val_RMSE: 0.7560\n",
            "Epoch 122/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7238 - RMSE: 0.7143 - val_loss: 0.7643 - val_RMSE: 0.7561\n",
            "Epoch 123/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7254 - RMSE: 0.7160 - val_loss: 0.7644 - val_RMSE: 0.7562\n",
            "Epoch 124/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7265 - RMSE: 0.7171 - val_loss: 0.7644 - val_RMSE: 0.7561\n",
            "Epoch 125/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7184 - RMSE: 0.7090 - val_loss: 0.7644 - val_RMSE: 0.7562\n",
            "Epoch 126/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7385 - RMSE: 0.7290 - val_loss: 0.7645 - val_RMSE: 0.7562\n",
            "Epoch 127/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7272 - RMSE: 0.7178 - val_loss: 0.7646 - val_RMSE: 0.7563\n",
            "Epoch 128/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7312 - RMSE: 0.7218 - val_loss: 0.7646 - val_RMSE: 0.7563\n",
            "Epoch 129/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7252 - RMSE: 0.7158 - val_loss: 0.7647 - val_RMSE: 0.7564\n",
            "Epoch 130/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7240 - RMSE: 0.7146 - val_loss: 0.7647 - val_RMSE: 0.7564\n",
            "Epoch 131/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7312 - RMSE: 0.7218 - val_loss: 0.7648 - val_RMSE: 0.7565\n",
            "Epoch 132/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7237 - RMSE: 0.7143 - val_loss: 0.7648 - val_RMSE: 0.7565\n",
            "Epoch 133/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7161 - RMSE: 0.7067 - val_loss: 0.7649 - val_RMSE: 0.7566\n",
            "Epoch 134/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7121 - RMSE: 0.7026 - val_loss: 0.7649 - val_RMSE: 0.7566\n",
            "Epoch 135/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7220 - RMSE: 0.7126 - val_loss: 0.7650 - val_RMSE: 0.7567\n",
            "Epoch 136/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7255 - RMSE: 0.7160 - val_loss: 0.7651 - val_RMSE: 0.7567\n",
            "Epoch 137/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7193 - RMSE: 0.7099 - val_loss: 0.7651 - val_RMSE: 0.7568\n",
            "Epoch 138/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7270 - RMSE: 0.7176 - val_loss: 0.7652 - val_RMSE: 0.7568\n",
            "Epoch 139/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7303 - RMSE: 0.7209 - val_loss: 0.7652 - val_RMSE: 0.7569\n",
            "Epoch 140/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7203 - RMSE: 0.7109 - val_loss: 0.7653 - val_RMSE: 0.7570\n",
            "Epoch 141/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7211 - RMSE: 0.7116 - val_loss: 0.7654 - val_RMSE: 0.7570\n",
            "Epoch 142/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7240 - RMSE: 0.7145 - val_loss: 0.7654 - val_RMSE: 0.7571\n",
            "Epoch 143/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7421 - RMSE: 0.7327 - val_loss: 0.7654 - val_RMSE: 0.7570\n",
            "Epoch 144/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7251 - RMSE: 0.7157 - val_loss: 0.7655 - val_RMSE: 0.7571\n",
            "Epoch 145/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7073 - RMSE: 0.6979 - val_loss: 0.7655 - val_RMSE: 0.7572\n",
            "Epoch 146/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7323 - RMSE: 0.7228 - val_loss: 0.7656 - val_RMSE: 0.7572\n",
            "Epoch 147/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7227 - RMSE: 0.7132 - val_loss: 0.7656 - val_RMSE: 0.7572\n",
            "Epoch 148/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7219 - RMSE: 0.7125 - val_loss: 0.7656 - val_RMSE: 0.7572\n",
            "Epoch 149/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7237 - RMSE: 0.7143 - val_loss: 0.7657 - val_RMSE: 0.7573\n",
            "Epoch 150/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7178 - RMSE: 0.7083 - val_loss: 0.7658 - val_RMSE: 0.7574\n",
            "Epoch 151/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7175 - RMSE: 0.7081 - val_loss: 0.7659 - val_RMSE: 0.7574\n",
            "Epoch 152/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7244 - RMSE: 0.7150 - val_loss: 0.7659 - val_RMSE: 0.7575\n",
            "Epoch 153/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7136 - RMSE: 0.7042 - val_loss: 0.7660 - val_RMSE: 0.7576\n",
            "Epoch 154/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7231 - RMSE: 0.7137 - val_loss: 0.7660 - val_RMSE: 0.7576\n",
            "Epoch 155/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7286 - RMSE: 0.7192 - val_loss: 0.7661 - val_RMSE: 0.7577\n",
            "Epoch 156/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7240 - RMSE: 0.7146 - val_loss: 0.7662 - val_RMSE: 0.7577\n",
            "Epoch 157/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7177 - RMSE: 0.7083 - val_loss: 0.7662 - val_RMSE: 0.7577\n",
            "Epoch 158/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7209 - RMSE: 0.7114 - val_loss: 0.7662 - val_RMSE: 0.7577\n",
            "Epoch 159/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7240 - RMSE: 0.7146 - val_loss: 0.7663 - val_RMSE: 0.7578\n",
            "Epoch 160/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7140 - RMSE: 0.7045 - val_loss: 0.7663 - val_RMSE: 0.7578\n",
            "Epoch 161/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7289 - RMSE: 0.7195 - val_loss: 0.7664 - val_RMSE: 0.7579\n",
            "Epoch 162/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7242 - RMSE: 0.7148 - val_loss: 0.7664 - val_RMSE: 0.7579\n",
            "Epoch 163/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7299 - RMSE: 0.7205 - val_loss: 0.7665 - val_RMSE: 0.7580\n",
            "Epoch 164/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7238 - RMSE: 0.7144 - val_loss: 0.7665 - val_RMSE: 0.7580\n",
            "Epoch 165/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7313 - RMSE: 0.7219 - val_loss: 0.7665 - val_RMSE: 0.7580\n",
            "Epoch 166/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7235 - RMSE: 0.7141 - val_loss: 0.7666 - val_RMSE: 0.7581\n",
            "Epoch 167/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7150 - RMSE: 0.7056 - val_loss: 0.7666 - val_RMSE: 0.7581\n",
            "Epoch 168/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7099 - RMSE: 0.7005 - val_loss: 0.7667 - val_RMSE: 0.7582\n",
            "Epoch 169/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7309 - RMSE: 0.7215 - val_loss: 0.7668 - val_RMSE: 0.7583\n",
            "Epoch 170/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7212 - RMSE: 0.7118 - val_loss: 0.7668 - val_RMSE: 0.7583\n",
            "Epoch 171/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7249 - RMSE: 0.7155 - val_loss: 0.7668 - val_RMSE: 0.7583\n",
            "Epoch 172/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7207 - RMSE: 0.7113 - val_loss: 0.7669 - val_RMSE: 0.7584\n",
            "Epoch 173/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7210 - RMSE: 0.7116 - val_loss: 0.7669 - val_RMSE: 0.7584\n",
            "Epoch 174/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7129 - RMSE: 0.7035 - val_loss: 0.7670 - val_RMSE: 0.7585\n",
            "Epoch 175/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7197 - RMSE: 0.7103 - val_loss: 0.7670 - val_RMSE: 0.7585\n",
            "Epoch 176/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7188 - RMSE: 0.7094 - val_loss: 0.7671 - val_RMSE: 0.7585\n",
            "Epoch 177/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7299 - RMSE: 0.7205 - val_loss: 0.7671 - val_RMSE: 0.7586\n",
            "Epoch 178/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7236 - RMSE: 0.7142 - val_loss: 0.7672 - val_RMSE: 0.7586\n",
            "Epoch 179/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7348 - RMSE: 0.7254 - val_loss: 0.7672 - val_RMSE: 0.7586\n",
            "Epoch 180/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7297 - RMSE: 0.7203 - val_loss: 0.7673 - val_RMSE: 0.7587\n",
            "Epoch 181/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7178 - RMSE: 0.7084 - val_loss: 0.7673 - val_RMSE: 0.7588\n",
            "Epoch 182/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7124 - RMSE: 0.7030 - val_loss: 0.7673 - val_RMSE: 0.7588\n",
            "Epoch 183/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7209 - RMSE: 0.7115 - val_loss: 0.7674 - val_RMSE: 0.7589\n",
            "Epoch 184/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7215 - RMSE: 0.7121 - val_loss: 0.7674 - val_RMSE: 0.7589\n",
            "Epoch 185/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7219 - RMSE: 0.7125 - val_loss: 0.7675 - val_RMSE: 0.7589\n",
            "Epoch 186/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7283 - RMSE: 0.7190 - val_loss: 0.7675 - val_RMSE: 0.7590\n",
            "Epoch 187/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7157 - RMSE: 0.7064 - val_loss: 0.7675 - val_RMSE: 0.7590\n",
            "Epoch 188/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7234 - RMSE: 0.7141 - val_loss: 0.7675 - val_RMSE: 0.7590\n",
            "Epoch 189/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7235 - RMSE: 0.7142 - val_loss: 0.7676 - val_RMSE: 0.7591\n",
            "Epoch 190/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7229 - RMSE: 0.7136 - val_loss: 0.7677 - val_RMSE: 0.7592\n",
            "Epoch 191/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7273 - RMSE: 0.7179 - val_loss: 0.7677 - val_RMSE: 0.7592\n",
            "Epoch 192/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7140 - RMSE: 0.7047 - val_loss: 0.7678 - val_RMSE: 0.7592\n",
            "Epoch 193/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7150 - RMSE: 0.7056 - val_loss: 0.7678 - val_RMSE: 0.7593\n",
            "Epoch 194/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7145 - RMSE: 0.7051 - val_loss: 0.7678 - val_RMSE: 0.7593\n",
            "Epoch 195/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7200 - RMSE: 0.7107 - val_loss: 0.7679 - val_RMSE: 0.7593\n",
            "Epoch 196/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7242 - RMSE: 0.7149 - val_loss: 0.7679 - val_RMSE: 0.7594\n",
            "Epoch 197/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7320 - RMSE: 0.7226 - val_loss: 0.7680 - val_RMSE: 0.7594\n",
            "Epoch 198/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7261 - RMSE: 0.7167 - val_loss: 0.7680 - val_RMSE: 0.7595\n",
            "Epoch 199/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7334 - RMSE: 0.7241 - val_loss: 0.7680 - val_RMSE: 0.7595\n",
            "Epoch 200/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7259 - RMSE: 0.7166 - val_loss: 0.7681 - val_RMSE: 0.7595\n",
            "Epoch 201/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7089 - RMSE: 0.6996 - val_loss: 0.7681 - val_RMSE: 0.7596\n",
            "Epoch 202/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7149 - RMSE: 0.7055 - val_loss: 0.7682 - val_RMSE: 0.7596\n",
            "Epoch 203/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7181 - RMSE: 0.7088 - val_loss: 0.7682 - val_RMSE: 0.7596\n",
            "Epoch 204/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7246 - RMSE: 0.7153 - val_loss: 0.7682 - val_RMSE: 0.7597\n",
            "Epoch 205/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7217 - RMSE: 0.7124 - val_loss: 0.7683 - val_RMSE: 0.7597\n",
            "Epoch 206/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7239 - RMSE: 0.7146 - val_loss: 0.7683 - val_RMSE: 0.7597\n",
            "Epoch 207/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7166 - RMSE: 0.7073 - val_loss: 0.7683 - val_RMSE: 0.7598\n",
            "Epoch 208/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7209 - RMSE: 0.7117 - val_loss: 0.7684 - val_RMSE: 0.7598\n",
            "Epoch 209/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7229 - RMSE: 0.7136 - val_loss: 0.7684 - val_RMSE: 0.7599\n",
            "Epoch 210/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7184 - RMSE: 0.7091 - val_loss: 0.7684 - val_RMSE: 0.7599\n",
            "Epoch 211/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7275 - RMSE: 0.7182 - val_loss: 0.7684 - val_RMSE: 0.7599\n",
            "Epoch 212/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7305 - RMSE: 0.7212 - val_loss: 0.7685 - val_RMSE: 0.7600\n",
            "Epoch 213/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7223 - RMSE: 0.7131 - val_loss: 0.7685 - val_RMSE: 0.7600\n",
            "Epoch 214/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7260 - RMSE: 0.7168 - val_loss: 0.7686 - val_RMSE: 0.7600\n",
            "Epoch 215/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7209 - RMSE: 0.7117 - val_loss: 0.7686 - val_RMSE: 0.7601\n",
            "Epoch 216/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7139 - RMSE: 0.7047 - val_loss: 0.7686 - val_RMSE: 0.7601\n",
            "Epoch 217/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7163 - RMSE: 0.7071 - val_loss: 0.7686 - val_RMSE: 0.7601\n",
            "Epoch 218/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7272 - RMSE: 0.7179 - val_loss: 0.7687 - val_RMSE: 0.7602\n",
            "Epoch 219/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7154 - RMSE: 0.7061 - val_loss: 0.7687 - val_RMSE: 0.7602\n",
            "Epoch 220/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7255 - RMSE: 0.7163 - val_loss: 0.7688 - val_RMSE: 0.7603\n",
            "Epoch 221/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7306 - RMSE: 0.7214 - val_loss: 0.7687 - val_RMSE: 0.7602\n",
            "Epoch 222/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7174 - RMSE: 0.7082 - val_loss: 0.7688 - val_RMSE: 0.7603\n",
            "Epoch 223/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7273 - RMSE: 0.7181 - val_loss: 0.7688 - val_RMSE: 0.7603\n",
            "Epoch 224/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7308 - RMSE: 0.7216 - val_loss: 0.7688 - val_RMSE: 0.7603\n",
            "Epoch 225/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7169 - RMSE: 0.7077 - val_loss: 0.7689 - val_RMSE: 0.7604\n",
            "Epoch 226/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7204 - RMSE: 0.7112 - val_loss: 0.7689 - val_RMSE: 0.7605\n",
            "Epoch 227/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7290 - RMSE: 0.7198 - val_loss: 0.7689 - val_RMSE: 0.7604\n",
            "Epoch 228/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7238 - RMSE: 0.7146 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 229/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7167 - RMSE: 0.7076 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 230/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7069 - RMSE: 0.6978 - val_loss: 0.7690 - val_RMSE: 0.7605\n",
            "Epoch 231/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7174 - RMSE: 0.7083 - val_loss: 0.7690 - val_RMSE: 0.7606\n",
            "Epoch 232/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7147 - RMSE: 0.7055 - val_loss: 0.7691 - val_RMSE: 0.7606\n",
            "Epoch 233/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7181 - RMSE: 0.7089 - val_loss: 0.7691 - val_RMSE: 0.7606\n",
            "Epoch 234/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7249 - RMSE: 0.7158 - val_loss: 0.7691 - val_RMSE: 0.7607\n",
            "Epoch 235/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7109 - RMSE: 0.7018 - val_loss: 0.7692 - val_RMSE: 0.7607\n",
            "Epoch 236/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7291 - RMSE: 0.7199 - val_loss: 0.7692 - val_RMSE: 0.7608\n",
            "Epoch 237/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7185 - RMSE: 0.7094 - val_loss: 0.7692 - val_RMSE: 0.7608\n",
            "Epoch 238/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7253 - RMSE: 0.7162 - val_loss: 0.7692 - val_RMSE: 0.7607\n",
            "Epoch 239/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7127 - RMSE: 0.7036 - val_loss: 0.7692 - val_RMSE: 0.7608\n",
            "Epoch 240/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7140 - RMSE: 0.7049 - val_loss: 0.7693 - val_RMSE: 0.7608\n",
            "Epoch 241/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7209 - RMSE: 0.7118 - val_loss: 0.7693 - val_RMSE: 0.7609\n",
            "Epoch 242/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7250 - RMSE: 0.7159 - val_loss: 0.7693 - val_RMSE: 0.7609\n",
            "Epoch 243/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7217 - RMSE: 0.7126 - val_loss: 0.7694 - val_RMSE: 0.7609\n",
            "Epoch 244/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7203 - RMSE: 0.7113 - val_loss: 0.7694 - val_RMSE: 0.7610\n",
            "Epoch 245/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7235 - RMSE: 0.7144 - val_loss: 0.7694 - val_RMSE: 0.7610\n",
            "Epoch 246/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7157 - RMSE: 0.7066 - val_loss: 0.7694 - val_RMSE: 0.7609\n",
            "Epoch 247/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7258 - RMSE: 0.7167 - val_loss: 0.7694 - val_RMSE: 0.7610\n",
            "Epoch 248/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7135 - RMSE: 0.7045 - val_loss: 0.7694 - val_RMSE: 0.7610\n",
            "Epoch 249/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7133 - RMSE: 0.7042 - val_loss: 0.7694 - val_RMSE: 0.7610\n",
            "Epoch 250/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7122 - RMSE: 0.7032 - val_loss: 0.7695 - val_RMSE: 0.7611\n",
            "Epoch 251/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7182 - RMSE: 0.7092 - val_loss: 0.7695 - val_RMSE: 0.7611\n",
            "Epoch 252/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7071 - RMSE: 0.6981 - val_loss: 0.7695 - val_RMSE: 0.7611\n",
            "Epoch 253/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7197 - RMSE: 0.7107 - val_loss: 0.7695 - val_RMSE: 0.7611\n",
            "Epoch 254/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7076 - RMSE: 0.6986 - val_loss: 0.7695 - val_RMSE: 0.7611\n",
            "Epoch 255/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7192 - RMSE: 0.7102 - val_loss: 0.7696 - val_RMSE: 0.7612\n",
            "Epoch 256/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7134 - RMSE: 0.7044 - val_loss: 0.7696 - val_RMSE: 0.7612\n",
            "Epoch 257/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7281 - RMSE: 0.7191 - val_loss: 0.7696 - val_RMSE: 0.7612\n",
            "Epoch 258/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7210 - RMSE: 0.7120 - val_loss: 0.7696 - val_RMSE: 0.7613\n",
            "Epoch 259/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7186 - RMSE: 0.7096 - val_loss: 0.7696 - val_RMSE: 0.7613\n",
            "Epoch 260/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7250 - RMSE: 0.7160 - val_loss: 0.7697 - val_RMSE: 0.7613\n",
            "Epoch 261/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7233 - RMSE: 0.7144 - val_loss: 0.7696 - val_RMSE: 0.7613\n",
            "Epoch 262/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7201 - RMSE: 0.7111 - val_loss: 0.7696 - val_RMSE: 0.7613\n",
            "Epoch 263/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7289 - RMSE: 0.7199 - val_loss: 0.7697 - val_RMSE: 0.7613\n",
            "Epoch 264/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7268 - RMSE: 0.7178 - val_loss: 0.7697 - val_RMSE: 0.7613\n",
            "Epoch 265/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7199 - RMSE: 0.7109 - val_loss: 0.7697 - val_RMSE: 0.7614\n",
            "Epoch 266/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7225 - RMSE: 0.7136 - val_loss: 0.7698 - val_RMSE: 0.7614\n",
            "Epoch 267/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7160 - RMSE: 0.7070 - val_loss: 0.7697 - val_RMSE: 0.7614\n",
            "Epoch 268/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7218 - RMSE: 0.7129 - val_loss: 0.7698 - val_RMSE: 0.7614\n",
            "Epoch 269/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7158 - RMSE: 0.7068 - val_loss: 0.7698 - val_RMSE: 0.7615\n",
            "Epoch 270/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7065 - RMSE: 0.6975 - val_loss: 0.7698 - val_RMSE: 0.7615\n",
            "Epoch 271/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7268 - RMSE: 0.7179 - val_loss: 0.7698 - val_RMSE: 0.7614\n",
            "Epoch 272/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7104 - RMSE: 0.7016 - val_loss: 0.7698 - val_RMSE: 0.7614\n",
            "Epoch 273/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7149 - RMSE: 0.7060 - val_loss: 0.7698 - val_RMSE: 0.7615\n",
            "Epoch 274/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7239 - RMSE: 0.7150 - val_loss: 0.7698 - val_RMSE: 0.7615\n",
            "Epoch 275/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7110 - RMSE: 0.7021 - val_loss: 0.7698 - val_RMSE: 0.7615\n",
            "Epoch 276/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7155 - RMSE: 0.7066 - val_loss: 0.7698 - val_RMSE: 0.7615\n",
            "Epoch 277/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7236 - RMSE: 0.7147 - val_loss: 0.7698 - val_RMSE: 0.7615\n",
            "Epoch 278/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7168 - RMSE: 0.7079 - val_loss: 0.7697 - val_RMSE: 0.7615\n",
            "Epoch 279/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7137 - RMSE: 0.7049 - val_loss: 0.7699 - val_RMSE: 0.7616\n",
            "Epoch 280/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7327 - RMSE: 0.7239 - val_loss: 0.7698 - val_RMSE: 0.7615\n",
            "Epoch 281/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7226 - RMSE: 0.7138 - val_loss: 0.7698 - val_RMSE: 0.7616\n",
            "Epoch 282/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7245 - RMSE: 0.7157 - val_loss: 0.7698 - val_RMSE: 0.7616\n",
            "Epoch 283/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7190 - RMSE: 0.7101 - val_loss: 0.7699 - val_RMSE: 0.7616\n",
            "Epoch 284/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7206 - RMSE: 0.7118 - val_loss: 0.7699 - val_RMSE: 0.7617\n",
            "Epoch 285/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7108 - RMSE: 0.7020 - val_loss: 0.7699 - val_RMSE: 0.7617\n",
            "Epoch 286/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7251 - RMSE: 0.7163 - val_loss: 0.7699 - val_RMSE: 0.7617\n",
            "Epoch 287/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7232 - RMSE: 0.7144 - val_loss: 0.7699 - val_RMSE: 0.7617\n",
            "Epoch 288/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7287 - RMSE: 0.7199 - val_loss: 0.7699 - val_RMSE: 0.7617\n",
            "Epoch 289/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7228 - RMSE: 0.7140 - val_loss: 0.7699 - val_RMSE: 0.7618\n",
            "Epoch 290/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7210 - RMSE: 0.7123 - val_loss: 0.7699 - val_RMSE: 0.7617\n",
            "Epoch 291/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7291 - RMSE: 0.7204 - val_loss: 0.7700 - val_RMSE: 0.7618\n",
            "Epoch 292/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7199 - RMSE: 0.7112 - val_loss: 0.7700 - val_RMSE: 0.7618\n",
            "Epoch 293/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7218 - RMSE: 0.7131 - val_loss: 0.7700 - val_RMSE: 0.7618\n",
            "Epoch 294/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7224 - RMSE: 0.7136 - val_loss: 0.7701 - val_RMSE: 0.7619\n",
            "Epoch 295/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7189 - RMSE: 0.7101 - val_loss: 0.7700 - val_RMSE: 0.7619\n",
            "Epoch 296/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7179 - RMSE: 0.7091 - val_loss: 0.7700 - val_RMSE: 0.7619\n",
            "Epoch 297/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7284 - RMSE: 0.7197 - val_loss: 0.7701 - val_RMSE: 0.7620\n",
            "Epoch 298/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7265 - RMSE: 0.7177 - val_loss: 0.7700 - val_RMSE: 0.7619\n",
            "Epoch 299/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7170 - RMSE: 0.7082 - val_loss: 0.7700 - val_RMSE: 0.7619\n",
            "Epoch 300/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7208 - RMSE: 0.7121 - val_loss: 0.7700 - val_RMSE: 0.7619\n",
            "Epoch 301/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7139 - RMSE: 0.7052 - val_loss: 0.7700 - val_RMSE: 0.7619\n",
            "Epoch 302/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7196 - RMSE: 0.7109 - val_loss: 0.7700 - val_RMSE: 0.7619\n",
            "Epoch 303/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7149 - RMSE: 0.7062 - val_loss: 0.7700 - val_RMSE: 0.7619\n",
            "Epoch 304/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7182 - RMSE: 0.7095 - val_loss: 0.7701 - val_RMSE: 0.7620\n",
            "Epoch 305/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7083 - RMSE: 0.6996 - val_loss: 0.7701 - val_RMSE: 0.7620\n",
            "Epoch 306/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7111 - RMSE: 0.7025 - val_loss: 0.7702 - val_RMSE: 0.7621\n",
            "Epoch 307/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7136 - RMSE: 0.7049 - val_loss: 0.7701 - val_RMSE: 0.7620\n",
            "Epoch 308/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7168 - RMSE: 0.7081 - val_loss: 0.7702 - val_RMSE: 0.7621\n",
            "Epoch 309/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7273 - RMSE: 0.7186 - val_loss: 0.7701 - val_RMSE: 0.7621\n",
            "Epoch 310/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7205 - RMSE: 0.7118 - val_loss: 0.7702 - val_RMSE: 0.7622\n",
            "Epoch 311/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7054 - RMSE: 0.6968 - val_loss: 0.7702 - val_RMSE: 0.7622\n",
            "Epoch 312/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7114 - RMSE: 0.7028 - val_loss: 0.7702 - val_RMSE: 0.7621\n",
            "Epoch 313/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7080 - RMSE: 0.6993 - val_loss: 0.7702 - val_RMSE: 0.7622\n",
            "Epoch 314/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7350 - RMSE: 0.7264 - val_loss: 0.7702 - val_RMSE: 0.7622\n",
            "Epoch 315/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7134 - RMSE: 0.7048 - val_loss: 0.7703 - val_RMSE: 0.7622\n",
            "Epoch 316/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7136 - RMSE: 0.7050 - val_loss: 0.7702 - val_RMSE: 0.7621\n",
            "Epoch 317/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7203 - RMSE: 0.7117 - val_loss: 0.7702 - val_RMSE: 0.7621\n",
            "Epoch 318/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7204 - RMSE: 0.7118 - val_loss: 0.7702 - val_RMSE: 0.7622\n",
            "Epoch 319/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7146 - RMSE: 0.7060 - val_loss: 0.7703 - val_RMSE: 0.7623\n",
            "Epoch 320/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7198 - RMSE: 0.7112 - val_loss: 0.7703 - val_RMSE: 0.7623\n",
            "Epoch 321/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7298 - RMSE: 0.7213 - val_loss: 0.7703 - val_RMSE: 0.7623\n",
            "Epoch 322/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7201 - RMSE: 0.7115 - val_loss: 0.7702 - val_RMSE: 0.7622\n",
            "Epoch 323/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7127 - RMSE: 0.7042 - val_loss: 0.7702 - val_RMSE: 0.7623\n",
            "Epoch 324/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7200 - RMSE: 0.7114 - val_loss: 0.7703 - val_RMSE: 0.7623\n",
            "Epoch 325/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7056 - RMSE: 0.6970 - val_loss: 0.7703 - val_RMSE: 0.7623\n",
            "Epoch 326/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7220 - RMSE: 0.7135 - val_loss: 0.7703 - val_RMSE: 0.7623\n",
            "Epoch 327/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7085 - RMSE: 0.6999 - val_loss: 0.7703 - val_RMSE: 0.7623\n",
            "Epoch 328/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7252 - RMSE: 0.7167 - val_loss: 0.7703 - val_RMSE: 0.7624\n",
            "Epoch 329/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7171 - RMSE: 0.7086 - val_loss: 0.7703 - val_RMSE: 0.7624\n",
            "Epoch 330/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7082 - RMSE: 0.6997 - val_loss: 0.7703 - val_RMSE: 0.7624\n",
            "Epoch 331/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7111 - RMSE: 0.7026 - val_loss: 0.7704 - val_RMSE: 0.7625\n",
            "Epoch 332/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7292 - RMSE: 0.7207 - val_loss: 0.7703 - val_RMSE: 0.7624\n",
            "Epoch 333/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7193 - RMSE: 0.7108 - val_loss: 0.7705 - val_RMSE: 0.7626\n",
            "Epoch 334/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7071 - RMSE: 0.6986 - val_loss: 0.7703 - val_RMSE: 0.7624\n",
            "Epoch 335/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7192 - RMSE: 0.7107 - val_loss: 0.7704 - val_RMSE: 0.7625\n",
            "Epoch 336/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7206 - RMSE: 0.7121 - val_loss: 0.7703 - val_RMSE: 0.7624\n",
            "Epoch 337/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7141 - RMSE: 0.7056 - val_loss: 0.7703 - val_RMSE: 0.7624\n",
            "Epoch 338/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7214 - RMSE: 0.7129 - val_loss: 0.7704 - val_RMSE: 0.7625\n",
            "Epoch 339/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7198 - RMSE: 0.7113 - val_loss: 0.7704 - val_RMSE: 0.7625\n",
            "Epoch 340/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7144 - RMSE: 0.7059 - val_loss: 0.7703 - val_RMSE: 0.7624\n",
            "Epoch 341/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7144 - RMSE: 0.7059 - val_loss: 0.7704 - val_RMSE: 0.7625\n",
            "Epoch 342/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7184 - RMSE: 0.7099 - val_loss: 0.7703 - val_RMSE: 0.7625\n",
            "Epoch 343/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7061 - RMSE: 0.6977 - val_loss: 0.7704 - val_RMSE: 0.7625\n",
            "Epoch 344/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7237 - RMSE: 0.7153 - val_loss: 0.7703 - val_RMSE: 0.7625\n",
            "Epoch 345/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7217 - RMSE: 0.7133 - val_loss: 0.7704 - val_RMSE: 0.7625\n",
            "Epoch 346/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7131 - RMSE: 0.7047 - val_loss: 0.7704 - val_RMSE: 0.7625\n",
            "Epoch 347/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7132 - RMSE: 0.7048 - val_loss: 0.7703 - val_RMSE: 0.7624\n",
            "Epoch 348/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7183 - RMSE: 0.7099 - val_loss: 0.7704 - val_RMSE: 0.7626\n",
            "Epoch 349/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7157 - RMSE: 0.7073 - val_loss: 0.7703 - val_RMSE: 0.7625\n",
            "Epoch 350/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7254 - RMSE: 0.7170 - val_loss: 0.7703 - val_RMSE: 0.7625\n",
            "Epoch 351/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7155 - RMSE: 0.7071 - val_loss: 0.7704 - val_RMSE: 0.7626\n",
            "Epoch 352/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7275 - RMSE: 0.7191 - val_loss: 0.7704 - val_RMSE: 0.7626\n",
            "Epoch 353/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7226 - RMSE: 0.7142 - val_loss: 0.7703 - val_RMSE: 0.7625\n",
            "Epoch 354/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7199 - RMSE: 0.7116 - val_loss: 0.7704 - val_RMSE: 0.7626\n",
            "Epoch 355/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7165 - RMSE: 0.7081 - val_loss: 0.7704 - val_RMSE: 0.7626\n",
            "Epoch 356/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7127 - RMSE: 0.7043 - val_loss: 0.7704 - val_RMSE: 0.7626\n",
            "Epoch 357/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7215 - RMSE: 0.7131 - val_loss: 0.7705 - val_RMSE: 0.7627\n",
            "Epoch 358/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7172 - RMSE: 0.7088 - val_loss: 0.7705 - val_RMSE: 0.7627\n",
            "Epoch 359/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7121 - RMSE: 0.7037 - val_loss: 0.7705 - val_RMSE: 0.7627\n",
            "Epoch 360/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7122 - RMSE: 0.7038 - val_loss: 0.7704 - val_RMSE: 0.7626\n",
            "Epoch 361/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7097 - RMSE: 0.7014 - val_loss: 0.7704 - val_RMSE: 0.7626\n",
            "Epoch 362/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7208 - RMSE: 0.7125 - val_loss: 0.7703 - val_RMSE: 0.7626\n",
            "Epoch 363/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7184 - RMSE: 0.7101 - val_loss: 0.7705 - val_RMSE: 0.7627\n",
            "Epoch 364/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7207 - RMSE: 0.7124 - val_loss: 0.7705 - val_RMSE: 0.7628\n",
            "Epoch 365/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7144 - RMSE: 0.7061 - val_loss: 0.7705 - val_RMSE: 0.7628\n",
            "Epoch 366/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7217 - RMSE: 0.7134 - val_loss: 0.7705 - val_RMSE: 0.7628\n",
            "Epoch 367/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7185 - RMSE: 0.7102 - val_loss: 0.7705 - val_RMSE: 0.7628\n",
            "Epoch 368/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7184 - RMSE: 0.7101 - val_loss: 0.7706 - val_RMSE: 0.7628\n",
            "Epoch 369/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7283 - RMSE: 0.7200 - val_loss: 0.7705 - val_RMSE: 0.7629\n",
            "Epoch 370/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7127 - RMSE: 0.7044 - val_loss: 0.7705 - val_RMSE: 0.7628\n",
            "Epoch 371/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7178 - RMSE: 0.7095 - val_loss: 0.7705 - val_RMSE: 0.7628\n",
            "Epoch 372/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7196 - RMSE: 0.7114 - val_loss: 0.7705 - val_RMSE: 0.7628\n",
            "Epoch 373/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7151 - RMSE: 0.7069 - val_loss: 0.7706 - val_RMSE: 0.7629\n",
            "Epoch 374/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7280 - RMSE: 0.7197 - val_loss: 0.7705 - val_RMSE: 0.7628\n",
            "Epoch 375/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7234 - RMSE: 0.7152 - val_loss: 0.7706 - val_RMSE: 0.7629\n",
            "Epoch 376/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7184 - RMSE: 0.7102 - val_loss: 0.7706 - val_RMSE: 0.7629\n",
            "Epoch 377/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7158 - RMSE: 0.7076 - val_loss: 0.7706 - val_RMSE: 0.7629\n",
            "Epoch 378/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7238 - RMSE: 0.7156 - val_loss: 0.7706 - val_RMSE: 0.7629\n",
            "Epoch 379/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7230 - RMSE: 0.7148 - val_loss: 0.7706 - val_RMSE: 0.7629\n",
            "Epoch 380/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7054 - RMSE: 0.6971 - val_loss: 0.7705 - val_RMSE: 0.7629\n",
            "Epoch 381/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7295 - RMSE: 0.7213 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 382/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7213 - RMSE: 0.7131 - val_loss: 0.7705 - val_RMSE: 0.7629\n",
            "Epoch 383/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7166 - RMSE: 0.7084 - val_loss: 0.7706 - val_RMSE: 0.7629\n",
            "Epoch 384/400\n",
            "59/59 [==============================] - 0s 2ms/step - loss: 0.7144 - RMSE: 0.7062 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 385/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7250 - RMSE: 0.7168 - val_loss: 0.7705 - val_RMSE: 0.7629\n",
            "Epoch 386/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7269 - RMSE: 0.7187 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 387/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7244 - RMSE: 0.7162 - val_loss: 0.7706 - val_RMSE: 0.7629\n",
            "Epoch 388/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7106 - RMSE: 0.7024 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 389/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7225 - RMSE: 0.7143 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 390/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7142 - RMSE: 0.7060 - val_loss: 0.7707 - val_RMSE: 0.7631\n",
            "Epoch 391/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7184 - RMSE: 0.7102 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 392/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7182 - RMSE: 0.7101 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 393/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7152 - RMSE: 0.7070 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 394/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7128 - RMSE: 0.7047 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 395/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7130 - RMSE: 0.7049 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 396/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7180 - RMSE: 0.7099 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 397/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7188 - RMSE: 0.7107 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 398/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7275 - RMSE: 0.7194 - val_loss: 0.7707 - val_RMSE: 0.7631\n",
            "Epoch 399/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7163 - RMSE: 0.7082 - val_loss: 0.7706 - val_RMSE: 0.7630\n",
            "Epoch 400/400\n",
            "59/59 [==============================] - 0s 3ms/step - loss: 0.7206 - RMSE: 0.7125 - val_loss: 0.7706 - val_RMSE: 0.7630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "mHqhQ3waejbj",
        "outputId": "99aaef69-2fcb-4447-b633-3a56a6f81835"
      },
      "source": [
        "plt.plot(result.history['RMSE'], label=\"Train RMSE\")\n",
        "plt.plot(result.history['val_RMSE'], label=\"Test RMSE\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b19YaK1JQ8-T",
        "outputId": "32236c79-d7e3-4b9b-cfd0-75e695cdfac0"
      },
      "source": [
        "model=best_model\n",
        "user_ids = ratings_test.users.values\n",
        "track_ids = ratings_test.tracks.values\n",
        "nat_ids = test_nat.values\n",
        "y_pred = model.predict([user_ids, track_ids,nat_ids]) + mu\n",
        "y_pred = np.ravel(y_pred, order='C')\n",
        "y_true = np.array(ratings_test.score)\n",
        "\n",
        "display(RMSE2(y_true, y_pred))\n",
        "y_pred=list(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.725"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHATq0beejbj"
      },
      "source": [
        "model.save(f'/gdrive/Shareddrives/Spotify Recommendation/Recommendation Models/NeuralNet_Nationality_{today()}.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EpvqGcwiTAC"
      },
      "source": [
        "### get_top_n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN-dbVCBbPWi",
        "outputId": "9ab24328-0978-427c-b492-cc7f3f4d424f"
      },
      "source": [
        "def NN_top_n(predictions,n=5):\n",
        "  temp=defaultdict(list)\n",
        "  for uidx,iidx,nat,est in zip(user_ids,track_ids,nat_ids,y_pred):\n",
        "    temp[idx2user[uidx]].append((idx2track[iidx],est))\n",
        "\n",
        "  for uid,user_ratings in temp.items():\n",
        "    user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "    temp[uid] = user_ratings[:n]\n",
        "\n",
        "  return temp\n",
        "\n",
        "def NN_Recommend(uid,top_n):\n",
        "  if uid =='윤지현':\n",
        "    print(f'<{uid} (a.k.a. 윤영롱) 님을 위한 추천곡>\\n')\n",
        "  else:\n",
        "    print(f'<{uid} 님을 위한 추천곡>\\n')\n",
        "  for (iid,est) in top_n[uid]:\n",
        "    print(f'{iid} => {round(round(est,4)*100,2)} %의 확률로 좋아하실 거에요')\n",
        "top_n = NN_top_n(y_pred)\n",
        "Recommend('도윤',top_n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<도윤 님을 위한 추천곡>\n",
            "\n",
            "Billie Jean__Michael Jackson => 86.27 %의 확률로 좋아하실 거에요\n",
            "I'm the One__DJ Khaled => 83.2 %의 확률로 좋아하실 거에요\n",
            "Hold On, We're Going Home__Drake => 71.49 %의 확률로 좋아하실 거에요\n",
            "Levels - Radio Edit__Avicii => 68.89 %의 확률로 좋아하실 거에요\n",
            "Take Care__Drake => 47.19 %의 확률로 좋아하실 거에요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SLT6Y51IVZ7"
      },
      "source": [
        "# Hybrid : CF + MF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDaswBCjh1O4"
      },
      "source": [
        "# CF 추천 알고리즘\n",
        "rating_matrix = ratings_train.pivot(index='users', columns='movieId', values='rating')\n",
        "\n",
        "# train set 사용자들의 Cosine similarities 계산\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "matrix_dummy = rating_matrix.copy().fillna(0)\n",
        "user_similarity = cosine_similarity(matrix_dummy, matrix_dummy)\n",
        "user_similarity = pd.DataFrame(user_similarity, index=rating_matrix.index, columns=rating_matrix.index)\n",
        "\n",
        "# train 데이터의 user의 rating 평균과 영화의 평점편차 계산 \n",
        "rating_mean = rating_matrix.mean(axis=1)\n",
        "rating_bias = (rating_matrix.T - rating_mean).T\n",
        "\n",
        "def CF_knn_bias(user_id, movie_id, neighbor_size=0):\n",
        "    if movie_id in rating_bias:\n",
        "        sim_scores = user_similarity[user_id]\n",
        "        movie_ratings = rating_bias[movie_id]\n",
        "        none_rating_idx = movie_ratings[movie_ratings.isnull()].index\n",
        "        movie_ratings = movie_ratings.drop(none_rating_idx)\n",
        "        sim_scores = sim_scores.drop(none_rating_idx)\n",
        "        if neighbor_size == 0:\n",
        "            prediction = np.dot(sim_scores, movie_ratings) / sim_scores.sum()\n",
        "            prediction = prediction + rating_mean[user_id]\n",
        "        else:\n",
        "            if len(sim_scores) > 1:\n",
        "                neighbor_size = min(neighbor_size, len(sim_scores))\n",
        "                sim_scores = np.array(sim_scores)\n",
        "                movie_ratings = np.array(movie_ratings)\n",
        "                user_idx = np.argsort(sim_scores)\n",
        "                sim_scores = sim_scores[user_idx][-neighbor_size:]\n",
        "                movie_ratings = movie_ratings[user_idx][-neighbor_size:]\n",
        "                prediction = np.dot(sim_scores, movie_ratings) / sim_scores.sum()\n",
        "                prediction = prediction + rating_mean[user_id]\n",
        "            else:\n",
        "                prediction = rating_mean[user_id]\n",
        "    else:\n",
        "        prediction = rating_mean[user_id]\n",
        "    return prediction\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRZqthxzh69_",
        "outputId": "28efbf00-ac82-432c-9a92-42c979524aab"
      },
      "source": [
        "# MF 추천 알고리즘\n",
        "class NEW_MF():\n",
        "    def __init__(self, ratings, K, alpha, beta, iterations, verbose=True):\n",
        "        self.R = np.array(ratings)\n",
        "        item_id_index = []\n",
        "        index_item_id = []\n",
        "        for i, one_id in enumerate(ratings):\n",
        "            item_id_index.append([one_id, i])\n",
        "            index_item_id.append([i, one_id])\n",
        "        self.item_id_index = dict(item_id_index)\n",
        "        self.index_item_id = dict(index_item_id)        \n",
        "        user_id_index = []\n",
        "        index_user_id = []\n",
        "        for i, one_id in enumerate(ratings.T):\n",
        "            user_id_index.append([one_id, i])\n",
        "            index_user_id.append([i, one_id])\n",
        "        self.user_id_index = dict(user_id_index)\n",
        "        self.index_user_id = dict(index_user_id)\n",
        "        self.num_users, self.num_items = np.shape(self.R)\n",
        "        self.K = K\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.iterations = iterations\n",
        "        self.verbose = verbose\n",
        "\n",
        "    # train set의 RMSE 계산\n",
        "    def rmse(self):\n",
        "        xs, ys = self.R.nonzero()\n",
        "        self.predictions = []\n",
        "        self.errors = []\n",
        "        for x, y in zip(xs, ys):\n",
        "            prediction = self.get_prediction(x, y)\n",
        "            self.predictions.append(prediction)\n",
        "            self.errors.append(self.R[x, y] - prediction)\n",
        "        self.predictions = np.array(self.predictions)\n",
        "        self.errors = np.array(self.errors)\n",
        "        return np.sqrt(np.mean(self.errors**2))\n",
        "\n",
        "    # Ratings for user i and item j\n",
        "    def get_prediction(self, i, j):\n",
        "        prediction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
        "        return prediction\n",
        "\n",
        "    # Stochastic gradient descent to get optimized P and Q matrix\n",
        "    def sgd(self):\n",
        "        for i, j, r in self.samples:\n",
        "            prediction = self.get_prediction(i, j)\n",
        "            e = (r - prediction)\n",
        "\n",
        "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
        "            self.b_d[j] += self.alpha * (e - self.beta * self.b_d[j])\n",
        "\n",
        "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
        "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n",
        "\n",
        "    # Test set을 선정\n",
        "    def set_test(self, ratings_test):\n",
        "        test_set = []\n",
        "        for i in range(len(ratings_test)):\n",
        "            x = self.user_id_index[ratings_test.iloc[i, 0]]\n",
        "            y = self.item_id_index[ratings_test.iloc[i, 1]]\n",
        "            z = ratings_test.iloc[i, 2]\n",
        "            test_set.append([x, y, z])\n",
        "            self.R[x, y] = 0                    # Setting test set ratings to 0\n",
        "        self.test_set = test_set\n",
        "        return test_set                         # Return test set\n",
        "\n",
        "    # Test set의 RMSE 계산\n",
        "    def test_rmse(self):\n",
        "        error = 0\n",
        "        for one_set in self.test_set:\n",
        "            predicted = self.get_prediction(one_set[0], one_set[1])\n",
        "            error += pow(one_set[2] - predicted, 2)\n",
        "        return np.sqrt(error/len(self.test_set))\n",
        "\n",
        "    # Training 하면서 test set의 정확도를 계산\n",
        "    def test(self):\n",
        "        # Initializing user-feature and item-feature matrix\n",
        "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
        "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
        "\n",
        "        # Initializing the bias terms\n",
        "        self.b_u = np.zeros(self.num_users)\n",
        "        self.b_d = np.zeros(self.num_items)\n",
        "        self.b = np.mean(self.R[self.R.nonzero()])\n",
        "\n",
        "        # List of training samples\n",
        "        rows, columns = self.R.nonzero()\n",
        "        self.samples = [(i, j, self.R[i,j]) for i, j in zip(rows, columns)]\n",
        "\n",
        "        # Stochastic gradient descent for given number of iterations\n",
        "        training_process = []\n",
        "        for i in range(self.iterations):\n",
        "            np.random.shuffle(self.samples)\n",
        "            self.sgd()\n",
        "            rmse1 = self.rmse()\n",
        "            rmse2 = self.test_rmse()\n",
        "            training_process.append((i+1, rmse1, rmse2))\n",
        "            if self.verbose:\n",
        "                if (i+1) % 10 == 0:\n",
        "                    print(\"Iteration: %d ; Train RMSE = %.4f ; Test RMSE = %.4f\" % (i+1, rmse1, rmse2))\n",
        "        return training_process\n",
        "\n",
        "    # Ratings for given user_id and item_id\n",
        "    def get_one_prediction(self, user_id, item_id):\n",
        "        prediction = self.get_prediction(self.user_id_index[user_id], self.item_id_index[item_id])\n",
        "        return prediction\n",
        "\n",
        "    # Full user-movie rating matrix\n",
        "    def full_prediction(self):\n",
        "        return self.b + self.b_u[:,np.newaxis] + self.b_d[np.newaxis,:] + self.P.dot(self.Q.T)\n",
        "\n",
        "# MF클래스 생성 및 학습\n",
        "R_temp = ratings.pivot(index='users', columns='movieId', values='rating').fillna(0)\n",
        "mf = NEW_MF(R_temp, K=200, alpha=0.001, beta=0.02, iterations=250, verbose=True)\n",
        "test_set = mf.set_test(ratings_test)\n",
        "result = mf.test()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 10 ; Train RMSE = 0.8609 ; Test RMSE = 0.7942\n",
            "Iteration: 20 ; Train RMSE = 0.8380 ; Test RMSE = 0.8002\n",
            "Iteration: 30 ; Train RMSE = 0.8255 ; Test RMSE = 0.8046\n",
            "Iteration: 40 ; Train RMSE = 0.8173 ; Test RMSE = 0.8070\n",
            "Iteration: 50 ; Train RMSE = 0.8115 ; Test RMSE = 0.8082\n",
            "Iteration: 60 ; Train RMSE = 0.8073 ; Test RMSE = 0.8089\n",
            "Iteration: 70 ; Train RMSE = 0.8041 ; Test RMSE = 0.8093\n",
            "Iteration: 80 ; Train RMSE = 0.8017 ; Test RMSE = 0.8096\n",
            "Iteration: 90 ; Train RMSE = 0.7998 ; Test RMSE = 0.8100\n",
            "Iteration: 100 ; Train RMSE = 0.7984 ; Test RMSE = 0.8104\n",
            "Iteration: 110 ; Train RMSE = 0.7972 ; Test RMSE = 0.8108\n",
            "Iteration: 120 ; Train RMSE = 0.7961 ; Test RMSE = 0.8111\n",
            "Iteration: 130 ; Train RMSE = 0.7953 ; Test RMSE = 0.8115\n",
            "Iteration: 140 ; Train RMSE = 0.7945 ; Test RMSE = 0.8118\n",
            "Iteration: 150 ; Train RMSE = 0.7937 ; Test RMSE = 0.8122\n",
            "Iteration: 160 ; Train RMSE = 0.7929 ; Test RMSE = 0.8125\n",
            "Iteration: 170 ; Train RMSE = 0.7921 ; Test RMSE = 0.8127\n",
            "Iteration: 180 ; Train RMSE = 0.7912 ; Test RMSE = 0.8130\n",
            "Iteration: 190 ; Train RMSE = 0.7901 ; Test RMSE = 0.8131\n",
            "Iteration: 200 ; Train RMSE = 0.7889 ; Test RMSE = 0.8133\n",
            "Iteration: 210 ; Train RMSE = 0.7875 ; Test RMSE = 0.8133\n",
            "Iteration: 220 ; Train RMSE = 0.7859 ; Test RMSE = 0.8133\n",
            "Iteration: 230 ; Train RMSE = 0.7840 ; Test RMSE = 0.8133\n",
            "Iteration: 240 ; Train RMSE = 0.7816 ; Test RMSE = 0.8131\n",
            "Iteration: 250 ; Train RMSE = 0.7788 ; Test RMSE = 0.8128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3so825DtUUcw",
        "outputId": "bb7c250b-283e-46e7-da17-d628d4d19dc7"
      },
      "source": [
        "# HYBRID 추천 알고리즘\n",
        "def recommender0(recomm_list, mf):\n",
        "    recommendations = np.array([mf.get_one_prediction(user, movie) for (user, movie) in recomm_list])\n",
        "    return recommendations\n",
        "\n",
        "def recommender1(recomm_list, neighbor_size=0):\n",
        "    recommendations = np.array([CF_knn_bias(user, movie, neighbor_size) for (user, movie) in recomm_list])\n",
        "    return recommendations\n",
        "\n",
        "recomm_list = np.array(ratings_test.iloc[:, [0, 1]])\n",
        "predictions0 = recommender0(recomm_list, mf)\n",
        "RMSE2(ratings_test.iloc[:, 2], predictions0)\n",
        "predictions1 = recommender1(recomm_list, 37)\n",
        "RMSE2(ratings_test.iloc[:, 2], predictions1)\n",
        "\n",
        "weight = [0.8, 0.2]\n",
        "predictions = predictions0 * weight[0] + predictions1 * weight[1]\n",
        "RMSE2(ratings_test.iloc[:, 2], predictions)\n",
        "\n",
        "for i in np.arange(0, 1, 0.01):\n",
        "    weight = [i, 1.0 - i]\n",
        "    predictions = predictions0 * weight[0] + predictions1 * weight[1]\n",
        "    print(\"Weights - %.2f : %.2f ; RMSE = %.7f\" % (weight[0], \n",
        "           weight[1], RMSE2(ratings_test.iloc[:, 2], predictions)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights - 0.00 : 1.00 ; RMSE = 0.7492532\n",
            "Weights - 0.01 : 0.99 ; RMSE = 0.7493927\n",
            "Weights - 0.02 : 0.98 ; RMSE = 0.7495428\n",
            "Weights - 0.03 : 0.97 ; RMSE = 0.7497035\n",
            "Weights - 0.04 : 0.96 ; RMSE = 0.7498746\n",
            "Weights - 0.05 : 0.95 ; RMSE = 0.7500563\n",
            "Weights - 0.06 : 0.94 ; RMSE = 0.7502485\n",
            "Weights - 0.07 : 0.93 ; RMSE = 0.7504511\n",
            "Weights - 0.08 : 0.92 ; RMSE = 0.7506643\n",
            "Weights - 0.09 : 0.91 ; RMSE = 0.7508880\n",
            "Weights - 0.10 : 0.90 ; RMSE = 0.7511221\n",
            "Weights - 0.11 : 0.89 ; RMSE = 0.7513667\n",
            "Weights - 0.12 : 0.88 ; RMSE = 0.7516218\n",
            "Weights - 0.13 : 0.87 ; RMSE = 0.7518873\n",
            "Weights - 0.14 : 0.86 ; RMSE = 0.7521632\n",
            "Weights - 0.15 : 0.85 ; RMSE = 0.7524495\n",
            "Weights - 0.16 : 0.84 ; RMSE = 0.7527463\n",
            "Weights - 0.17 : 0.83 ; RMSE = 0.7530535\n",
            "Weights - 0.18 : 0.82 ; RMSE = 0.7533710\n",
            "Weights - 0.19 : 0.81 ; RMSE = 0.7536989\n",
            "Weights - 0.20 : 0.80 ; RMSE = 0.7540372\n",
            "Weights - 0.21 : 0.79 ; RMSE = 0.7543858\n",
            "Weights - 0.22 : 0.78 ; RMSE = 0.7547447\n",
            "Weights - 0.23 : 0.77 ; RMSE = 0.7551140\n",
            "Weights - 0.24 : 0.76 ; RMSE = 0.7554936\n",
            "Weights - 0.25 : 0.75 ; RMSE = 0.7558834\n",
            "Weights - 0.26 : 0.74 ; RMSE = 0.7562835\n",
            "Weights - 0.27 : 0.73 ; RMSE = 0.7566939\n",
            "Weights - 0.28 : 0.72 ; RMSE = 0.7571145\n",
            "Weights - 0.29 : 0.71 ; RMSE = 0.7575453\n",
            "Weights - 0.30 : 0.70 ; RMSE = 0.7579863\n",
            "Weights - 0.31 : 0.69 ; RMSE = 0.7584375\n",
            "Weights - 0.32 : 0.68 ; RMSE = 0.7588989\n",
            "Weights - 0.33 : 0.67 ; RMSE = 0.7593704\n",
            "Weights - 0.34 : 0.66 ; RMSE = 0.7598520\n",
            "Weights - 0.35 : 0.65 ; RMSE = 0.7603438\n",
            "Weights - 0.36 : 0.64 ; RMSE = 0.7608456\n",
            "Weights - 0.37 : 0.63 ; RMSE = 0.7613575\n",
            "Weights - 0.38 : 0.62 ; RMSE = 0.7618795\n",
            "Weights - 0.39 : 0.61 ; RMSE = 0.7624115\n",
            "Weights - 0.40 : 0.60 ; RMSE = 0.7629535\n",
            "Weights - 0.41 : 0.59 ; RMSE = 0.7635055\n",
            "Weights - 0.42 : 0.58 ; RMSE = 0.7640674\n",
            "Weights - 0.43 : 0.57 ; RMSE = 0.7646393\n",
            "Weights - 0.44 : 0.56 ; RMSE = 0.7652211\n",
            "Weights - 0.45 : 0.55 ; RMSE = 0.7658128\n",
            "Weights - 0.46 : 0.54 ; RMSE = 0.7664144\n",
            "Weights - 0.47 : 0.53 ; RMSE = 0.7670258\n",
            "Weights - 0.48 : 0.52 ; RMSE = 0.7676470\n",
            "Weights - 0.49 : 0.51 ; RMSE = 0.7682781\n",
            "Weights - 0.50 : 0.50 ; RMSE = 0.7689189\n",
            "Weights - 0.51 : 0.49 ; RMSE = 0.7695695\n",
            "Weights - 0.52 : 0.48 ; RMSE = 0.7702298\n",
            "Weights - 0.53 : 0.47 ; RMSE = 0.7708999\n",
            "Weights - 0.54 : 0.46 ; RMSE = 0.7715796\n",
            "Weights - 0.55 : 0.45 ; RMSE = 0.7722689\n",
            "Weights - 0.56 : 0.44 ; RMSE = 0.7729679\n",
            "Weights - 0.57 : 0.43 ; RMSE = 0.7736765\n",
            "Weights - 0.58 : 0.42 ; RMSE = 0.7743947\n",
            "Weights - 0.59 : 0.41 ; RMSE = 0.7751224\n",
            "Weights - 0.60 : 0.40 ; RMSE = 0.7758596\n",
            "Weights - 0.61 : 0.39 ; RMSE = 0.7766064\n",
            "Weights - 0.62 : 0.38 ; RMSE = 0.7773626\n",
            "Weights - 0.63 : 0.37 ; RMSE = 0.7781282\n",
            "Weights - 0.64 : 0.36 ; RMSE = 0.7789033\n",
            "Weights - 0.65 : 0.35 ; RMSE = 0.7796877\n",
            "Weights - 0.66 : 0.34 ; RMSE = 0.7804815\n",
            "Weights - 0.67 : 0.33 ; RMSE = 0.7812846\n",
            "Weights - 0.68 : 0.32 ; RMSE = 0.7820971\n",
            "Weights - 0.69 : 0.31 ; RMSE = 0.7829187\n",
            "Weights - 0.70 : 0.30 ; RMSE = 0.7837497\n",
            "Weights - 0.71 : 0.29 ; RMSE = 0.7845898\n",
            "Weights - 0.72 : 0.28 ; RMSE = 0.7854392\n",
            "Weights - 0.73 : 0.27 ; RMSE = 0.7862976\n",
            "Weights - 0.74 : 0.26 ; RMSE = 0.7871652\n",
            "Weights - 0.75 : 0.25 ; RMSE = 0.7880419\n",
            "Weights - 0.76 : 0.24 ; RMSE = 0.7889277\n",
            "Weights - 0.77 : 0.23 ; RMSE = 0.7898225\n",
            "Weights - 0.78 : 0.22 ; RMSE = 0.7907263\n",
            "Weights - 0.79 : 0.21 ; RMSE = 0.7916390\n",
            "Weights - 0.80 : 0.20 ; RMSE = 0.7925607\n",
            "Weights - 0.81 : 0.19 ; RMSE = 0.7934913\n",
            "Weights - 0.82 : 0.18 ; RMSE = 0.7944308\n",
            "Weights - 0.83 : 0.17 ; RMSE = 0.7953792\n",
            "Weights - 0.84 : 0.16 ; RMSE = 0.7963363\n",
            "Weights - 0.85 : 0.15 ; RMSE = 0.7973022\n",
            "Weights - 0.86 : 0.14 ; RMSE = 0.7982769\n",
            "Weights - 0.87 : 0.13 ; RMSE = 0.7992603\n",
            "Weights - 0.88 : 0.12 ; RMSE = 0.8002524\n",
            "Weights - 0.89 : 0.11 ; RMSE = 0.8012531\n",
            "Weights - 0.90 : 0.10 ; RMSE = 0.8022624\n",
            "Weights - 0.91 : 0.09 ; RMSE = 0.8032804\n",
            "Weights - 0.92 : 0.08 ; RMSE = 0.8043069\n",
            "Weights - 0.93 : 0.07 ; RMSE = 0.8053419\n",
            "Weights - 0.94 : 0.06 ; RMSE = 0.8063854\n",
            "Weights - 0.95 : 0.05 ; RMSE = 0.8074374\n",
            "Weights - 0.96 : 0.04 ; RMSE = 0.8084978\n",
            "Weights - 0.97 : 0.03 ; RMSE = 0.8095665\n",
            "Weights - 0.98 : 0.02 ; RMSE = 0.8106437\n",
            "Weights - 0.99 : 0.01 ; RMSE = 0.8117292\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}